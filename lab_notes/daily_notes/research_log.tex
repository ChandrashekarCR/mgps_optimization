\documentclass{article}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{booktabs}

% Header and Footer
\pagestyle{fancy}
\fancyhf{}
\rhead{\textbf{Bioinformatics Research Log}}
\rfoot{Page \thepage}

% Title format
\titleformat{\section}{\Large\bfseries}{}{0pt}{}

\title{\textbf{Bioinformatics Research Project Log}}
\author{Chandrashekar CR \\ Supervisor: Dr. Eran Elhaik \\ Lund University}
\date{April 10, 2025}

\begin{document}

\maketitle
\tableofcontents
\newpage

% March 31, 2025
\section{March 31, 2025}

\subsection*{Tasks for the Day}
\begin{itemize}
    \item Understood the mGPS algorithm from the R code and implemented the preprocessing steps.
    \item Set up the working environment and installed all required libraries.
    \item Began tracking the project.
\end{itemize}

\subsection*{Notes and Observations}
\begin{itemize}
    \item Utilizing the \texttt{ai\_env} environment from previous projects and accessing resources on the bioinformatics server.
    \item Initial challenges in understanding the R code are anticipated to decrease with further engagement.
\end{itemize}

% April 1, 2025
\section{April 1, 2025}

\subsection*{Tasks for the Day}
\begin{itemize}
    \item Understood and implemented Recursive Feature Elimination followed by the XGBoost machine learning algorithm with the correct hierarchical steps.
    \item Initialized Git for version control and pushed code to GitHub.
    \item Gained a deeper understanding of cross-validation principles.
\end{itemize}

\subsection*{Notes and Observations}
\begin{itemize}
    \item Achieved a general understanding of the workflow and added comments to the MetaSUB preprocessing script in R.
    \item Acquired knowledge regarding the importance of cross-validation, although implementation is pending.
\end{itemize}

% April 2, 2025
\section{April 2, 2025}

\subsection*{Tasks for the Day}
\begin{itemize}
    \item Acquired information from Vignesh regarding access to the LUNARC server.
    \item Determined that reimplementing the exact XGBoost model is unnecessary; the focus is on understanding the input data preprocessing.
\end{itemize}

\subsection*{Notes and Observations}
\begin{itemize}
    \item Git repository initialized for the project, tracking all files except data and research papers.
    \item The \texttt{metasub\_global\_git.csv} file contains the Geographically Informative Taxa (GITs) required for the XGBoost model.
    \item The primary objective is to comprehend the preprocessing of input data for XGBoost.
    \item Key questions identified: What is the shape of the input data? What are the prediction targets?
\end{itemize}

% April 3, 2025
\section{April 3, 2025}

\subsection*{Tasks for the Day}
\begin{itemize}
    \item Implemented basic neural network architectures in PyTorch. Hyperparameter tuning indicates that 200 GITs are sufficient for accurate predictions, despite the dataset containing \( n \) data points.
    \item Integrated Ray parallel processing to optimize hyperparameters, aiming to reduce the estimated 4-5 hour search time.
    \item Preprocessed data into numerical format by converting categorical variables (continents and cities) using one-hot encoding.
    \item Deferred the implementation of stratified K-fold cross-validation for later.
\end{itemize}

\subsection*{Notes and Observations}
\begin{itemize}
    \item Successfully logged into the LUNARC server's login node, but GPU access and utilization for neural network training require further investigation.
    \item Authentication and login to LUNARC are complete; however, assistance is needed to understand:
    \begin{itemize}
        \item The location of allocated storage.
        \item The process of submitting jobs using SBATCH.
        \item The fundamentals of working on High-Performance Computing (HPC).
    \end{itemize}
    \item Following Recursive Feature Elimination (RFE), the final dataset has a shape of \(4070 \times 204\), with 200 features and 4 target variables.
    \item Each data point comprises 200 features (GITs) representing the relative sequence abundance (RSA) of microorganisms. The 4 target variables are continent, city, latitude, and longitude.
    \item The dataset includes samples from 40 unique cities across 7 continents.
    \item Categorical variables (continent and city) were encoded using \texttt{sklearn}'s \texttt{LabelEncoder}, while latitude and longitude were standardized using \texttt{StandardScaler}.
    \item Initial consideration of stratified cross-validation was temporarily replaced with \texttt{train\_test\_split} for initial model development. Stratified cross-validation will be revisited for enhanced model performance.
\end{itemize}

\subsubsection*{Neural Network Architecture}
\begin{itemize}
    \item The initial model is a simple feedforward neural network, inspired by the hierarchical structure of the previous XGBoost study.
    \item The first version includes an input layer (200 nodes), two hidden layers (400 nodes each), a smaller hidden layer (2 nodes), and an output layer (7 nodes for the 7 continents).
    \item The plan is to initially train this model to predict the continent. Subsequently, the predicted continent probabilities will be concatenated with the original 200 features as input for a second neural network (similar architecture) to predict the city.
    \item The optimal handling of latitude and longitude values within the neural network remains an open question.
    \item Long training times due to CPU-based computation on the bioinformatics server are a significant challenge. GPU access on the LUNARC cluster is required.
\end{itemize}

% April 4, 2025
\section{April 4, 2025}

\subsection*{Tasks for the Day}
\begin{itemize}
    \item Finalized the presentation for the weekly lab meeting.
    \item Focused on obtaining GPU access and understanding HPC architecture.
    \item Initiated preprocessing for marine and soil datasets, aiming for modular script design applicable to all datasets.
\end{itemize}

\subsection*{Notes and Observations}
Explored various neural network implementations and gained initial understanding of HPC principles.

% April 7, 2025
\section{April 7, 2025}

\subsection*{Tasks for the Day}
\begin{itemize}
    \item Developed a functional neural network to predict all target variables (continent, city, latitude, and longitude).
    \item Completed preprocessing steps for marine and soil datasets.
    \item Began modularizing code for efficient execution on the HPC.
\end{itemize}

\subsection*{Notes and Observations}
\begin{itemize}
    \item Developed a working script for MetaSUB data processing, with pending error handling and file format validation.
    \item Created a working script to extract relevant features using Recursive Feature Elimination with a Random Forest base model.
    \item Initiated work on the HPC, understanding basic operations and starting to modularize scripts for GPU compute nodes.
\end{itemize}

% April 8, 2025
\section{April 8, 2025}
\subsection*{Notes and Observations}
\begin{itemize}
    \item Started building multiple neural network models.
    \item Began learning batch scripting using SLURM.
\end{itemize}

% April 10, 2025
\section{April 10, 2025}
\subsection*{Notes and Observations}
\begin{itemize}
    \item Exploring alternative scaling methods for latitude and longitude, including conversion to radians and trigonometric transformation into a two-dimensional space.
\end{itemize}

% April 11, 2025
\section{April 11, 2025}
\subsection*{Tasks for the day}
\begin{itemize}
    \item These are some of my thoughts. The currrent approach is always making a new neural network model from scratch for each dataset. A final model should be made 
    that utilizes all the iterations done and must work for all type of datasets regardless of the layers of predictions. 
    \item For example, the MetaSUB dataset contains informaiton on the continent, city, latitude and longitude. Whereas the marine dataset contain information only the sea, latitude and longitude.
    There should be a way that can handle these cases instead of definfing a newtwork from scratch.
    \item Finish the logic for the latitude and longitude neural network model. 
    \item Compare the three models with metasub dataset on accuracy, confusion matrix, plot on world map.
\end{itemize}

\end{document}