\documentclass{article}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{booktabs}

% Header and Footer
\pagestyle{fancy}
\fancyhf{}
\rhead{\textbf{Bioinformatics Research Log}}
\rfoot{Page \thepage}

% Title format
\titleformat{\section}{\Large\bfseries}{}{0pt}{}

\title{\textbf{Bioinformatics Research Project Log}}
\author{Chandrashekar CR \\ Supervisor: Dr. Eran Elhaik \\ Lund University}
\date{March 31, 2025}

\begin{document}

\maketitle
\tableofcontents
\newpage

% March 31, 2025
\section{March 31, 2025}

\subsection*{Tasks for the Day}
\begin{itemize}
    \item Understand the mGPS algorithm from the R code and implement the preprocessing steps. (Done)
    \item Set up the working environment and install all required libraries. (Done)
    \item Begin tracking the project. (Done)
\end{itemize}

\subsection*{Notes and Observations}
\begin{itemize}
    \item I am currently using the \texttt{ai\_env} environment from my previous projects and accessing resources on the bioinformatics server.
    \item I am still struggling to understand the R code, but I expect to progress faster once I grasp the underlying logic.
\end{itemize}

% April 1, 2025
\section{April 1, 2025}

\subsection*{Tasks for the Day}
\begin{itemize}
    \item Understand and implement Recursive Feature Elimination followed by the XGBoost machine learning algorithm with the correct hierarchical steps. (Done)
    \item Initialize Git for version control and push code to GitHub. (Done)
    \item Gain a deeper understanding of cross-validation.
\end{itemize}

\subsection*{Notes and Observations}
\begin{itemize}
    \item I have understood the general workflow and added comments to the MetaSUB preprocessing script in R.
    \item I learned about cross-validation and its importance, though I have not yet implemented it.
\end{itemize}

\subsection*{Pending Tasks}
\begin{itemize}
    \item Git tracking for the project has not been set up yet. (Done)
    \item Preprocessing steps are still in progress.
\end{itemize}

% April 2, 2025
\section{April 2, 2025}

\subsection*{Tasks for the Day}
\begin{itemize}
    \item Ask Vignesh how to access the LUNARC server. (Done)
    \item Reimplement the XGBoost machine learning algorithm with the correct hierarchical steps from the previous day. (I don't need to do that)
\end{itemize}

\subsection*{Notes and Observations}
\begin{itemize}
    \item I initialized Git for the project and started tracking all files except data files and research papers.
    \item The \texttt{metasub\_global\_git.csv} file contains all the Geographically Informative Taxa (GITs) required for the XGBoost model.
    \item I realized that I do not need to recreate the exact model; my primary objective is to understand how the input data for XGBoost is preprocessed.
    \item The key questions to answer: What is the shape of the input data? What are we predicting?
\end{itemize}

\subsection*{Pending Tasks}
\begin{itemize}
    \item Gain access to the LUNARC server. (Done)
\end{itemize}

% April 3, 2025
\section{April 3, 2025}

\subsection*{Tasks for the Day}
\begin{itemize}
    \item Implement basic neural network architectures in PyTorch. The dataset contains \( n \) data points, but hyperparameter tuning has shown that 200 GITs are sufficient for accurate predictions. (Done)
    \item Integrate Ray parallel processing into the hyperparameter optimization process, reducing the search time (currently estimated at 4-5 hours). (Done)
    \item Preprocess the data into numerical format by converting categorical variables (continents and cities) using one-hot encoding or label encoding. One-hot encoding seems to be the safer option. (Done)
    \item Implement stratified K-fold cross-validation before proceeding with a simple neural network model. (Will do it later.)
\end{itemize}

\subsection*{Notes and Observations}
\begin{itemize}
    \item I successfully logged into the LUNARC server’s login node, but I still need to figure out how to access and use the GPUs for training neural networks.
    \item Authentication and login are complete, but I still need assistance in understanding the following:
    \begin{itemize}
        \item \textbf{Where my allocated storage is located?}
        \item \textbf{How to submit jobs using SBATCH.}
        \item \textbf{I need to learn the basics of working on HPC?}
    \end{itemize}
    \item After performing Recursive Feature Elimination (RFE), the final dataset was created with a shape of 4070 × 204. Out of the 204 columns, 200 represent the selected features, while the remaining 4 correspond to the target variables.
    \item Each row in the dataset contains 200 features (GITs) representing the relative sequence abundance (RSA) of microorganisms. The 4 target columns include the continent, city, latitude, and longitude of the sample collection site.
    \item The dataset comprises samples collected from 40 unique cities across 7 continents.
    \item Categorical variables (continent and city) were encoded using \texttt{sklearn}'s \texttt{LabelEncoder}, while latitude and longitude were standardized using \texttt{StandardScaler}.
    \item Initially, stratified cross-validation was considered for training the neural network. However, this approach was temporarily replaced with \texttt{train\_test\_split} for initial model development. Once the basic neural network is functional, stratified cross-validation will be reconsidered for improved model performance.
\end{itemize}

\subsubsection*{Neural Network Architecture}
\begin{itemize}
    \item The initial model is a simple feedforward neural network inspired by the hierarchical structure used in the previous study with XGBoost.
    \item The first version of the network includes an input layer with 200 nodes (corresponding to the 200 selected features), followed by two hidden layers with 400 nodes each. This is followed by a smaller hidden layer with 2 nodes, and finally, an output layer with 7 nodes representing the 7 target continents.
    \item The plan is to first train this model to predict the continent. Then, I will use the predicted probabilities of the continents as additional features by concatenating them with the original 200 input features. This combined input will be used in a second neural network (with a similar architecture) to predict the city.
    \item I am still unsure how to best handle the latitude and longitude values within the neural network, and this remains an open issue to resolve.
    \item One major challenge I’ve encountered is the long runtime during training. As I am currently working on the bioinformatics server, which only supports CPU-based computation, training is slow. I require access to GPUs hosted on the LUNARC cluster and need guidance on how to proceed with that setup.
\end{itemize}

% April 4, 2025
\section{April 4, 2025}

\subsection*{Tasks for the Day}
\begin{itemize}
    \item Finish presentation today for the weekly lab meeting.
    \item Get GPUs access and understand HPC working architecture. 
    \item Start the pre-processing for marine and soil data. The pre-processing for metasub, marine and soil data should be written in a modular format for easier excecution of the scripts.
\end{itemize}

\subsection*{Notes and Observations}
Read about different implemetation of neural networks. Also did some reading and understanding about HPC.

% April 7, 2025
\section{April 7, 2025}

\subsection*{Tasks for the Day}
\begin{itemize}
    \item Write a proper working neural netowrk that predicts all the target variables (continent, city, latitiude and longitude).
    \item Finish preprocessing steps for marine dataset and soil dataset.
    \item Start modularizing all the code so that it can be easily run on the HPC.
\end{itemize}

\subsection*{Notes and Observations}
\begin{itemize}
    \item Wrote a working script for processing metasub data. I need to do error handling and add function to check if the file format is correct. For now the script works and the preprocessing step also works.
    \item Wrote a working script to get valid information from the entire dataset using recursive feature elimination, with the base model as random forest.
    \item Started working the HPC. I understood the basics and now I have started to modularize the script for running them on the HPC GPU compute nodes.
\end{itemize}
\end{document}
