\section{Materials and Methods}
\subsection{Global Datasets}
\subsection{Microorganism DIstribution and Pathogenicity}
\subsection{mGPS Implementation}
\subsubsection{QC Procedure}
\begin{itemize}
    \item \textbf{Objective:} To identify informative, discriminating, and independent microbial taxa (Geographically Informative Taxa or GITs) for accurate geographic location prediction.
    \item \textbf{Data Preparation:} Datasets, encompassing microbial relative sequence abundance (RSA) from various geographic origins (cities, stations, countries, oceanic bodies) were utilized.
    Each dataset was randomly partitioned into training (80\%) and testing (20\%) subsets.
    \item \textbf{Recursive Feature Elimination (RFE) with Random Forest:} An initial random forest classifier was trained on the training data, using all avilable taxa as predictors.
    The predictive accuracy of th classifier was assessed using the testing data. Out-of-bag (OOB) prediction accuracy was recorded, and feature importance was calculated by permuting each variable and measuring the impact on prediction accuracy.
    Variables were ranked in descending order of importance based on this metric.
    \item \textbf{Iterative Subset Selection:} Five subsets of predictors variables () were iteratively created, representing the \textit{i} most important variables. The model
    was re-trained for each subset, and predictive accuracy was evaluated on the testing set. This process was repeated five times with different training/testing splits to mitigate selection bias.
    \item \textbf{Optimal Subset Determination:} The subset size that yielded the highest average classification accuracy was selected as the optimal size. The optimal number of features varied by dataset, for example
    200 features were optimal for sail and marine data.
    \item \textbf{GIT Identification:} The most infromative variables, corresponding to the \textit{i} highest average importance values were designated as GITs.
    \item  \textbf{Variable importance visualization:} Variable importance plots were generated to illustrate the contribution of each GIT to model prediction accuracy.
    \item \textbf{mGPS integration and user customization:} The feature selection process is integrated into the mGPS framework. 
\end{itemize}
\subsubsection{Model Training}
\begin{enumerate}
    \item \textbf{Hierarchial Geographical Prediction Framework:}
    \begin{itemize}
        \item mGPS is desgined to exploit the inherent hierarchical structure of geographic data. This structure, exmemplified by continent, country, city, station
    and precise coordinates (latitude and longitude), provides a natural ordering for prediction.
        \item The model employs a chained series of gradient-boosted decision trees (GBDT) to navigate this hierarchy, progressively refining location predictions.       
    \end{itemize}
    \item \textbf{Chained Submodel Architecture:}
    \begin{itemize}
        \item The model comprises a series of interconnected submodel (M1,M2,M3,M4) each dedicated to predicting a specific level of geographical hierarchy.
        \item  This chaining mechanism allows the model to leverage dependencies between hierarchical levels. For instance, the predicted continent from M1 influences the subsequent
        prediction of the country or city by M2, and so on.
        \item For local studies, where the continental level is not relevant, the M1 submodel is excluded.
    \end{itemize}
    \item \textbf{Input Data and Feature Integration:}
    \begin{itemize}
        \item The initial input to M1 is a vector containing the relative sequence abundance (RSA) of the selected Geographically Informative Taxa (GITs).
        \item Subsequent submodels (M2, M3, M4) recieve as input the original RSA vector augmented with the predicted class probabilitis from the preceding submodel. This iterative input augmentation ensures that the model incorporates infromation from previous prediction stages.
    \end{itemize}
    \item \textbf{Submodel Functionality and Prediction Types:}
    \begin{itemize}
        \item M1 is a classification model that predicts the highest-level geographical category (e.g., continent).
        \item M2, also a classification model, predicts the next level of granularity (e.g., country, city or transit station.)
        \item M3 and M4 are regression models that predict the continuous latitude and longitude, respectively.
    \end{itemize}
    \item \textbf{Gradient-Boosted Decision Trees (GBDT) with XGBoost:}
    \begin{itemize}
        \item Each submodel utilizes the GBDT algorithm, a powerful ensemble learning methods that builds decision trees iteratively.
        \item The XGBoost library, an optimized implementation of GBDT, is employed to enhance computational efficiency and model perfromace. XGBoost is known for its speed and ability to handle large datasets.
        \item GBDT operates by sequentially constructing decision trees, where each subsequent tree aims to correct the erros made by its predecessors.
        This iterative process refines the model's predictive accuracy.
    \end{itemize}
        \item The XGBoost algorithm requires careful tuning of several hyperparameters, including:
    \begin{itemize}
        \item learing rate
        \item maximum depth of trees
    \end{itemize}
    \item The final model performance is evaluated based on predictions generated for the unseen test datasets.
    \item All downstream analyses, including the analysis of AMR transfer patterns, are performed exclusively on these unseen test datasets, ensuring unbiased results.
    \item The entire training procedure, including hyperparameter optimization, is repeated for each dataset indenpendently to account for dataset-specific characteristics and optimze performance.   
\end{enumerate}
