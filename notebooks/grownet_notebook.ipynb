{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is all about just mimicking the eaxt architecture of grownet using the kaggle notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, QuantileTransformer, OneHotEncoder, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"feat_d\": 200,\n",
    "    \"hidden_size\": 128,\n",
    "    \"n_classes\": 7,\n",
    "    \"num_nets\": 20,\n",
    "    \"boost_rate\": 0.05,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs_per_stage\": 10,\n",
    "    \"correct_epoch\": 3,\n",
    "    \"early_stopping_steps\": 10,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the data to feed it into the neural network\n",
    "def process_data(data_path):\n",
    "\n",
    "    try:\n",
    "        in_data = pd.read_csv(data_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {data_path}\")\n",
    "\n",
    "\n",
    "    # Initialize label and scalers\n",
    "    le_continent = LabelEncoder()\n",
    "    le_city = LabelEncoder()\n",
    "    stdscaler_lat = StandardScaler() \n",
    "    stdscaler_long = StandardScaler() \n",
    "    coordinate_scaler = StandardScaler()\n",
    "\n",
    "    \n",
    "    # Convert all the categorical variables into numbers\n",
    "    in_data['city_encoding'] = in_data[['city']].apply(le_city.fit_transform)\n",
    "    in_data['continent_encoding'] = in_data[['continent']].apply(le_continent.fit_transform)\n",
    "    in_data['lat_scaled'] = stdscaler_lat.fit_transform(in_data[['latitude']])\n",
    "    in_data['long_scaled'] = stdscaler_long.fit_transform(in_data[['longitude']])\n",
    "\n",
    "    \n",
    "    # Another way of scaling latitiude and longitude data to avoid exploding gradient problem.\n",
    "    # https://datascience.stackexchange.com/questions/13567/ways-to-deal-with-longitude-latitude-feature \n",
    "    # Convert latitude and longitutde into radians\n",
    "    in_data['latitude_rad'] = np.deg2rad(in_data['latitude'])\n",
    "    in_data['longitude_rad'] = np.deg2rad(in_data['longitude'])\n",
    "\n",
    "    # Calculate x, y, z coordinates -  Converting polar co-ordinates into cartesian co-ordinates\n",
    "    in_data['x'] = np.cos(in_data['latitude_rad']) * np.cos(in_data['longitude_rad'])\n",
    "    in_data['y'] = np.cos(in_data['latitude_rad']) * np.sin(in_data['longitude_rad'])\n",
    "    in_data['z'] = np.sin(in_data['latitude_rad'])\n",
    "\n",
    "    # Scale the x, y, z coordinates together\n",
    "    in_data[['scaled_x','scaled_y','scaled_z']] = coordinate_scaler.fit_transform (in_data[['x','y','z']])\n",
    "\n",
    "    # Encoding dictionary for simpler plotting and understanding the results\n",
    "    continent_encoding_map = dict(zip(le_continent.transform(le_continent.classes_), le_continent.classes_))\n",
    "    city_encoding_map = dict(zip(le_city.transform(le_city.classes_),le_city.classes_))\n",
    "\n",
    "    # Define all non-feature columns\n",
    "    non_feature_columns = [\n",
    "        'city', 'continent', 'latitude', 'longitude', # Original identifier/target columns\n",
    "        'city_encoding', 'continent_encoding', # Encoded categorical targets\n",
    "        'lat_scaled', 'long_scaled', # Old scaled lat/long (if not used as features)\n",
    "        'latitude_rad', 'longitude_rad', # Intermediate radian values\n",
    "        'x', 'y', 'z', # Intermediate cartesian coordinates\n",
    "        'scaled_x', 'scaled_y', 'scaled_z','Unnamed: 0' # Final XYZ targets\n",
    "    ]\n",
    "\n",
    "    # Select X by dropping non-feature columns\n",
    "    # Use errors='ignore' in case some columns don't exist (e.g., if you only keep one scaling method)\n",
    "    X = in_data.drop(columns=non_feature_columns, errors='ignore').values.astype(np.float32)\n",
    "\n",
    "    # Define target columns explicitly\n",
    "    y_columns = ['continent_encoding', 'city_encoding', 'scaled_x','scaled_y','scaled_z']\n",
    "    y = in_data[y_columns].values.astype(np.float32)\n",
    "\n",
    "    return in_data, X, y, le_continent, le_city, coordinate_scaler, continent_encoding_map, city_encoding_map\n",
    "\n",
    "# Inverse transform xyz cordinates into latitude and longitude values\n",
    "def inverse_transform_spherical(scaled_xyz, coordinate_scaler):\n",
    "    \"\"\"Inverse transforms scaled x, y, z back to latitude and longitude (degrees).\"\"\"\n",
    "    xyz = coordinate_scaler.inverse_transform(scaled_xyz)\n",
    "    x = xyz[:, 0]\n",
    "    y = xyz[:, 1]\n",
    "    z = xyz[:, 2]\n",
    "    latitude_rad = np.arcsin(np.clip(z, -1, 1))\n",
    "    longitude_rad = np.arctan2(y, x)\n",
    "    latitude_deg = np.degrees(latitude_rad)\n",
    "    longitude_deg = np.degrees(longitude_rad)\n",
    "    return latitude_deg, longitude_deg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4070, 200)\n",
      "['east_asia' 'europe' 'middle_east' 'north_america' 'oceania'\n",
      " 'south_america' 'sub_saharan_africa']\n",
      "{np.int64(0): 'east_asia', np.int64(1): 'europe', np.int64(2): 'middle_east', np.int64(3): 'north_america', np.int64(4): 'oceania', np.int64(5): 'south_america', np.int64(6): 'sub_saharan_africa'}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/chandru/binp37/results/metasub/metasub_training_testing_data.csv\")\n",
    "df = pd.concat([df.iloc[:,:-4],df['continent']],axis=1)\n",
    "x_data = df[df.columns[:-1]][:].to_numpy()\n",
    "print(x_data.shape)\n",
    "y_data = df[df.columns[-1]][:].to_numpy()\n",
    "le = LabelEncoder()\n",
    "y_data = le.fit_transform(y_data)\n",
    "print(le.classes_)\n",
    "\n",
    "continent_encoding_map = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "print(continent_encoding_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        dct = {\n",
    "            'x': torch.tensor(self.features[idx,:],dtype=torch.float),\n",
    "            'y': torch.tensor(self.targets[idx,:],dtype=torch.float)\n",
    "        }\n",
    "        return dct\n",
    "\n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class ForwardType(Enum):\n",
    "    SIMPLE = 0\n",
    "    STACKED = 1\n",
    "    CASCADE = 2\n",
    "    GRADIENT = 3\n",
    "\n",
    "class DynamicNet(object):\n",
    "    def __init__(self, c0, lr):\n",
    "        self.models = []\n",
    "        self.c0 = c0\n",
    "        self.lr = lr\n",
    "        self.boost_rate  = nn.Parameter(torch.tensor(lr, requires_grad=True))\n",
    "\n",
    "    def to(self, device):\n",
    "        self.c0 = self.c0.to(device)\n",
    "        self.boost_rate = self.boost_rate.to(device)\n",
    "        for m in self.models:\n",
    "            m.to(device)\n",
    "\n",
    "    def add(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for m in self.models:\n",
    "            params.extend(m.parameters())\n",
    "\n",
    "        params.append(self.boost_rate)\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for m in self.models:\n",
    "            m.zero_grad()\n",
    "\n",
    "    def to_cuda(self):\n",
    "        for m in self.models:\n",
    "            m.cuda()\n",
    "\n",
    "    def to_eval(self):\n",
    "        for m in self.models:\n",
    "            m.eval()\n",
    "\n",
    "    def to_train(self):\n",
    "        for m in self.models:\n",
    "            m.train(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(self.models) == 0:\n",
    "            batch = x.shape[0]\n",
    "            c0 = np.repeat(self.c0.detach().cpu().numpy().reshape(1,-1), batch, axis=0)\n",
    "            return None, torch.Tensor(c0).to(device=device)\n",
    "        middle_feat_cum = None\n",
    "        prediction = None\n",
    "        with torch.no_grad():\n",
    "            for m in self.models:\n",
    "                if middle_feat_cum is None:\n",
    "                    middle_feat_cum, prediction = m(x, middle_feat_cum)\n",
    "                else:\n",
    "                    middle_feat_cum, pred = m(x, middle_feat_cum)\n",
    "                    prediction += pred\n",
    "        return middle_feat_cum, self.c0 + self.boost_rate * prediction\n",
    "\n",
    "    def forward_grad(self, x):\n",
    "        if len(self.models) == 0:\n",
    "            batch = x.shape[0]\n",
    "            c0 = np.repeat(self.c0.detach().cpu().numpy().reshape(1, -1), batch, axis=0)\n",
    "            return None, torch.Tensor(c0).cuda()\n",
    "        # at least one model\n",
    "        middle_feat_cum = None\n",
    "        prediction = None\n",
    "        for m in self.models:\n",
    "            if middle_feat_cum is None:\n",
    "                middle_feat_cum, prediction = m(x, middle_feat_cum)\n",
    "            else:\n",
    "                middle_feat_cum, pred = m(x, middle_feat_cum)\n",
    "                prediction += pred\n",
    "        return middle_feat_cum, self.c0 + self.boost_rate * prediction\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, path, builder):\n",
    "        d = torch.load(path)\n",
    "        net = DynamicNet(d['c0'], d['lr'])\n",
    "        net.boost_rate = d['boost_rate']\n",
    "        for stage, m in enumerate(d['models']):\n",
    "            submod = builder(stage)\n",
    "            submod.load_state_dict(m)\n",
    "            net.add(submod)\n",
    "        return net\n",
    "\n",
    "    def to_file(self, path):\n",
    "        models = [m.state_dict() for m in self.models]\n",
    "        d = {'models': models, 'c0': self.c0, 'lr': self.lr, 'boost_rate': self.boost_rate}\n",
    "        torch.save(d, path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_1HL(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden1, dim_out):\n",
    "        super(MLP_1HL, self).__init__()\n",
    "        \n",
    "        # Layer 1: Input -> Hidden\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(dim_in,dim_hidden1),\n",
    "            nn.BatchNorm1d(dim_hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)            \n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "           nn.Linear(dim_hidden1,dim_out)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, lower_f):\n",
    "        if lower_f is not None:\n",
    "            x = torch.cat([x, lower_f], dim=1)\n",
    "        out = self.layer1(x)\n",
    "        return out, self.layer2(out)\n",
    "\n",
    "    @classmethod\n",
    "    def get_model(cls, stage, params):\n",
    "        if stage == 0:\n",
    "            dim_in = params[\"feat_d\"]\n",
    "        else:\n",
    "            dim_in = params[\"feat_d\"] + params[\"hidden_size\"]\n",
    "        model = MLP_1HL(dim_in, params[\"hidden_size\"], params[\"hidden_size\"])\n",
    "        return model\n",
    "\n",
    "\n",
    "class MLP_2HL(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden1, dim_hidden2,dim_out=7, sparse=False, bn=True):\n",
    "        super(MLP_2HL, self).__init__()\n",
    "        self.bn2 = nn.BatchNorm1d(dim_in)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(dim_in, dim_hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(dim_hidden1),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(dim_hidden1, dim_hidden2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden2, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lower_f):\n",
    "        if lower_f is not None:\n",
    "            x = torch.cat([x, lower_f], dim=1)\n",
    "            x = self.bn2(x)\n",
    "\n",
    "        middle_feat = self.layer1(x)\n",
    "        out = self.layer2(middle_feat)\n",
    "        return middle_feat, out\n",
    "\n",
    "    @classmethod\n",
    "    def get_model(cls, stage, params):\n",
    "        if stage == 0:\n",
    "            dim_in = params[\"feat_d\"]\n",
    "        else:\n",
    "            dim_in = params[\"feat_d\"] + params[\"hidden_size\"]\n",
    "        model = MLP_2HL(dim_in, params[\"hidden_size\"], params[\"hidden_size\"])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optim(params, lr, weight_decay):\n",
    "    optimizer = optim.Adam(params, lr, weight_decay=weight_decay)\n",
    "    #optimizer = SGD(params, lr, weight_decay=weight_decay)\n",
    "    return optimizer\n",
    "\n",
    "def logloss(net_ensemble, test_loader):\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    loss_f = nn.BCEWithLogitsLoss() # Binary cross entopy loss with logits, reduction=mean by default\n",
    "    for data in test_loader:\n",
    "        x = data[\"x\"].to(device)\n",
    "        y = data[\"y\"].to(device)\n",
    "        # y = (y + 1) / 2\n",
    "        with torch.no_grad():\n",
    "            _, out = net_ensemble.forward(x)\n",
    "        # out = torch.as_tensor(out, dtype=torch.float32).cuda().view(-1, 1)\n",
    "        loss += loss_f(out, y)\n",
    "        total += 1\n",
    "\n",
    "    return loss / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3256, Val size: 814\n"
     ]
    }
   ],
   "source": [
    "y_onehot = np.eye(params[\"n_classes\"])[y_data]\n",
    "\n",
    "\n",
    "# Split intp train and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_onehot, test_size=0.2, random_state=42, stratify=y_onehot)\n",
    "\n",
    "train_ds = TrainDataset(X_train, y_train)\n",
    "val_ds = TrainDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=params[\"batch_size\"],shuffle=False)\n",
    "\n",
    "print(f\"Train size: {len(train_ds)}, Val size: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Logloss: 0.33429235219955444\n"
     ]
    }
   ],
   "source": [
    "# Initialiaze GrowNet\n",
    "c0 = torch.tensor(np.log(np.mean(y_train, axis=0)), dtype=torch.float).unsqueeze(0).to(device)\n",
    "\n",
    "net_ensemble = DynamicNet(c0, params[\"boost_rate\"])\n",
    "net_ensemble.to(device)\n",
    "\n",
    "loss_stagewise = nn.MSELoss(reduction=\"none\")\n",
    "loss_corrective = SmoothBCEwLogits(smoothing=0.001, reduction=\"None\")\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_stage = 0\n",
    "early_stop = 0\n",
    "lr = params[\"lr\"]\n",
    "\n",
    "print(\"Initial Logloss:\", logloss(net_ensemble,val_loader).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training weak learner 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Training weak learner \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[33m\"\u001b[39m\u001b[33mnum_nets\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m model = MLP_2HL.get_model(stage,params).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m optimizer = \u001b[43mget_optim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mget_optim\u001b[39m\u001b[34m(params, lr, weight_decay)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_optim\u001b[39m(params, lr, weight_decay):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     optimizer = \u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m#optimizer = SGD(params, lr, weight_decay=weight_decay)\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/optim/adam.py:100\u001b[39m, in \u001b[36mAdam.__init__\u001b[39m\u001b[34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[39m\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTensor betas[1] must be 1-element\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m defaults = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     88\u001b[39m     lr=lr,\n\u001b[32m     89\u001b[39m     betas=betas,\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m     decoupled_weight_decay=decoupled_weight_decay,\n\u001b[32m     99\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/optim/optimizer.py:369\u001b[39m, in \u001b[36mOptimizer.__init__\u001b[39m\u001b[34m(self, params, defaults)\u001b[39m\n\u001b[32m    366\u001b[39m     param_groups = [{\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: param_groups}]\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;28mself\u001b[39m._warned_capturable_if_run_uncaptured = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/_compile.py:46\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m disable_fn = \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[33m\"\u001b[39m\u001b[33m__dynamo_disable\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\n\u001b[32m     48\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     49\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/_dynamo/__init__.py:53\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m graph_break_reasons, guard_failures, orig_code_map, reset_frame_count\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Register polyfill functions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolyfills\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m loader \u001b[38;5;28;01mas\u001b[39;00m _  \u001b[38;5;66;03m# usort: skip # noqa: F401\u001b[39;00m\n\u001b[32m     56\u001b[39m __all__ = [\n\u001b[32m     57\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_in_graph\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     58\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massume_constant_result\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     83\u001b[39m ]\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# allowlist this for weights_only load of NJTs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/_dynamo/polyfills/loader.py:25\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# See also the TYPE_CHECKING block in torch/_dynamo/polyfills/__init__.py\u001b[39;00m\n\u001b[32m     15\u001b[39m POLYFILLED_MODULE_NAMES: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, ...] = (\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbuiltins\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfunctools\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m POLYFILLED_MODULES: \u001b[38;5;28mtuple\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mModuleType\u001b[39m\u001b[33m\"\u001b[39m, ...] = \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msubmodule\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolyfills\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPOLYFILLED_MODULE_NAMES\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Unregister the builtin functions from _builtin_function_ids to let them to be\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# dispatched with the appropriate VariableTracker type. Otherwise, they will be\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# dispatched with BuiltinVariable if present in _builtin_function_ids.\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m polyfill_module \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/_dynamo/polyfills/loader.py:26\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# See also the TYPE_CHECKING block in torch/_dynamo/polyfills/__init__.py\u001b[39;00m\n\u001b[32m     15\u001b[39m POLYFILLED_MODULE_NAMES: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, ...] = (\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbuiltins\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfunctools\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m )\n\u001b[32m     25\u001b[39m POLYFILLED_MODULES: \u001b[38;5;28mtuple\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mModuleType\u001b[39m\u001b[33m\"\u001b[39m, ...] = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msubmodule\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolyfills\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m submodule \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULE_NAMES\n\u001b[32m     28\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Unregister the builtin functions from _builtin_function_ids to let them to be\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# dispatched with the appropriate VariableTracker type. Otherwise, they will be\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# dispatched with BuiltinVariable if present in _builtin_function_ids.\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m polyfill_module \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/importlib/__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/_dynamo/polyfills/builtins.py:30\u001b[39m\n\u001b[32m     19\u001b[39m __all__ = [\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33many\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33menumerate\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m ]\n\u001b[32m     27\u001b[39m _T = TypeVar(\u001b[33m\"\u001b[39m\u001b[33m_T\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;129;43m@substitute_in_graph\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_constant_fold_through\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/_dynamo/decorators.py:427\u001b[39m, in \u001b[36msubstitute_in_graph.<locals>.wrapper\u001b[39m\u001b[34m(traceable_fn)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(original_fn) \u001b[38;5;129;01min\u001b[39;00m _polyfilled_function_ids:\n\u001b[32m    425\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDuplicate polyfilled object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m rule_map: \u001b[38;5;28mdict\u001b[39m[Any, \u001b[38;5;28mtype\u001b[39m[VariableTracker]] = \u001b[43mget_torch_obj_rule_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m original_fn \u001b[38;5;129;01min\u001b[39;00m rule_map:\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    430\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDuplicate object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with different rules: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    431\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPolyfilledFunctionVariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrule_map[original_fn]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    432\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/_dynamo/trace_rules.py:2870\u001b[39m, in \u001b[36mget_torch_obj_rule_map\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   2868\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m m.items():  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m   2869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m.py#\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k:\n\u001b[32m-> \u001b[39m\u001b[32m2870\u001b[39m         obj = \u001b[43mload_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2871\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2872\u001b[39m         obj = _module_dir(torch) + k[\u001b[38;5;28mlen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtorch/\u001b[39m\u001b[33m\"\u001b[39m) :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/_dynamo/trace_rules.py:2901\u001b[39m, in \u001b[36mload_object\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   2899\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2900\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) == \u001b[32m1\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid obj name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2901\u001b[39m         val = \u001b[43m_load_obj_from_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2902\u001b[39m     val = unwrap_if_wrapper(val)\n\u001b[32m   2903\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/_dynamo/trace_rules.py:2885\u001b[39m, in \u001b[36m_load_obj_from_str\u001b[39m\u001b[34m(fully_qualified_name)\u001b[39m\n\u001b[32m   2883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_obj_from_str\u001b[39m(fully_qualified_name):\n\u001b[32m   2884\u001b[39m     module, obj_name = fully_qualified_name.rsplit(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, maxsplit=\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2885\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m, obj_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/importlib/__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/_higher_order_ops/map.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DispatchKey\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dispatch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m suspend_functionalization\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maot_autograd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AOTConfig, create_joint\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_higher_order_ops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     _has_potential_branch_input_alias,\n\u001b[32m      9\u001b[39m     _has_potential_branch_input_mutation,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     UnsupportedAliasMutationException,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HigherOrderOperator\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py:135\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_aot_autograd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraced_function_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    113\u001b[39m     aot_dispatch_subclass,\n\u001b[32m    114\u001b[39m     create_functional_call,\n\u001b[32m   (...)\u001b[39m\u001b[32m    119\u001b[39m     fn_prepped_for_autograd,\n\u001b[32m    120\u001b[39m )\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_aot_autograd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    122\u001b[39m     _get_autocast_states,\n\u001b[32m    123\u001b[39m     _get_symint_hints,\n\u001b[32m   (...)\u001b[39m\u001b[32m    133\u001b[39m     strict_zip,\n\u001b[32m    134\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartitioners\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m default_partition\n\u001b[32m    138\u001b[39m \u001b[38;5;28mzip\u001b[39m = strict_zip\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# This global counter increments every time we compile a graph with\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# AOTAutograd.  You can use this to correlate runtime error messages\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# with compile time (e.g., if you get an error at runtime saying\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# one counter is allocated per entire compiled block (but this block\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# may involve compiling multiple subgraphs; e.g., for forwards/backwards)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/_functorch/partitioners.py:37\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CheckpointPolicy\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_activation_checkpointing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_info_provider\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphInfoProvider\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_activation_checkpointing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mknapsack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     39\u001b[39m     dp_knapsack,\n\u001b[32m     40\u001b[39m     greedy_knapsack,\n\u001b[32m     41\u001b[39m     ilp_knapsack,\n\u001b[32m     42\u001b[39m )\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_activation_checkpointing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mknapsack_evaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KnapsackEvaluator\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/torch/_functorch/_activation_checkpointing/graph_info_provider.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Optional\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnx\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Graph, Node\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mGraphInfoProvider\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/networkx/__init__.py:46\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreadwrite\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Need to test with SciPy, when available\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m algorithms\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m linalg\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/networkx/algorithms/__init__.py:65\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwiener\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Make certain subpackages available to the user as direct imports from\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# the `networkx` namespace.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m approximation\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m assortativity\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bipartite\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/binp37_env/lib/python3.13/site-packages/networkx/algorithms/approximation/__init__.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapproximation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mramsey\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapproximation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msteinertree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapproximation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraveling_salesman\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapproximation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtreewidth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapproximation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvertex_cover\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1022\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1118\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1218\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for stage in range(params[\"num_nets\"]):\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(f\"\\n Training weak learner {stage+1}/{params[\"num_nets\"]}\")\n",
    "\n",
    "    model = MLP_2HL.get_model(stage,params).to(device)\n",
    "    optimizer = get_optim(model.parameters(), lr, params[\"weight_decay\"])\n",
    "    net_ensemble.to_train()\n",
    "\n",
    "    stage_train_losses = []\n",
    "\n",
    "    for epoch in range(params[\"epochs_per_stage\"]):\n",
    "        for batch in train_loader:\n",
    "            x = batch[\"x\"].to(device)\n",
    "            y = batch[\"y\"].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, out_prev = net_ensemble.forward(x)\n",
    "                h = 1 / ((1 + torch.exp(y * out_prev)) * (1 + torch.exp(-y * out_prev)))\n",
    "                grad_direction = y * (1 + torch.exp(-y * out_prev))\n",
    "            \n",
    "            middle_feat, out = model(x, None if stage == 0 else net_ensemble.forward_grad(x)[0])\n",
    "            loss = loss_stagewise(net_ensemble.boost_rate*out, grad_direction)\n",
    "            loss = (loss*h).mean()\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            stage_train_losses.append(loss.item())\n",
    "        \n",
    "    net_ensemble.add(model)\n",
    "    avg_stage_loss = np.mean(stage_train_losses)\n",
    "    print(f\"Stage {stage+1} finished | Avg Train Loss: {avg_stage_loss:.5f} | Time: {time.time() - t0:.1f}s\")\n",
    "\n",
    "\n",
    "    # Corrective step\n",
    "    if stage > 0:\n",
    "        if stage % 3 == 0:\n",
    "            lr /= 2\n",
    "        corrective_optimizer = get_optim(net_ensemble.parameters(), lr/2, params[\"weight_decay\"])\n",
    "        corrective_losses = []\n",
    "\n",
    "        for _ in range(params[\"correct_epoch\"]):\n",
    "            for batch in train_loader:\n",
    "                x = batch[\"x\"].to(device)\n",
    "                y = batch[\"y\"].to(device)\n",
    "\n",
    "                _, out = net_ensemble.forward_grad(x)\n",
    "                loss = loss_corrective(out,y).mean()\n",
    "                corrective_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                corrective_optimizer.step()\n",
    "                corrective_losses.append(loss.item())\n",
    "        print(f\"Fully corrective step avg losse: {np.mean(corrective_losses):.3f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss = logloss(net_ensemble, val_loader).item()\n",
    "    print(f\"Validation LogLoss: {val_loss:.5f} | Boost rate: {net_ensemble.boost_rate.item():.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_stage = stage\n",
    "        early_stop = 0\n",
    "    else:\n",
    "        early_stop += 1\n",
    "        if early_stop > params[\"early_stopping_steps\"]:\n",
    "            print(\" Early stopping!\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nBest model was at stage {best_stage+1} with Val LogLoss: {best_val_loss:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binp37_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
