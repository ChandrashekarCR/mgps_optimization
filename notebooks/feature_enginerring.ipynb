{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from Bio import Entrez\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from Bio import Entrez\n",
    "import gzip\n",
    "import subprocess \n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data \n",
    "df = pd.read_csv(\"/home/chandru/binp37/results/metasub/processed_metasub.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_df = pd.read_csv(\"/home/chandru/binp37/results/metasub/metasub_training_testing_data.csv\")\n",
    "rfe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geograpical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = df[['city_total_population','city_population_density',\n",
    "                  'city_land_area_km2','city_ave_june_temp_c','city_elevation_meters','city_koppen_climate','continent','city','latitude','longitude']]\n",
    "\n",
    "# Fix city elevation of hanoi, yamaguchi in meters\n",
    "feature_data.loc[feature_data['city'] == 'hanoi','city_elevation_meters'] = 12\n",
    "feature_data.loc[feature_data['city'] == 'yamaguchi','city_elevation_meters'] = 23\n",
    "feature_data.loc[feature_data['city'] == 'marseille','city_elevation_meters'] = 42 # city elevation of marseille on google is 42 m here it is 0\n",
    "\n",
    "# Get city population density, city ladn ares in km2, city avg temp in june and city elevation in meters of offa \n",
    "offa_data = {\n",
    "    'city_population_density': 2500.0,\n",
    "    'city_land_area_km2': 74.0,\n",
    "    'city_ave_june_temp_c': 28.0,\n",
    "    'city_elevation_meters': 457.0\n",
    "}\n",
    "\n",
    "feature_data.loc[feature_data['city'] == 'offa', list(offa_data.keys())] = list(offa_data.values())\n",
    "\n",
    "# Get city land area in km2 of marseille  \n",
    "feature_data.loc[feature_data['city'] == 'marseille','city_land_area_km2'] = 240\n",
    "\n",
    "# Fix all the nan values of london\n",
    "london_data = {\n",
    "    'city_total_population': 8787892.0,\n",
    "    'city_population_density': 5590.0,\n",
    "    'city_land_area_km2': 1572.0,\n",
    "    'city_ave_june_temp_c': 14.4,\n",
    "    'city_elevation_meters': 11.0,\n",
    "    'city_koppen_climate': 'marine_west_coast_climate'\n",
    "}\n",
    "feature_data.loc[feature_data['city'] == 'london', list(london_data.keys())] = list(london_data.values())\n",
    "\n",
    "\n",
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for skewness in the data before appling long transformer -> \n",
    "# Note to self: The city_land_area_km2 is right skewed, so we will go with log scale transformation\n",
    "#             : The city_elevation_meters is multi modal there we will go with QuantileTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, QuantileTransformer, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define input columns\n",
    "#log_cols = ['city_land_area_km2']\n",
    "#quantile_cols = ['city_elevation_meters']\n",
    "scale_cols = ['city_total_population', 'city_ave_june_temp_c']\n",
    "cat_cols = ['city_koppen_climate']\n",
    "\n",
    "# Step 2: Log-transform function\n",
    "#def safe_log1p(x):\n",
    "#    return np.log1p(np.maximum(x, 0))\n",
    "\n",
    "# Step 3: Create log pipeline\n",
    "#log_pipeline = Pipeline([\n",
    "#    ('log', FunctionTransformer(safe_log1p)),\n",
    "#    ('scale', StandardScaler())\n",
    "#])\n",
    "\n",
    "# Step 4: Build the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "#    ('log', log_pipeline, log_cols),\n",
    "#    ('quantile', QuantileTransformer(output_distribution='normal'), quantile_cols),\n",
    "    ('scale', StandardScaler(), scale_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "])\n",
    "\n",
    "# Step 5: Fit and transform\n",
    "geo_features_processed = preprocessor.fit_transform(feature_data)\n",
    "\n",
    "# Step 6: Extract column names correctly\n",
    "output_feature_names = []\n",
    "\n",
    "for name, transformer, cols in preprocessor.transformers_:\n",
    "    if name == 'cat':\n",
    "        # For OneHotEncoder\n",
    "        encoder = transformer\n",
    "        if isinstance(encoder, Pipeline):\n",
    "            encoder = encoder.named_steps['onehot']\n",
    "        cats = encoder.categories_[0]\n",
    "        output_feature_names.extend([f\"{cols[0]}_{cat}\" for cat in cats])\n",
    "    else:\n",
    "        output_feature_names.extend(cols)\n",
    "\n",
    "# Step 7: Convert to DataFrame\n",
    "geo_features_df = pd.DataFrame(geo_features_processed)\n",
    "\n",
    "# Step 8: Merge with main features (RFE-selected ones)\n",
    "final_df = pd.concat([rfe_df, geo_features_df], axis=1)\n",
    "final_df.to_csv(\"/home/chandru/binp37/results/metasub/metasub_geo_training_testing.csv\", index=False)\n",
    "\n",
    "print(\"Final dataset shape:\", final_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Select your input columns\n",
    "scale_cols = ['city_ave_june_temp_c']\n",
    "#cat_cols = ['city_koppen_climate']\n",
    "\n",
    "# Step 2: Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale', StandardScaler(), scale_cols),\n",
    "#        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 3: Fit and transform the geo feature data\n",
    "geo_features_processed = preprocessor.fit_transform(feature_data)\n",
    "\n",
    "# Step 4: Get feature names\n",
    "feature_names = []\n",
    "\n",
    "# Handle scaled columns\n",
    "feature_names.extend(scale_cols)\n",
    "\n",
    "# Handle one-hot columns\n",
    "#ohe = preprocessor.named_transformers_['cat']\n",
    "#cat_feature_names = ohe.get_feature_names_out(cat_cols)\n",
    "#feature_names.extend(cat_feature_names)\n",
    "\n",
    "# Step 5: Convert to DataFrame\n",
    "geo_features_df = pd.DataFrame(geo_features_processed.toarray() if hasattr(geo_features_processed, 'toarray') else geo_features_processed)\n",
    "\n",
    "# Step 6: Merge with selected features and save\n",
    "final_df = pd.concat([rfe_df.reset_index(drop=True), geo_features_df.reset_index(drop=True)], axis=1)\n",
    "#final_df.to_csv(\"/home/chandru/binp37/results/metasub/metasub_geo_training_testing.csv\", index=False)\n",
    "\n",
    "print(\"Final dataset shape:\", final_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/chandru/binp37/results/metasub/metasub_geo_training_testing.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microbiome features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the raw sequence of all these top hundered species and get a phylogenetic tree to determine the relationship between species.\n",
    "# We can then use the information as well as a feature to predict the lat and long.\n",
    "\n",
    "microbe_data = rfe_df.iloc[:,:-4]\n",
    "microbe_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phylogenetic Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = []\n",
    "for name in microbe_data.columns:\n",
    "    species_list.append(name)\n",
    "    \n",
    "\n",
    "tax_df = pd.read_csv(\"/home/chandru/binp37/results/metasub/taxonomic_info.csv\")\n",
    "lin_df = tax_df[tax_df['Species'].isin(species_list)].dropna(axis=1,how='all')\n",
    "lin_df = lin_df.dropna(subset=lin_df.columns[1:7]).iloc[:,:7]\n",
    "lin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(lin_df['Rank_1'],return_counts=True)[0],np.unique(lin_df['Rank_1'],return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count values\n",
    "counts = lin_df['Rank_2'].value_counts()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "counts.plot(kind='bar', color=['tomato', 'skyblue'])\n",
    "plt.title('Frequency of Rank_1 Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Entrez.email = \"1ms19bt011@gmail.com\" # Remember to set your actual email\n",
    "\n",
    "def download_genome(species, output_dir=\"genomes\"):\n",
    "    \"\"\"\n",
    "    Downloads the complete genome for a given species from NCBI RefSeq,\n",
    "    handling both FTP and HTTP URLs.\n",
    "\n",
    "    Args:\n",
    "        species (str): The scientific name of the species (e.g., \"Escherichia coli\").\n",
    "        output_dir (str): The directory where the genome file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the genome was successfully downloaded and decompressed, False otherwise.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    search_terms = [\n",
    "        f'\"{species}\"[Organism] AND \"complete genome\"[Assembly Level]',\n",
    "        f'\"{species}\"[Organism] AND \"reference genome\"[Refseq Category]',\n",
    "        f'\"{species}\"[Organism] AND latest[filter]',\n",
    "        f'\"{species}\"[Organism]' # Broadest term as a last resort\n",
    "    ]\n",
    "\n",
    "    for term_index, term in enumerate(search_terms):\n",
    "        print(f\"Searching for '{species}' with term: '{term}'\")\n",
    "        try:\n",
    "            # Search for latest RefSeq assembly\n",
    "            handle = Entrez.esearch(db=\"assembly\", term=term, retmax=1)\n",
    "            record = Entrez.read(handle)\n",
    "            handle.close() # Always close the handle\n",
    "\n",
    "            if record[\"IdList\"]:\n",
    "                assembly_id = record[\"IdList\"][0]\n",
    "                print(f\"Found assembly ID: {assembly_id} for {species}\")\n",
    "\n",
    "                # Fetch summary to get FTP path\n",
    "                summary_handle = Entrez.esummary(db=\"assembly\", id=assembly_id)\n",
    "                doc = Entrez.read(summary_handle)\n",
    "                summary_handle.close() # Always close the handle\n",
    "\n",
    "                ftp_path = doc[\"DocumentSummarySet\"][\"DocumentSummary\"][0][\"FtpPath_RefSeq\"]\n",
    "                if ftp_path:\n",
    "                    filename_stem = ftp_path.split(\"/\")[-1]\n",
    "                    fasta_url = f\"{ftp_path}/{filename_stem}_genomic.fna.gz\"\n",
    "                    output_gz_path = os.path.join(output_dir, f\"{species.replace(' ', '_')}.fna.gz\")\n",
    "                    output_fna_path = os.path.join(output_dir, f\"{species.replace(' ', '_')}.fna\")\n",
    "\n",
    "                    print(f\"Attempting to download from: {fasta_url}\")\n",
    "\n",
    "                    try:\n",
    "                        if fasta_url.startswith(\"ftp://\"):\n",
    "                            # Use wget for FTP paths\n",
    "                            print(f\"Using wget for FTP download: {fasta_url}\")\n",
    "                            # -q for quiet, -O for output file, --show-progress for progress bar\n",
    "                            # --no-verbose for cleaner output\n",
    "                            # Use subprocess.run for better control and error handling than os.system\n",
    "                            result = subprocess.run(\n",
    "                                [\"wget\", \"--no-verbose\", \"--show-progress\", \"-O\", output_gz_path, fasta_url],\n",
    "                                check=True, # Raise CalledProcessError if wget returns non-zero exit code\n",
    "                                capture_output=True, # Capture stdout/stderr for debugging if needed\n",
    "                                text=True # Decode stdout/stderr as text\n",
    "                            )\n",
    "                            # print(result.stdout) # Uncomment for detailed wget output\n",
    "                            # print(result.stderr) # Uncomment for detailed wget output\n",
    "                            print(f\"Downloaded {species} to {output_gz_path} using wget.\")\n",
    "                        else:\n",
    "                            # Use requests for HTTP/HTTPS paths\n",
    "                            print(f\"Using requests for HTTP/HTTPS download: {fasta_url}\")\n",
    "                            response = requests.get(fasta_url, stream=True)\n",
    "                            response.raise_for_status() # Raise an exception for HTTP errors\n",
    "\n",
    "                            total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "                            block_size = 1024 # 1 Kibibyte\n",
    "                            progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True, desc=f\"Downloading {species}\")\n",
    "\n",
    "                            with open(output_gz_path, 'wb') as f:\n",
    "                                for chunk in response.iter_content(chunk_size=block_size):\n",
    "                                    progress_bar.update(len(chunk))\n",
    "                                    f.write(chunk)\n",
    "                            progress_bar.close()\n",
    "\n",
    "                            if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "                                print(\"ERROR, something went wrong during download!\")\n",
    "                                return False\n",
    "                            print(f\"Downloaded {species} to {output_gz_path} using requests.\")\n",
    "\n",
    "                        # Decompress the file, regardless of how it was downloaded\n",
    "                        print(f\"Decompressing {output_gz_path}...\")\n",
    "                        with gzip.open(output_gz_path, 'rb') as f_in:\n",
    "                            with open(output_fna_path, 'wb') as f_out:\n",
    "                                f_out.write(f_in.read())\n",
    "                        os.remove(output_gz_path) # Remove the compressed file\n",
    "                        print(f\"Decompressed to {output_fna_path}\")\n",
    "                        return True\n",
    "                    except subprocess.CalledProcessError as sub_e:\n",
    "                        print(f\"wget failed for {species} from {fasta_url}: {sub_e}\")\n",
    "                        print(f\"wget stdout: {sub_e.stdout}\")\n",
    "                        print(f\"wget stderr: {sub_e.stderr}\")\n",
    "                        continue # Try next search term\n",
    "                    except requests.exceptions.RequestException as req_e:\n",
    "                        print(f\"Download failed for {species} from {fasta_url}: {req_e}\")\n",
    "                        continue # Try next search term\n",
    "                    except Exception as download_e:\n",
    "                        print(f\"An unexpected error occurred during download/decompression for {species}: {download_e}\")\n",
    "                        continue # Try next search term\n",
    "                else:\n",
    "                    print(f\"No FTP path found for {species} with term '{term}'. Trying next search term.\")\n",
    "            else:\n",
    "                print(f\"No assembly found for {species} with term '{term}'. Trying next search term.\")\n",
    "            time.sleep(1) # Small delay between Entrez calls to be polite\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Entrez search for {species} with term '{term}': {e}\")\n",
    "            time.sleep(2) # Longer delay if Entrez call itself fails\n",
    "    print(f\"Failed to download genome for {species} after trying all search terms.\")\n",
    "    return False\n",
    "\n",
    "\n",
    "output_directory = \"genomes\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process each species in the list\n",
    "print(\"\\nStarting genome download process...\")\n",
    "for species in tqdm(filtered_species_list[:], desc=\"Overall Genome Download Progress\"):\n",
    "    print(f\"\\nProcessing species: {species}\")\n",
    "    success = download_genome(species, output_directory)\n",
    "    if not success:\n",
    "        print(f\"Could not download genome for {species}. Please check the species name or try again later.\")\n",
    "    time.sleep(2) # Respect NCBI rate limits between species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clutering using K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method to determine ideal cluster size -> Note I am getting the cut iff to be 15.\n",
    "# Calculate inertia for k=1 to 50\n",
    "inertias = []\n",
    "for k in range(1, 50):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(np.array(microbe_data))\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow Curve\n",
    "plt.plot(range(1, 50), inertias, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=15, random_state=42, n_init=\"auto\").fit(np.array(microbe_data))\n",
    "kmeans.cluster_centers_\n",
    "\n",
    "\n",
    "centorid_distances = cdist(np.array(microbe_data),kmeans.cluster_centers_,\"euclidean\")\n",
    "closet_indices = np.argmin(centorid_distances,axis=0)\n",
    "\n",
    "augment_data = pd.concat([microbe_data,pd.DataFrame(centorid_distances)],axis=1)\n",
    "augment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/chandru/binp37/scripts/ensemble\")\n",
    "y_test_cont = np.load(\"saved_results/y_test_cont.npy\")\n",
    "y_pred_cont = np.load(\"saved_results/y_pred_cont.npy\")\n",
    "\n",
    "y_test_city = np.load(\"saved_results/y_test_city.npy\")\n",
    "y_pred_city = np.load(\"saved_results/y_pred_city.npy\")\n",
    "\n",
    "y_test_coords = np.load(\"saved_results/y_test_coord.npy\")\n",
    "y_pred_coords = np.load(\"saved_results/y_pred_coord.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'true_cont': y_test_cont,\n",
    "    'pred_cont': y_pred_cont,\n",
    "    'true_city': y_test_city,\n",
    "    'pred_city': y_pred_city,\n",
    "    'true_lat': y_test_coords[:, 0],\n",
    "    'true_lon': y_test_coords[:, 1],\n",
    "    'pred_lat': y_pred_coords[:, 0],\n",
    "    'pred_lon': y_pred_coords[:, 1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "continents = np.array([\n",
    "    'east_asia', 'europe', 'middle_east', 'north_america',\n",
    "    'oceania', 'south_america', 'sub_saharan_africa'\n",
    "])\n",
    "\n",
    "cities = np.array([\n",
    "    'auckland', 'baltimore', 'barcelona', 'berlin', 'bogota', 'brisbane',\n",
    "    'denver', 'doha', 'europe', 'fairbanks', 'hamilton', 'hanoi',\n",
    "    'hong_kong', 'ilorin', 'kuala_lumpur', 'kyiv', 'lisbon', 'london',\n",
    "    'marseille', 'minneapolis', 'naples', 'new_york_city', 'offa', 'oslo',\n",
    "    'paris', 'rio_de_janeiro', 'sacramento', 'san_francisco', 'santiago',\n",
    "    'sao_paulo', 'sendai', 'seoul', 'singapore', 'sofia', 'stockholm',\n",
    "    'taipei', 'tokyo', 'vienna', 'yamaguchi', 'zurich'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['true_cont_name'] = df['true_cont'].map(lambda i: continents[i])\n",
    "df['pred_cont_name'] = df['pred_cont'].map(lambda i: continents[i])\n",
    "\n",
    "df['true_city_name'] = df['true_city'].map(lambda i: cities[i])\n",
    "df['pred_city_name'] = df['pred_city'].map(lambda i: cities[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute the correctness \n",
    "df['continent_correct'] = df['true_cont'] == df['pred_cont']\n",
    "df['city_correct'] = df['true_city'] == df['pred_city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute coordinates distance\n",
    "\n",
    "# Distance between two points on the earth\n",
    "def haversine_distance(lat1,lon1,lat2,lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points on the earth\n",
    "    \"\"\"\n",
    "    # Radius of the earth\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert from degrees to radians\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2) **2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "    return R * c # in kilometers\n",
    "\n",
    "\n",
    "\n",
    "df['coord_error'] = haversine_distance(df['true_lat'].values,df['true_lon'].values,df['pred_lat'].values,df['pred_lon'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Group into the 4 categories\n",
    "def group_label(row):\n",
    "    if row['continent_correct'] and row['city_correct']:\n",
    "        return 'C_correct Z_correct'\n",
    "    elif row['continent_correct'] and not row['city_correct']:\n",
    "        return 'C_correct Z_wrong'\n",
    "    elif not row['continent_correct'] and row['city_correct']:\n",
    "        return 'C_wrong Z_correct'\n",
    "    else:\n",
    "        return 'C_wrong Z_wrong'\n",
    "\n",
    "df['error_group'] = df.apply(group_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Aggregate stats\n",
    "group_stats = df.groupby('error_group')['coord_error'].agg([\n",
    "    ('count', 'count'),\n",
    "    ('mean_error_km', 'mean'),\n",
    "    ('median_error_km', 'median')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Coordinate Error E[D]: 727.45 km\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Calculate proportions and expected error\n",
    "total = len(df)\n",
    "group_stats['proportion'] = group_stats['count'] / total\n",
    "group_stats['weighted_error'] = group_stats['mean_error_km'] * group_stats['proportion']\n",
    "expected_total_error = group_stats['weighted_error'].sum()\n",
    "print(f\"\\nExpected Coordinate Error E[D]: {expected_total_error:.2f} km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean_error_km</th>\n",
       "      <th>median_error_km</th>\n",
       "      <th>proportion</th>\n",
       "      <th>weighted_error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>error_group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_correct Z_correct</th>\n",
       "      <td>715</td>\n",
       "      <td>244.943222</td>\n",
       "      <td>15.259644</td>\n",
       "      <td>0.878378</td>\n",
       "      <td>215.152830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_correct Z_wrong</th>\n",
       "      <td>54</td>\n",
       "      <td>3361.478271</td>\n",
       "      <td>1511.047852</td>\n",
       "      <td>0.066339</td>\n",
       "      <td>222.997330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_wrong Z_correct</th>\n",
       "      <td>23</td>\n",
       "      <td>3241.101807</td>\n",
       "      <td>1677.860474</td>\n",
       "      <td>0.028256</td>\n",
       "      <td>91.579044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_wrong Z_wrong</th>\n",
       "      <td>22</td>\n",
       "      <td>7315.582520</td>\n",
       "      <td>6775.188477</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>197.718446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count  mean_error_km  median_error_km  proportion  \\\n",
       "error_group                                                              \n",
       "C_correct Z_correct    715     244.943222        15.259644    0.878378   \n",
       "C_correct Z_wrong       54    3361.478271      1511.047852    0.066339   \n",
       "C_wrong Z_correct       23    3241.101807      1677.860474    0.028256   \n",
       "C_wrong Z_wrong         22    7315.582520      6775.188477    0.027027   \n",
       "\n",
       "                     weighted_error  \n",
       "error_group                          \n",
       "C_correct Z_correct      215.152830  \n",
       "C_correct Z_wrong        222.997330  \n",
       "C_wrong Z_correct         91.579044  \n",
       "C_wrong Z_wrong          197.718446  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binp37_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
