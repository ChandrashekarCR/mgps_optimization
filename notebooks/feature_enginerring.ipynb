{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from Bio import Entrez\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from Bio import Entrez\n",
    "import gzip\n",
    "import subprocess \n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_844114/4278898321.py:2: DtypeWarning: Columns (1,11,12,15,18,19,35,37,38,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"/home/chandru/binp37/results/metasub/processed_metasub.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>metasub_name</th>\n",
       "      <th>core_project</th>\n",
       "      <th>project</th>\n",
       "      <th>city</th>\n",
       "      <th>city_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>surface_material</th>\n",
       "      <th>control_type</th>\n",
       "      <th>...</th>\n",
       "      <th>cyanobacterium endosymbiont of Epithemia turgida</th>\n",
       "      <th>endosymbiont 'TC1' of Trimyema compressum</th>\n",
       "      <th>endosymbiont of Acanthamoeba sp. UWC8</th>\n",
       "      <th>endosymbiont of unidentified scaly snail isolate Monju</th>\n",
       "      <th>gamma proteobacterium HdN1</th>\n",
       "      <th>halophilic archaeon DL31</th>\n",
       "      <th>halophilic archaeon True-ADL</th>\n",
       "      <th>secondary endosymbiont of Ctenarytaina eucalypti</th>\n",
       "      <th>secondary endosymbiont of Heteropsylla cubana</th>\n",
       "      <th>uncultured crAssphage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>haib17CEM4890_H75CGCCXY_SL263639</td>\n",
       "      <td>CSD16-HAM-001</td>\n",
       "      <td>core</td>\n",
       "      <td>CSD16</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>HAM</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>haib17CEM4890_H75CGCCXY_SL263651</td>\n",
       "      <td>CSD16-HAM-006</td>\n",
       "      <td>core</td>\n",
       "      <td>CSD16</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>HAM</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>haib17CEM4890_H75CGCCXY_SL263663</td>\n",
       "      <td>CSD16-HAM-008</td>\n",
       "      <td>core</td>\n",
       "      <td>CSD16</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>HAM</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>haib17CEM4890_H75CGCCXY_SL263675</td>\n",
       "      <td>CSD16-HAM-012</td>\n",
       "      <td>core</td>\n",
       "      <td>CSD16</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>HAM</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haib17CEM4890_H75CGCCXY_SL263687</td>\n",
       "      <td>CSD16-HAM-015</td>\n",
       "      <td>core</td>\n",
       "      <td>CSD16</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>HAM</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3711 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               uuid   metasub_name core_project project  \\\n",
       "0  haib17CEM4890_H75CGCCXY_SL263639  CSD16-HAM-001         core   CSD16   \n",
       "1  haib17CEM4890_H75CGCCXY_SL263651  CSD16-HAM-006         core   CSD16   \n",
       "2  haib17CEM4890_H75CGCCXY_SL263663  CSD16-HAM-008         core   CSD16   \n",
       "3  haib17CEM4890_H75CGCCXY_SL263675  CSD16-HAM-012         core   CSD16   \n",
       "4  haib17CEM4890_H75CGCCXY_SL263687  CSD16-HAM-015         core   CSD16   \n",
       "\n",
       "       city city_code  latitude  longitude surface_material  control_type  \\\n",
       "0  hamilton       HAM -37.78333  175.28333              NaN           NaN   \n",
       "1  hamilton       HAM -37.78333  175.28333              NaN           NaN   \n",
       "2  hamilton       HAM -37.78333  175.28333              NaN           NaN   \n",
       "3  hamilton       HAM -37.78333  175.28333              NaN           NaN   \n",
       "4  hamilton       HAM -37.78333  175.28333              NaN           NaN   \n",
       "\n",
       "   ... cyanobacterium endosymbiont of Epithemia turgida  \\\n",
       "0  ...                                              0.0   \n",
       "1  ...                                              0.0   \n",
       "2  ...                                              0.0   \n",
       "3  ...                                              0.0   \n",
       "4  ...                                              0.0   \n",
       "\n",
       "  endosymbiont 'TC1' of Trimyema compressum  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       0.0   \n",
       "3                                       0.0   \n",
       "4                                       0.0   \n",
       "\n",
       "  endosymbiont of Acanthamoeba sp. UWC8  \\\n",
       "0                                   0.0   \n",
       "1                                   0.0   \n",
       "2                                   0.0   \n",
       "3                                   0.0   \n",
       "4                                   0.0   \n",
       "\n",
       "  endosymbiont of unidentified scaly snail isolate Monju  \\\n",
       "0                                            0.00000       \n",
       "1                                            0.00001       \n",
       "2                                            0.00002       \n",
       "3                                            0.00002       \n",
       "4                                            0.00000       \n",
       "\n",
       "   gamma proteobacterium HdN1 halophilic archaeon DL31  \\\n",
       "0                         0.0                      0.0   \n",
       "1                         0.0                      0.0   \n",
       "2                         0.0                      0.0   \n",
       "3                         0.0                      0.0   \n",
       "4                         0.0                      0.0   \n",
       "\n",
       "  halophilic archaeon True-ADL  \\\n",
       "0                          0.0   \n",
       "1                          0.0   \n",
       "2                          0.0   \n",
       "3                          0.0   \n",
       "4                          0.0   \n",
       "\n",
       "   secondary endosymbiont of Ctenarytaina eucalypti  \\\n",
       "0                                               0.0   \n",
       "1                                               0.0   \n",
       "2                                               0.0   \n",
       "3                                               0.0   \n",
       "4                                               0.0   \n",
       "\n",
       "  secondary endosymbiont of Heteropsylla cubana uncultured crAssphage  \n",
       "0                                           0.0                   0.0  \n",
       "1                                           0.0                   0.0  \n",
       "2                                           0.0                   0.0  \n",
       "3                                           0.0                   0.0  \n",
       "4                                           0.0                   0.0  \n",
       "\n",
       "[5 rows x 3711 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data \n",
    "df = pd.read_csv(\"/home/chandru/binp37/results/metasub/processed_metasub.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acidovorax ebreus</th>\n",
       "      <th>Acidovorax sp. JS42</th>\n",
       "      <th>Acidovorax sp. KKS102</th>\n",
       "      <th>Acinetobacter baumannii</th>\n",
       "      <th>Acinetobacter haemolyticus</th>\n",
       "      <th>Acinetobacter johnsonii</th>\n",
       "      <th>Acinetobacter junii</th>\n",
       "      <th>Acinetobacter pittii</th>\n",
       "      <th>Acinetobacter schindleri</th>\n",
       "      <th>Acinetobacter sp. LoGeW2-3</th>\n",
       "      <th>...</th>\n",
       "      <th>Variovorax boronicumulans</th>\n",
       "      <th>Variovorax paradoxus</th>\n",
       "      <th>Variovorax sp. PAMC 28711</th>\n",
       "      <th>Veillonella parvula</th>\n",
       "      <th>Weissella cibaria</th>\n",
       "      <th>Xanthomonas campestris</th>\n",
       "      <th>continent</th>\n",
       "      <th>city</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00480</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00028</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>0.00142</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00091</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00181</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.00274</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>0.00191</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00208</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00397</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Acidovorax ebreus  Acidovorax sp. JS42  Acidovorax sp. KKS102  \\\n",
       "0            0.00000              0.00000                0.00023   \n",
       "1            0.00000              0.00001                0.00003   \n",
       "2            0.00003              0.00000                0.00011   \n",
       "3            0.00000              0.00000                0.00000   \n",
       "4            0.00000              0.00000                0.00000   \n",
       "\n",
       "   Acinetobacter baumannii  Acinetobacter haemolyticus  \\\n",
       "0                  0.00015                     0.00000   \n",
       "1                  0.00028                     0.00016   \n",
       "2                  0.00181                     0.00060   \n",
       "3                  0.00002                     0.00001   \n",
       "4                  0.00003                     0.00000   \n",
       "\n",
       "   Acinetobacter johnsonii  Acinetobacter junii  Acinetobacter pittii  \\\n",
       "0                  0.00006              0.00001               0.00007   \n",
       "1                  0.00142              0.00017               0.00013   \n",
       "2                  0.00274              0.00030               0.00110   \n",
       "3                  0.00003              0.00000               0.00000   \n",
       "4                  0.00000              0.00000               0.00002   \n",
       "\n",
       "   Acinetobacter schindleri  Acinetobacter sp. LoGeW2-3  ...  \\\n",
       "0                   0.00010                     0.00005  ...   \n",
       "1                   0.00262                     0.00140  ...   \n",
       "2                   0.00191                     0.00132  ...   \n",
       "3                   0.00003                     0.00001  ...   \n",
       "4                   0.00009                     0.00001  ...   \n",
       "\n",
       "   Variovorax boronicumulans  Variovorax paradoxus  Variovorax sp. PAMC 28711  \\\n",
       "0                    0.00031               0.00075                    0.00021   \n",
       "1                    0.00013               0.00024                    0.00003   \n",
       "2                    0.00010               0.00025                    0.00001   \n",
       "3                    0.00003               0.00002                    0.00000   \n",
       "4                    0.00004               0.00008                    0.00003   \n",
       "\n",
       "   Veillonella parvula  Weissella cibaria  Xanthomonas campestris  continent  \\\n",
       "0                  0.0                0.0                 0.00480    oceania   \n",
       "1                  0.0                0.0                 0.00091    oceania   \n",
       "2                  0.0                0.0                 0.00208    oceania   \n",
       "3                  0.0                0.0                 0.00137    oceania   \n",
       "4                  0.0                0.0                 0.00397    oceania   \n",
       "\n",
       "       city  latitude  longitude  \n",
       "0  hamilton -37.78333  175.28333  \n",
       "1  hamilton -37.78333  175.28333  \n",
       "2  hamilton -37.78333  175.28333  \n",
       "3  hamilton -37.78333  175.28333  \n",
       "4  hamilton -37.78333  175.28333  \n",
       "\n",
       "[5 rows x 204 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe_df = pd.read_csv(\"/home/chandru/binp37/results/metasub/metasub_training_testing_data.csv\")\n",
    "rfe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geograpical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = df[['city_total_population','city_population_density',\n",
    "                  'city_land_area_km2','city_ave_june_temp_c','city_elevation_meters','city_koppen_climate','continent','city','latitude','longitude']]\n",
    "\n",
    "# Fix city elevation of hanoi, yamaguchi in meters\n",
    "feature_data.loc[feature_data['city'] == 'hanoi','city_elevation_meters'] = 12\n",
    "feature_data.loc[feature_data['city'] == 'yamaguchi','city_elevation_meters'] = 23\n",
    "feature_data.loc[feature_data['city'] == 'marseille','city_elevation_meters'] = 42 # city elevation of marseille on google is 42 m here it is 0\n",
    "\n",
    "# Get city population density, city ladn ares in km2, city avg temp in june and city elevation in meters of offa \n",
    "offa_data = {\n",
    "    'city_population_density': 2500.0,\n",
    "    'city_land_area_km2': 74.0,\n",
    "    'city_ave_june_temp_c': 28.0,\n",
    "    'city_elevation_meters': 457.0\n",
    "}\n",
    "\n",
    "feature_data.loc[feature_data['city'] == 'offa', list(offa_data.keys())] = list(offa_data.values())\n",
    "\n",
    "# Get city land area in km2 of marseille  \n",
    "feature_data.loc[feature_data['city'] == 'marseille','city_land_area_km2'] = 240\n",
    "\n",
    "# Fix all the nan values of london\n",
    "london_data = {\n",
    "    'city_total_population': 8787892.0,\n",
    "    'city_population_density': 5590.0,\n",
    "    'city_land_area_km2': 1572.0,\n",
    "    'city_ave_june_temp_c': 14.4,\n",
    "    'city_elevation_meters': 11.0,\n",
    "    'city_koppen_climate': 'marine_west_coast_climate'\n",
    "}\n",
    "feature_data.loc[feature_data['city'] == 'london', list(london_data.keys())] = list(london_data.values())\n",
    "\n",
    "\n",
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for skewness in the data before appling long transformer -> \n",
    "# Note to self: The city_land_area_km2 is right skewed, so we will go with log scale transformation\n",
    "#             : The city_elevation_meters is multi modal there we will go with QuantileTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, QuantileTransformer, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define input columns\n",
    "#log_cols = ['city_land_area_km2']\n",
    "#quantile_cols = ['city_elevation_meters']\n",
    "scale_cols = ['city_total_population', 'city_ave_june_temp_c']\n",
    "cat_cols = ['city_koppen_climate']\n",
    "\n",
    "# Step 2: Log-transform function\n",
    "#def safe_log1p(x):\n",
    "#    return np.log1p(np.maximum(x, 0))\n",
    "\n",
    "# Step 3: Create log pipeline\n",
    "#log_pipeline = Pipeline([\n",
    "#    ('log', FunctionTransformer(safe_log1p)),\n",
    "#    ('scale', StandardScaler())\n",
    "#])\n",
    "\n",
    "# Step 4: Build the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "#    ('log', log_pipeline, log_cols),\n",
    "#    ('quantile', QuantileTransformer(output_distribution='normal'), quantile_cols),\n",
    "    ('scale', StandardScaler(), scale_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "])\n",
    "\n",
    "# Step 5: Fit and transform\n",
    "geo_features_processed = preprocessor.fit_transform(feature_data)\n",
    "\n",
    "# Step 6: Extract column names correctly\n",
    "output_feature_names = []\n",
    "\n",
    "for name, transformer, cols in preprocessor.transformers_:\n",
    "    if name == 'cat':\n",
    "        # For OneHotEncoder\n",
    "        encoder = transformer\n",
    "        if isinstance(encoder, Pipeline):\n",
    "            encoder = encoder.named_steps['onehot']\n",
    "        cats = encoder.categories_[0]\n",
    "        output_feature_names.extend([f\"{cols[0]}_{cat}\" for cat in cats])\n",
    "    else:\n",
    "        output_feature_names.extend(cols)\n",
    "\n",
    "# Step 7: Convert to DataFrame\n",
    "geo_features_df = pd.DataFrame(geo_features_processed)\n",
    "\n",
    "# Step 8: Merge with main features (RFE-selected ones)\n",
    "final_df = pd.concat([rfe_df, geo_features_df], axis=1)\n",
    "final_df.to_csv(\"/home/chandru/binp37/results/metasub/metasub_geo_training_testing.csv\", index=False)\n",
    "\n",
    "print(\"Final dataset shape:\", final_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Select your input columns\n",
    "scale_cols = ['city_ave_june_temp_c']\n",
    "#cat_cols = ['city_koppen_climate']\n",
    "\n",
    "# Step 2: Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale', StandardScaler(), scale_cols),\n",
    "#        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 3: Fit and transform the geo feature data\n",
    "geo_features_processed = preprocessor.fit_transform(feature_data)\n",
    "\n",
    "# Step 4: Get feature names\n",
    "feature_names = []\n",
    "\n",
    "# Handle scaled columns\n",
    "feature_names.extend(scale_cols)\n",
    "\n",
    "# Handle one-hot columns\n",
    "#ohe = preprocessor.named_transformers_['cat']\n",
    "#cat_feature_names = ohe.get_feature_names_out(cat_cols)\n",
    "#feature_names.extend(cat_feature_names)\n",
    "\n",
    "# Step 5: Convert to DataFrame\n",
    "geo_features_df = pd.DataFrame(geo_features_processed.toarray() if hasattr(geo_features_processed, 'toarray') else geo_features_processed)\n",
    "\n",
    "# Step 6: Merge with selected features and save\n",
    "final_df = pd.concat([rfe_df.reset_index(drop=True), geo_features_df.reset_index(drop=True)], axis=1)\n",
    "#final_df.to_csv(\"/home/chandru/binp37/results/metasub/metasub_geo_training_testing.csv\", index=False)\n",
    "\n",
    "print(\"Final dataset shape:\", final_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/chandru/binp37/results/metasub/metasub_geo_training_testing.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microbiome features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the raw sequence of all these top hundered species and get a phylogenetic tree to determine the relationship between species.\n",
    "# We can then use the information as well as a feature to predict the lat and long.\n",
    "\n",
    "microbe_data = rfe_df.iloc[:,:-4]\n",
    "microbe_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phylogenetic Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = []\n",
    "for name in microbe_data.columns:\n",
    "    species_list.append(name)\n",
    "    \n",
    "\n",
    "tax_df = pd.read_csv(\"/home/chandru/binp37/results/metasub/taxonomic_info.csv\")\n",
    "lin_df = tax_df[tax_df['Species'].isin(species_list)].dropna(axis=1,how='all')\n",
    "lin_df = lin_df.dropna(subset=lin_df.columns[1:7]).iloc[:,:7]\n",
    "lin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(lin_df['Rank_1'],return_counts=True)[0],np.unique(lin_df['Rank_1'],return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count values\n",
    "counts = lin_df['Rank_2'].value_counts()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "counts.plot(kind='bar', color=['tomato', 'skyblue'])\n",
    "plt.title('Frequency of Rank_1 Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Entrez.email = \"1ms19bt011@gmail.com\" # Remember to set your actual email\n",
    "\n",
    "def download_genome(species, output_dir=\"genomes\"):\n",
    "    \"\"\"\n",
    "    Downloads the complete genome for a given species from NCBI RefSeq,\n",
    "    handling both FTP and HTTP URLs.\n",
    "\n",
    "    Args:\n",
    "        species (str): The scientific name of the species (e.g., \"Escherichia coli\").\n",
    "        output_dir (str): The directory where the genome file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the genome was successfully downloaded and decompressed, False otherwise.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    search_terms = [\n",
    "        f'\"{species}\"[Organism] AND \"complete genome\"[Assembly Level]',\n",
    "        f'\"{species}\"[Organism] AND \"reference genome\"[Refseq Category]',\n",
    "        f'\"{species}\"[Organism] AND latest[filter]',\n",
    "        f'\"{species}\"[Organism]' # Broadest term as a last resort\n",
    "    ]\n",
    "\n",
    "    for term_index, term in enumerate(search_terms):\n",
    "        print(f\"Searching for '{species}' with term: '{term}'\")\n",
    "        try:\n",
    "            # Search for latest RefSeq assembly\n",
    "            handle = Entrez.esearch(db=\"assembly\", term=term, retmax=1)\n",
    "            record = Entrez.read(handle)\n",
    "            handle.close() # Always close the handle\n",
    "\n",
    "            if record[\"IdList\"]:\n",
    "                assembly_id = record[\"IdList\"][0]\n",
    "                print(f\"Found assembly ID: {assembly_id} for {species}\")\n",
    "\n",
    "                # Fetch summary to get FTP path\n",
    "                summary_handle = Entrez.esummary(db=\"assembly\", id=assembly_id)\n",
    "                doc = Entrez.read(summary_handle)\n",
    "                summary_handle.close() # Always close the handle\n",
    "\n",
    "                ftp_path = doc[\"DocumentSummarySet\"][\"DocumentSummary\"][0][\"FtpPath_RefSeq\"]\n",
    "                if ftp_path:\n",
    "                    filename_stem = ftp_path.split(\"/\")[-1]\n",
    "                    fasta_url = f\"{ftp_path}/{filename_stem}_genomic.fna.gz\"\n",
    "                    output_gz_path = os.path.join(output_dir, f\"{species.replace(' ', '_')}.fna.gz\")\n",
    "                    output_fna_path = os.path.join(output_dir, f\"{species.replace(' ', '_')}.fna\")\n",
    "\n",
    "                    print(f\"Attempting to download from: {fasta_url}\")\n",
    "\n",
    "                    try:\n",
    "                        if fasta_url.startswith(\"ftp://\"):\n",
    "                            # Use wget for FTP paths\n",
    "                            print(f\"Using wget for FTP download: {fasta_url}\")\n",
    "                            # -q for quiet, -O for output file, --show-progress for progress bar\n",
    "                            # --no-verbose for cleaner output\n",
    "                            # Use subprocess.run for better control and error handling than os.system\n",
    "                            result = subprocess.run(\n",
    "                                [\"wget\", \"--no-verbose\", \"--show-progress\", \"-O\", output_gz_path, fasta_url],\n",
    "                                check=True, # Raise CalledProcessError if wget returns non-zero exit code\n",
    "                                capture_output=True, # Capture stdout/stderr for debugging if needed\n",
    "                                text=True # Decode stdout/stderr as text\n",
    "                            )\n",
    "                            # print(result.stdout) # Uncomment for detailed wget output\n",
    "                            # print(result.stderr) # Uncomment for detailed wget output\n",
    "                            print(f\"Downloaded {species} to {output_gz_path} using wget.\")\n",
    "                        else:\n",
    "                            # Use requests for HTTP/HTTPS paths\n",
    "                            print(f\"Using requests for HTTP/HTTPS download: {fasta_url}\")\n",
    "                            response = requests.get(fasta_url, stream=True)\n",
    "                            response.raise_for_status() # Raise an exception for HTTP errors\n",
    "\n",
    "                            total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "                            block_size = 1024 # 1 Kibibyte\n",
    "                            progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True, desc=f\"Downloading {species}\")\n",
    "\n",
    "                            with open(output_gz_path, 'wb') as f:\n",
    "                                for chunk in response.iter_content(chunk_size=block_size):\n",
    "                                    progress_bar.update(len(chunk))\n",
    "                                    f.write(chunk)\n",
    "                            progress_bar.close()\n",
    "\n",
    "                            if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "                                print(\"ERROR, something went wrong during download!\")\n",
    "                                return False\n",
    "                            print(f\"Downloaded {species} to {output_gz_path} using requests.\")\n",
    "\n",
    "                        # Decompress the file, regardless of how it was downloaded\n",
    "                        print(f\"Decompressing {output_gz_path}...\")\n",
    "                        with gzip.open(output_gz_path, 'rb') as f_in:\n",
    "                            with open(output_fna_path, 'wb') as f_out:\n",
    "                                f_out.write(f_in.read())\n",
    "                        os.remove(output_gz_path) # Remove the compressed file\n",
    "                        print(f\"Decompressed to {output_fna_path}\")\n",
    "                        return True\n",
    "                    except subprocess.CalledProcessError as sub_e:\n",
    "                        print(f\"wget failed for {species} from {fasta_url}: {sub_e}\")\n",
    "                        print(f\"wget stdout: {sub_e.stdout}\")\n",
    "                        print(f\"wget stderr: {sub_e.stderr}\")\n",
    "                        continue # Try next search term\n",
    "                    except requests.exceptions.RequestException as req_e:\n",
    "                        print(f\"Download failed for {species} from {fasta_url}: {req_e}\")\n",
    "                        continue # Try next search term\n",
    "                    except Exception as download_e:\n",
    "                        print(f\"An unexpected error occurred during download/decompression for {species}: {download_e}\")\n",
    "                        continue # Try next search term\n",
    "                else:\n",
    "                    print(f\"No FTP path found for {species} with term '{term}'. Trying next search term.\")\n",
    "            else:\n",
    "                print(f\"No assembly found for {species} with term '{term}'. Trying next search term.\")\n",
    "            time.sleep(1) # Small delay between Entrez calls to be polite\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Entrez search for {species} with term '{term}': {e}\")\n",
    "            time.sleep(2) # Longer delay if Entrez call itself fails\n",
    "    print(f\"Failed to download genome for {species} after trying all search terms.\")\n",
    "    return False\n",
    "\n",
    "\n",
    "output_directory = \"genomes\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process each species in the list\n",
    "print(\"\\nStarting genome download process...\")\n",
    "for species in tqdm(filtered_species_list[:], desc=\"Overall Genome Download Progress\"):\n",
    "    print(f\"\\nProcessing species: {species}\")\n",
    "    success = download_genome(species, output_directory)\n",
    "    if not success:\n",
    "        print(f\"Could not download genome for {species}. Please check the species name or try again later.\")\n",
    "    time.sleep(2) # Respect NCBI rate limits between species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clutering using K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method to determine ideal cluster size -> Note I am getting the cut iff to be 15.\n",
    "# Calculate inertia for k=1 to 50\n",
    "inertias = []\n",
    "for k in range(1, 50):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(np.array(microbe_data))\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow Curve\n",
    "plt.plot(range(1, 50), inertias, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=15, random_state=42, n_init=\"auto\").fit(np.array(microbe_data))\n",
    "kmeans.cluster_centers_\n",
    "\n",
    "\n",
    "centorid_distances = cdist(np.array(microbe_data),kmeans.cluster_centers_,\"euclidean\")\n",
    "closet_indices = np.argmin(centorid_distances,axis=0)\n",
    "\n",
    "augment_data = pd.concat([microbe_data,pd.DataFrame(centorid_distances)],axis=1)\n",
    "augment_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binp37_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
