{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) XGBoost Classifier\n",
    "Implementing only XGBoost Classification Model on the dataset to predict the continent level accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read the data\n",
    "Here I will use pre-prcessed data for faster analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 200)\n",
      "['east_asia' 'oceania' 'south_america' 'sub_saharan_africa']\n"
     ]
    }
   ],
   "source": [
    "# Read the data \n",
    "df = pd.read_csv(\"/home/chandru/binp37/results/metasub/metasub_training_testing_data.csv\")\n",
    "df = pd.concat([df.iloc[:,:-4],df['continent']],axis=1)\n",
    "x_data = df[df.columns[:-1]][:1000]\n",
    "print(x_data.shape)\n",
    "y_data = df[df.columns[-1]][:1000]\n",
    "le = LabelEncoder()\n",
    "y_data = le.fit_transform(y_data)\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, Validation and Testing matrices shapes\n",
      "\n",
      "Training\n",
      "\n",
      "(640, 200) (640,)\n",
      "\n",
      "Validation\n",
      "\n",
      "(160, 200) (160,)\n",
      "\n",
      "Testing\n",
      "\n",
      "(200, 200) (200,)\n"
     ]
    }
   ],
   "source": [
    "# Train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data,y_data,random_state=123,test_size=0.2)\n",
    "# Split train into train and validation as well\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=123, test_size=0.2)\n",
    "\n",
    "print('Training, Validation and Testing matrices shapes')\n",
    "print(\"\\nTraining\\n\")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(\"\\nValidation\\n\")\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(\"\\nTesting\\n\")\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy on the validation dataset is 0.9563\n",
      "\n",
      "Classfication Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96        81\n",
      "           1       1.00      0.60      0.75        10\n",
      "           2       0.95      1.00      0.98        20\n",
      "           3       0.98      0.98      0.98        49\n",
      "\n",
      "    accuracy                           0.96       160\n",
      "   macro avg       0.97      0.89      0.92       160\n",
      "weighted avg       0.96      0.96      0.95       160\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      " [[79  0  1  1]\n",
      " [ 4  6  0  0]\n",
      " [ 0  0 20  0]\n",
      " [ 1  0  0 48]]\n"
     ]
    }
   ],
   "source": [
    "# Set the model XGB Classifier\n",
    "\n",
    "xgb_classifier = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Train on the training dataset\n",
    "xgb_classifier.fit(X_train,y_train)\n",
    "\n",
    "# Validate on the validation dataset\n",
    "y_pred = xgb_classifier.predict(X_val)\n",
    "\n",
    "test_accuracy = accuracy_score(y_val,y_pred)\n",
    "print(f\"The test accuracy on the validation dataset is {test_accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassfication Report:\\n\",classification_report(y_val,y_pred))\n",
    "\n",
    "# Print Confusion Matrix\n",
    "print(\"\\nConfusion Matrix\\n\", confusion_matrix(y_val,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic outline of XBNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XBNet(nn.Module):\n",
    "    \"\"\"\n",
    "    XBNet (Extremely Boosted Neural Network) implementation from scratch.\n",
    "\n",
    "    This implementation combines gradient boosted trees with neural networks using Boosted Gradient Descent (BGD) optimzation technique.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,input_size, hidden_layers=[400,200],num_classes=7, n_estimators=100, max_depth=3, dropout_rate=0.2, random_state=42):\n",
    "        super(XBNet,self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize XBNet architecture using Pytorch modules.\n",
    "\n",
    "        Parameters:\n",
    "        - input_size: Number of input features. In this case it is the GITs. # 200\n",
    "        - hidden_layers: List of hidden layers # 400, 200\n",
    "        - n_classes: Number of output classes\n",
    "        - n_estimators: Number of estimators for gradient boosting\n",
    "        - max_depth: Maximum depth for gradient boosted trees\n",
    "        - dropout_rate: L1 reqularization\n",
    "        - random_state: Random state for reporducibility\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Set random seeds\n",
    "        torch.manual_seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "        # Build the neural network\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "\n",
    "        # Create layer architecture\n",
    "        layer_sizes = [input_size] + hidden_layers + [num_classes]\n",
    "\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            if i < len(layer_sizes)-2: # Don't add dropout to ouput layer\n",
    "                self.dropouts.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "\n",
    "        # Initialiaze graident boosting components\n",
    "        self.xgb_tree_initial = None    \n",
    "        self.feature_importances = {}\n",
    "        self.layer_outputs = {}\n",
    "\n",
    "        # Training history\n",
    "        self.history = []\n",
    "        self.accuracry_history = []\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\" Initialize weights for the neiral network using Xavier initilization.\"\"\"\n",
    "        for layer in self.layers[:-1]: # Hidden layers\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "            \n",
    "        # Ouput layer\n",
    "        nn.init.xavier_uniform_(self.layers[-1].weights)\n",
    "        nn.init.zeros_(self.layers[-1].bias)\n",
    "\n",
    "    def initialize_first_layer_with_feature_importance(self,X,y):\n",
    "        \"\"\"\n",
    "        Initialize first layer weights using feature importance from gradient boosted tree. This is the most innovative part of XBNet.\n",
    "        Parameters:\n",
    "        - X: Input features (numpy array or tensor)\n",
    "        - y: Number of classes (numpy array or tensor)\n",
    "        \"\"\"\n",
    "        print(f\"Initializing the first layer with gradient boosting feautre importance...\")\n",
    "\n",
    "        # Convert to numpu if torch tensor\n",
    "        if torch.is_tensor(X):\n",
    "            X_np = X.detach().cpu().numpy()\n",
    "        else:\n",
    "            X_np = X\n",
    "        \n",
    "        if torch.is_tensor(y):\n",
    "            y_np = y.detach().cpu().numpy()\n",
    "        else:\n",
    "            y_np = y\n",
    "\n",
    "        # Train initial gradient boosted tree\n",
    "        # Initialize the XGB classifier model\n",
    "        self.gb_tree_initial = XGBClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth,random_state=self.random_state)\n",
    "\n",
    "        # Train it on the entire dataset\n",
    "        self.gb_tree_initial.fit(X_np,y_np)\n",
    "        feature_importance = self.gb_tree_initial.feature_importances_\n",
    "\n",
    "        # Initialize the first layer weights with feature importance\n",
    "        with torch.no_grad():\n",
    "            first_layer = self.layers[0]\n",
    "            input_size, first_hidden = first_layer.weight.shape[1], first_layer.weight.shape[0]\n",
    "            new_weights = torch.zeros_like(first_layer.weight)\n",
    "            for i in range(first_hidden):\n",
    "                # Use feature importance to initialize each neuron's weight\n",
    "                importance_weights = torch.tensor(feature_importance, dtype=torch.float32)\n",
    "                # Add some random variation \n",
    "                noise = torch.normal(0,0.01, size=(input_size,))\n",
    "                new_weights[i, :] = importance_weights * (0.1 + noise)\n",
    "            first_layer.weight.copy_(new_weights)\n",
    "\n",
    "        print(f\"First layer initialized with feature importance. Shape: {first_layer.weight.shape}\")\n",
    "\n",
    "    def forward(self, x, store_activations=False):\n",
    "        \"\"\"\n",
    "        Forward propoagation through the network\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor\n",
    "        - store_activations: Whether to store intermediate activations for tree training\n",
    "\n",
    "        \"\"\"\n",
    "        if store_activations:\n",
    "            self.layer_outputs = {}\n",
    "\n",
    "\n",
    "        current_input = x\n",
    "\n",
    "        # Forward pass through the hidden layers\n",
    "        for i, (layer, dropout) in enumerate(zip(self.layers[:-1],self.dropouts)):\n",
    "            \n",
    "            z = layer(current_input)\n",
    "            a = F.relu(z) # RelU activation for hidden layers\n",
    "\n",
    "            if store_activations:\n",
    "                self.layer_outputs[i] = a.detach().cpu().numpy()\n",
    "\n",
    "            a = dropout(a) if self.training else a # Apply dropout only during training phase\n",
    "            current_input = a\n",
    "\n",
    "\n",
    "        output = self.layers[-1](current_input)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def train_trees_on_hidden_layers(self,y):\n",
    "        \"\"\"\n",
    "        Train gradient boosted trees on each hidden layer output. This is the part of BGD optimization technique.\n",
    "\n",
    "        Parameters: \n",
    "        - y: Target labels (numpy array or tensor)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if torch.is_tensor():\n",
    "            y_np = y.detach().cpu().numpy()\n",
    "\n",
    "        else:\n",
    "            y_np = y\n",
    "\n",
    "        self.feature_importances = {}\n",
    "\n",
    "        for layer_idx, layer_output in self.layer.items():\n",
    "            try:\n",
    "                # Train a new gradient boosted tree on this layer's output\n",
    "                gb_tree_layer = XGBClassifier(\n",
    "                    n_estimators= self.n_estimators,\n",
    "                    max_depth = self.max_depth,\n",
    "                    random_state = self.random_state\n",
    "                )\n",
    "\n",
    "                gb_tree_layer.fit(layer_output,y_np)\n",
    "                self.feature_importances[layer_idx] = gb_tree_layer.feature_importances_\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not train tree on layer {layer_idx}: {e}\")\n",
    "                # Use unifrom importance as fallback option. In case the above tree does not work.\n",
    "                n_features = layer_output.shape[1]\n",
    "                self.feature_importances[layer_idx] = np.ones(n_features)/n_features\n",
    "\n",
    "    def update_weights_with_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Update weights using feature importance from gradient boosted trees. This is the second step of BGD optimization\n",
    "        \n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            for layer_idx in range(len(self.hidden_layers)):\n",
    "                if layer_idx in self.feature_importances:\n",
    "                    layer = self.layers[layer_idx]\n",
    "                    f_importance = self.feature_importances[layer_idx]\n",
    "\n",
    "                    # Get current weights\n",
    "                    current_weights = layer.weight.data\n",
    "\n",
    "                    # Calculate scaling factor\n",
    "                    non_zero_weights = current_weights[current_weights != 0]\n",
    "                    if  len(non_zero_weights) > 0:\n",
    "                        w_min = torch.min(torch.abs(non_zero_weights)).item()\n",
    "                        if w_min > 0:\n",
    "                            scaling_factor = 10 ** np.floor(np.log10(w_min))\n",
    "                        else:\n",
    "                            scaling_factor = 1e-6\n",
    "                    else:\n",
    "                        scaling_factor = 1e-6\n",
    "\n",
    "                f_scaled = torch.tensor(f_importance * scaling_factor, dtype=torch.float32, device=current_weights.device)\n",
    "\n",
    "                # Add scaled feature importance to weights\n",
    "                # Broadcast across th weight matrix\n",
    "\n",
    "                for i in range(current_weights.shape[0]):\n",
    "                    current_weights[i, :] += f_scaled\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XBNetTrainer:\n",
    "    \"\"\"\n",
    "    Training class for XBNet neural network architecture.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, learning_rate=0.001, weight_deacay=1e-5, device=None):\n",
    "        \"\"\"\n",
    "        Initialize trainer.\n",
    "\n",
    "        Parameters:\n",
    "        - model: XBNet model\n",
    "        - learning_rate: Learning rate for the model to learn\n",
    "        - weight_decay: L2 regularization weight\n",
    "        - device: Device to run training on\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "        self.model = model\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Initialiaze optimizer and loss function\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_deacay )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=10, verbose=True\n",
    "        )\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch):\n",
    "        \"\"\" Train for one epoch \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        progress_bar = tqdm(dataloader,desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "        for batch_idx, (data,target) in enumerate(progress_bar):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with activation storage\n",
    "            output = self.model(data,store_activations=True)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.criterion(output,target)\n",
    "\n",
    "            # Train trees on hidden layer outputs (Boosted Gradient Descent 1)\n",
    "            self.model.train_trees_on_hidden_layers(target)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Step 1: Standard gradient descent update\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Step 2: Update weights with feature importance (BGD Step 2)\n",
    "            self.model.update_weights_with_featre_importance()\n",
    "\n",
    "            # Statistics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data,1)\n",
    "            total_samples += target.size(0)\n",
    "            correct_predictions += (predicted==target).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Accuracy': f'{100. * correct_predictions / total_samples:.2f}%'\n",
    "            })\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = 100. * correct_predictions /total_samples\n",
    "\n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def validate(self, dataloader):\n",
    "        \"\"\" Validate the model \"\"\"  \n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0 \n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in dataloader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "                # Forward pass without activation storage (faster)\n",
    "                output = self.model(data,store_activations = False)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.criterion(output, target)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Predictiosn\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total_samples += target.size(0)\n",
    "                correct_predictions += (predicted==target).sum().item()\n",
    "\n",
    "        avg_loss = total_loss/ len(dataloader)\n",
    "        accuracy = 100. * correct_predictions / total_samples\n",
    "\n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def fit(self,train_loader, val_loader, epochs=100, patience=40, verbose=True):\n",
    "        \"\"\"\n",
    "        \n",
    "        Train the XBNet model.\n",
    "\n",
    "        Parameters:\n",
    "        - train_loader: Training data loader\n",
    "        - val_loader: Validation data loader\n",
    "        - epochs: Number of training epochs\n",
    "        - verbose: Whether to print the progress or not\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Starting XBNet training for {epochs} epochs ...\")\n",
    "\n",
    "        train_losses, train_accuracies = [],[]\n",
    "        val_losses, val_accuracies = [], []\n",
    "\n",
    "        best_val_accuracy = 0.0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch(train_loader,epoch)\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate(val_loader)\n",
    "\n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # Record history\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "\n",
    "            # Early stopping\n",
    "            if val_acc > best_val_accuracy:\n",
    "                best_val_accuracy = val_acc\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_xbnet_model.pth')\n",
    "            else:\n",
    "                patience_counter+=1\n",
    "\n",
    "            if verbose:\n",
    "                print(f'Epoch {epoch + 1}/{epochs}:')\n",
    "                print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "                print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "                print(f'  Best Val Acc: {best_val_accuracy:.2f}%')\n",
    "                print('-' * 60)\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "\n",
    "        # Load the best model\n",
    "        self.model.load_state_dict(torch.load('best_xbnet_mdeol.pth'))\n",
    "\n",
    "        # Store training history\n",
    "        self.model.loss_history = train_losses\n",
    "        self.model.accuracy_history = train_accuracies\n",
    "        self.val_loss_history = val_losses\n",
    "        self.val_accuracy_history = val_accuracies\n",
    "\n",
    "        print(\"XBNet training completed!\")\n",
    "        print(f\"Best validation accuracy: {best_val_accuracy:.4f}%\")\n",
    "\n",
    "\n",
    "    def predict(self,dataloader):\n",
    "        \"\"\" Make predictions on a dataset \"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, _ in dataloader:\n",
    "                data = data.to(self.device)\n",
    "                output = self.model(data, store_activations=False) # We are not training, so we don't need the activation funcitons\n",
    "\n",
    "                # Get probabilities and predictions\n",
    "                probs = F.softmax(output,dim=1)\n",
    "                _, preds = torch.max(output,1)\n",
    "\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                probabilities.extend(probs.cpu().numpy())\n",
    "\n",
    "        return np.array(predictions), np.array(probabilities)\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\" Plot training and validaiton history  \"\"\"\n",
    "        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "\n",
    "        # Loss plot\n",
    "        ax1.plot(self.model.loss_history, label='Train Loss', color='blue')\n",
    "        ax1.plot(self.val_loss_history, label='Val Loss',color = 'red')\n",
    "        ax1.set_title('XBNet Training and Validation Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.legend()\n",
    "\n",
    "\n",
    "        # Accuracy Plot\n",
    "        ax2.plot(self.model.accuracy_history, label='Train Accuracy', color='blue')\n",
    "        ax2.plot(self.val_accuracy_history, label='Val Accuracy',color = 'red')\n",
    "        ax2.set_title('XBNet Training and Validation Accuracies')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel(\"Accuracy %\")\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def create_dataloaders(X_train,y_train,X_val, y_val, X_test, y_test, batch_size=64):\n",
    "    \"\"\"Create Pytorch data loaders\"\"\"\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor,y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor,y_test_tensor)\n",
    "\n",
    "    # Create Data loaders\n",
    "    train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def implement_xbnet_pytorch():\n",
    "    \"\"\"Run the xbnet model on the datast\"\"\"\n",
    "\n",
    "    # Read the data \n",
    "    df = pd.read_csv(\"/home/chandru/binp37/results/metasub/metasub_training_testing_data.csv\")\n",
    "    df = pd.concat([df.iloc[:,:-4],df['continent']],axis=1)\n",
    "    x_data = df[df.columns[:-1]][:1000]\n",
    "    print(x_data.shape)\n",
    "    y_data = df[df.columns[-1]][:1000]\n",
    "    le = LabelEncoder()\n",
    "    y_data = le.fit_transform(y_data)\n",
    "    print(le.classes_)\n",
    "\n",
    "    # Train-test-split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_data,y_data,random_state=123,test_size=0.2)\n",
    "    # Split train into train and validation as well\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=123, test_size=0.2)\n",
    "\n",
    "    print('Training, Validation and Testing matrices shapes')\n",
    "    print(\"\\nTraining\\n\")\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(\"\\nValidation\\n\")\n",
    "    print(X_val.shape, y_val.shape)\n",
    "    print(\"\\nTesting\\n\")\n",
    "    print(X_test.shape, y_test.shape)\n",
    "\n",
    "    # Create dataloader\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(X_train,y_train, X_val, y_val, X_test, y_test, batch_size=64)\n",
    "\n",
    "    # Initialiaze XBNet model\n",
    "    model = XBNet(\n",
    "        input_size=200,\n",
    "        hidden_layers=[400,200],\n",
    "        num_classes=7,\n",
    "        n_estimators=50,\n",
    "        max_depth=3,\n",
    "        dropout_rate=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialiaze first layer with feature importance\n",
    "    model.initialize_first_layer_with_feature_importance(X_train,y_train)\n",
    "\n",
    "\n",
    "    # Initialiaze traineer\n",
    "    trainer = XBNetTrainer(model,learning_rate=0.001,weight_deacay=1e-5)\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(train_loader,val_loader,epochs=100,verbose=True,patience=40)\n",
    "\n",
    "    # Make predictions on test set\n",
    "    test_predictions, test_probabilities = trainer.predict(test_loader)\n",
    "\n",
    "    # Evaluate performance\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    print(f\"\\nXBNet Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report\")\n",
    "    print(classification_report(y_test,test_predictions))\n",
    "\n",
    "    trainer.plot_training_history()\n",
    "\n",
    "    return model, trainer, test_predictions, test_probabilities\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Run the test\n",
    "    model, trainer, predictions, probabilities = implement_xbnet_pytorch()\n",
    "    print(\"\\nXBNet PyTorch implementation test completed!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binp37_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
