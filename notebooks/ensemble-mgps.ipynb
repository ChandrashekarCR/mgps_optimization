{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12339762,"sourceType":"datasetVersion","datasetId":7779040}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T14:17:51.128699Z","iopub.execute_input":"2025-08-06T14:17:51.128884Z","iopub.status.idle":"2025-08-06T14:17:52.818353Z","shell.execute_reply.started":"2025-08-06T14:17:51.128868Z","shell.execute_reply":"2025-08-06T14:17:52.817556Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/metasub-data/metasub_training_testing_data.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"# --- SYSTEM LIBS (optional but may be required for geospatial stuff) ---\n!apt-get update -y && apt-get install -y \\\n    libgl1-mesa-glx \\\n    libglib2.0-0 \\\n    libsm6 \\\n    libxrender1 \\\n    libxext6 \\\n    libgeos-dev \\\n    gdal-bin \\\n    libgdal-dev \\\n    python3-gdal\n\n# --- PYTHON LIBRARIES ---\n\n!pip install --quiet \\\n    numpy \\\n    pandas \\\n    seaborn \\\n    matplotlib \\\n    scikit-learn \\\n    imbalanced-learn \\\n    xgboost==1.7.6 \\\n    lightgbm \\\n    catboost \\\n    optuna \\\n    shapely \\\n    geopandas \\\n    geopy \\\n    tqdm \\\n    folium \\\n    plotly \\\n    tab-transformer-pytorch \\\n    torch torchvision torchaudio \\\n    tabpfn==2.0.9 \\\n    tabpfn-extensions==0.0.4 \\\n    einops \\\n    adjusttext \\\n    sqlalchemy==2.0.41 \\\n    alembic \\\n    colorlog \\\n    huggingface-hub \\\n    fsspec \\\n    pyyaml \\\n    sympy \\\n    filelock \\\n    mako \\\n    tabpfn_client","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T14:17:52.819190Z","iopub.execute_input":"2025-08-06T14:17:52.820029Z","iopub.status.idle":"2025-08-06T14:20:23.410412Z","shell.execute_reply.started":"2025-08-06T14:17:52.820010Z","shell.execute_reply":"2025-08-06T14:20:23.409651Z"}},"outputs":[{"name":"stdout","text":"Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\nGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\nGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,918 kB]\nHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease                         \nGet:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \nGet:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \nGet:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]           \nGet:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,773 kB]\nGet:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\nGet:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]     \nGet:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,103 kB]\nGet:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,575 kB]\nHit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease   \nGet:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.9 kB]\nGet:16 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [51.0 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,518 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [75.9 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,290 kB]\nGet:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,270 kB]\nGet:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\nGet:22 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\nGet:23 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [48.5 kB]\nGet:24 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,207 kB]\nGet:25 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,161 kB] \nFetched 34.6 MB in 5s (7,392 kB/s)                                             \nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nlibsm6 is already the newest version (2:1.2.3-1build2).\nlibxext6 is already the newest version (2:1.3.4-1build1).\nlibxrender1 is already the newest version (1:0.9.10-1build4).\nlibglib2.0-0 is already the newest version (2.72.4-0ubuntu2.5).\nlibgl1-mesa-glx is already the newest version (23.0.4-0ubuntu1~22.04.1).\nlibgdal-dev is already the newest version (3.8.4+dfsg-1~jammy0).\nlibgeos-dev is already the newest version (3.12.1-1~jammy0).\nlibgeos-dev set to manually installed.\nThe following additional packages will be installed:\n  python3-numpy\nSuggested packages:\n  libgdal-grass python-numpy-doc python3-pytest\nThe following NEW packages will be installed:\n  gdal-bin python3-gdal python3-numpy\n0 upgraded, 3 newly installed, 0 to remove and 91 not upgraded.\nNeed to get 5,168 kB of archives.\nAfter this operation, 25.6 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-numpy amd64 1:1.21.5-1ubuntu22.04.1 [3,467 kB]\nGet:2 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-gdal amd64 3.8.4+dfsg-1~jammy0 [1,095 kB]\nGet:3 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 gdal-bin amd64 3.8.4+dfsg-1~jammy0 [605 kB]\nFetched 5,168 kB in 7s (704 kB/s)                                              \nSelecting previously unselected package python3-numpy.\n(Reading database ... 128663 files and directories currently installed.)\nPreparing to unpack .../python3-numpy_1%3a1.21.5-1ubuntu22.04.1_amd64.deb ...\nUnpacking python3-numpy (1:1.21.5-1ubuntu22.04.1) ...\nSelecting previously unselected package python3-gdal.\nPreparing to unpack .../python3-gdal_3.8.4+dfsg-1~jammy0_amd64.deb ...\nUnpacking python3-gdal (3.8.4+dfsg-1~jammy0) ...\nSelecting previously unselected package gdal-bin.\nPreparing to unpack .../gdal-bin_3.8.4+dfsg-1~jammy0_amd64.deb ...\nUnpacking gdal-bin (3.8.4+dfsg-1~jammy0) ...\nSetting up python3-numpy (1:1.21.5-1ubuntu22.04.1) ...\nSetting up python3-gdal (3.8.4+dfsg-1~jammy0) ...\nSetting up gdal-bin (3.8.4+dfsg-1~jammy0) ...\nProcessing triggers for man-db (2.10.2-1) ...\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ==================== OS & Warnings ====================\nimport os\nimport copy\nimport time\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ==================== Logging ====================\nimport logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# ==================== Numerical & Data Processing ====================\nimport numpy as np\nimport pandas as pd\n\n# ==================== Visualization ====================\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ==================== Scikit-learn ====================\nfrom sklearn.model_selection import (\n    train_test_split, StratifiedKFold, StratifiedShuffleSplit, KFold, cross_val_score\n)\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score, classification_report, confusion_matrix,\n    log_loss, mean_squared_error, mean_absolute_error, r2_score\n)\n\n# ==================== Imbalanced Data ====================\nfrom imblearn.over_sampling import SMOTE\n\n# ==================== Geospatial ====================\nimport geopandas as gpd\nfrom shapely.geometry import Point\nfrom scipy.spatial import cKDTree\nfrom geopy.distance import geodesic \n\n# ==================== Machine Learning Libraries ====================\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nimport optuna\n\n# ==================== Deep Learning - PyTorch ====================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils import clip_grad_norm_\n\n# ==================== TabPFN ====================\nfrom tabpfn import TabPFNClassifier, TabPFNRegressor\nfrom tabpfn_extensions.post_hoc_ensembles.sklearn_interface import AutoTabPFNClassifier\nfrom tabpfn_extensions.hpo import TunedTabPFNClassifier\nfrom tabpfn_extensions.many_class.many_class_classifier import ManyClassClassifier\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T14:20:23.412372Z","iopub.execute_input":"2025-08-06T14:20:23.412617Z","iopub.status.idle":"2025-08-06T14:20:34.752242Z","shell.execute_reply.started":"2025-08-06T14:20:23.412571Z","shell.execute_reply":"2025-08-06T14:20:34.751630Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# CatBoost","metadata":{}},{"cell_type":"code","source":"class CatBoostClassifierOptimizer:\n    def __init__(self, X_train, y_train, X_test, y_test, random_state=42, n_trials=20, timeout=1200, cat_features=None):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        self.random_state = random_state\n        self.n_trials = n_trials\n        self.timeout = timeout\n        self.cat_features = cat_features\n        self.best_params = None\n        self.final_model = None\n\n    def default_params(self):\n        return {\n            'loss_function': 'MultiClass',\n            'iterations': 300,\n            'learning_rate': 0.1,\n            'depth': 6,\n            'l2_leaf_reg': 3.0,\n            'random_seed': self.random_state,\n            'verbose': False\n        }\n\n    def objective(self, trial):\n        params = {\n            'loss_function': 'MultiClass',\n            'iterations': trial.suggest_int('iterations', 100, 400),\n            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n            'depth': trial.suggest_int('depth', 3, 10),\n            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10.0),\n            'random_seed': self.random_state,\n            'verbose': False\n        }\n        model = CatBoostClassifier(**params)\n        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n        # For categorical features, use .fit(X, y, cat_features=cat_features)\n        scores = cross_val_score(model, self.X_train, self.y_train, cv=skf, scoring='accuracy', fit_params={'cat_features': self.cat_features})\n        return scores.mean()\n\n    def tune(self):\n        study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2))\n        study.optimize(self.objective, n_trials=self.n_trials, timeout=self.timeout)\n        self.best_params = study.best_params\n        self.best_params.update({\n            'loss_function': 'MultiClass',\n            'random_seed': self.random_state,\n            'verbose': False\n        })\n        return self.best_params\n\n    def train(self, params):\n        model = CatBoostClassifier(**params)\n        model.fit(self.X_train, self.y_train, cat_features=self.cat_features)\n        self.final_model = model\n        return model\n\n    def evaluate(self, model=None):\n        if model is None:\n            model = self.final_model\n        preds = model.predict(self.X_test)\n        probs = model.predict_proba(self.X_test)\n        acc = accuracy_score(self.y_test, preds)\n        print(\"\\nClassification Report:\")\n        print(classification_report(self.y_test, preds))\n        print(f\"\\nAccuracy: {acc:.4f}\")\n        return preds, probs, acc\n\n\ndef run_catboost_classifier(X_train, y_train, X_test, y_test, \n                           tune_hyperparams=False, random_state=42, \n                           n_trials=20, timeout=1200, params=None, verbose=False):\n    \"\"\"\n    CatBoost classification wrapper for ensemble.\n    \"\"\"\n    tuner = CatBoostClassifierOptimizer(X_train, y_train, X_test, y_test, \n                         random_state=random_state, n_trials=n_trials,timeout=timeout,cat_features=None)\n\n    if tune_hyperparams:\n        best_params = tuner.tune()\n        if verbose:\n            print(\"Using tuned parameters:\", best_params)\n    else:\n        best_params = tuner.default_params()\n        if params:\n            best_params.update(params)\n        if verbose:\n            print(\"Using default (or custom) parameters:\", best_params)\n\n    model = tuner.train(best_params)\n    preds, probs, acc = tuner.evaluate(model) if verbose else (model.predict(X_test), model.predict_proba(X_test), accuracy_score(y_test, model.predict(X_test)))\n    \n    if verbose:\n        return {\n            'model': model,\n            'predictions': preds,\n            'predicted_probabilities': probs,\n            'accuracy': acc,\n            'params': best_params\n        }\n    else:\n        return {\n            'model': model,\n            'predictions': preds,\n            'predicted_probabilities': probs,\n            'accuracy': acc,\n            'params': best_params\n        }\n\n\n\nclass CatBoostRegressionOptimizer:\n    def __init__(self, X_train, y_train, X_test, y_test,\n                 random_state=42, n_trials=20, timeout=1200):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        self.random_state = random_state\n        self.n_trials = n_trials\n        self.timeout = timeout\n        self.best_params = None\n        self.final_model = None\n\n    def default_params(self):\n        return {\n            'loss_function': 'RMSE',\n            'eval_metric': 'RMSE',\n            'learning_rate': 0.1,\n            'depth': 6,\n            'l2_leaf_reg': 3,\n            'random_strength': 1,\n            'bagging_temperature': 1,\n            'border_count': 254,\n            'iterations': 300,\n        }\n\n    def objective(self, trial):\n        params = {\n            'loss_function': 'RMSE',\n            'eval_metric': 'RMSE',\n            'learning_rate': trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n            'depth': trial.suggest_int(\"depth\", 3, 10),\n            'l2_leaf_reg': trial.suggest_float(\"l2_leaf_reg\", 1, 10),\n            'random_strength': trial.suggest_float(\"random_strength\", 1e-9, 10, log=True),\n            'bagging_temperature': trial.suggest_float(\"bagging_temperature\", 0, 10),\n            'border_count': trial.suggest_int(\"border_count\", 1, 255),\n            'iterations': trial.suggest_int(\"iterations\", 100, 500),\n        }\n\n        model = CatBoostRegressor(**params, random_seed=self.random_state, verbose=False)\n        kf = KFold(n_splits=5, shuffle=True, random_state=self.random_state)\n        scores = cross_val_score(model, self.X_train, self.y_train, cv=kf, scoring='neg_mean_absolute_error')\n        return np.mean(scores)\n\n    def tune(self):\n        study = optuna.create_study(direction='maximize',\n                                    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2))\n        study.optimize(self.objective, n_trials=self.n_trials, timeout=self.timeout)\n        self.best_params = study.best_params\n        self.best_params.update({\n            'loss_function': 'RMSE',\n            'eval_metric': 'RMSE',\n        })\n        return self.best_params\n\n    def train(self, params):\n        model = CatBoostRegressor(**params, random_seed=self.random_state, verbose=False)\n        model.fit(self.X_train, self.y_train)\n        self.final_model = model\n        return model\n\n    def evaluate(self, model=None):\n        if model is None:\n            model = self.final_model\n        preds = model.predict(self.X_test)\n        mae = mean_absolute_error(self.y_test, preds)\n        r2 = r2_score(self.y_test, preds)\n        print(\"\\nRegression Report:\")\n        print(f\"MAE:  {mae:.4f}\")\n        print(f\"R2:   {r2:.4f}\")\n        return preds, mae, r2\n\n\ndef run_catboost_regressor(X_train, y_train, X_test, y_test,\n                           tune_hyperparams=False, random_state=42,\n                           n_trials=20, timeout=1200, params=None, verbose=True):\n    \"\"\"CatBoost regression wrapper for ensemble with proper error handling\"\"\"\n    \n    try:\n        # Handle multi-dimensional targets\n        if len(y_train.shape) > 1 and y_train.shape[1] > 1:\n            if verbose:\n                print(\"Warning: CatBoost doesn't support multi-output regression natively. Using first dimension only.\")\n            y_train = y_train[:, 0]\n            y_test = y_test[:, 0]\n\n        tuner = CatBoostRegressionOptimizer(X_train, y_train, X_test, y_test,\n                                       random_state=random_state, n_trials=n_trials, timeout=timeout)\n\n        if tune_hyperparams:\n            best_params = tuner.tune()\n            if verbose:\n                print(\"Using tuned parameters:\", best_params)\n        else:\n            best_params = tuner.default_params()\n            if params:\n                best_params.update(params)\n            if verbose:\n                print(\"Using default (or custom) parameters:\", best_params)\n\n        model = tuner.train(best_params)\n        preds, mae, r2 = tuner.evaluate(model) if verbose else (model.predict(X_test), None, None)\n\n        # Calculate additional metrics if not verbose\n        if not verbose:\n            mae = mean_absolute_error(y_test, preds)\n            r2 = r2_score(y_test, preds)\n\n        return {\n            'model': model,\n            'predictions': preds,\n            'mae': mae,\n            'r2_score': r2,  # Use r2_score for consistency\n            'params': best_params,\n            'skipped': False\n        }\n        \n    except Exception as e:\n        if verbose:\n            print(f\"Error in CatBoost regressor: {e}\")\n        # Return dummy predictions on error\n        n_samples = X_test.shape[0]\n        dummy_preds = np.zeros(n_samples)\n        \n        return {\n            'model': None,\n            'predictions': dummy_preds,\n            'mae': float('inf'),\n            'r2_score': -float('inf'),\n            'params': params,\n            'skipped': True,\n            'error': str(e)\n        }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T14:20:34.753009Z","iopub.execute_input":"2025-08-06T14:20:34.753650Z","iopub.status.idle":"2025-08-06T14:20:34.775956Z","shell.execute_reply.started":"2025-08-06T14:20:34.753623Z","shell.execute_reply":"2025-08-06T14:20:34.775194Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# GrowNet","metadata":{}},{"cell_type":"code","source":"# This model is used for classification tasks\ndef grownet_classification_default_params():\n    return {\n        \"hidden_size\": 256,\n        \"num_nets\": 10,\n        \"boost_rate\": 0.4,\n        \"lr\": 1e-3,\n        \"weight_decay\": 1e-5,\n        \"batch_size\": 128,\n        \"epochs_per_stage\": 30,\n        \"early_stopping_steps\": 7,\n        \"gradient_clip\": 1.0,\n        \"val_split\": 0.2,\n        \"test_split\": 0.2,\n        \"random_state\": 42,\n    }\n\n\nclass GrowNetClassificationTuner:\n    def __init__(self, X_train, y_train, X_val, y_val, params, device=\"cpu\",n_trials = 20, timeout=1200):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_val = X_val\n        self.y_val = y_val\n        self.params = params\n        self.device = device\n        self.n_trials = n_trials\n        self.timeout = timeout\n        self.best_params = None\n        self.best_score = None \n\n    def objective(self,trial):\n        # Suggest hyperparameters\n        params = self.params.copy()\n        params.update({\n            \"hidden_size\": trial.suggest_categorical(\"hidden_size\", [128, 256, 512]),\n            \"num_nets\": trial.suggest_int(\"num_nets\", 10, 30),\n            \"boost_rate\": trial.suggest_float(\"boost_rate\", 0.1, 0.8),\n            \"lr\": trial.suggest_loguniform(\"lr\", 1e-4, 1e-2),\n            \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128, 256]),\n            \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3),\n            \"epochs_per_stage\": trial.suggest_int(\"epochs_per_stage\", 5, 10),\n            \"gradient_clip\": trial.suggest_float(\"gradient_clip\", 0.5, 2.0),\n        })\n\n        # Train model\n        model = GrowNetClassifierUnique(params, device=self.device)\n        model.fit(self.X_train, self.y_train, X_val=self.X_val, y_val=self.y_val)\n        val_metrics = model.evaluate(self.X_val, self.y_val)\n        val_acc = val_metrics['class_accuracy']\n        return val_acc  # maximize accuracy\n\n    def tune(self):\n        study = optuna.create_study(direction='maximize')\n        study.optimize(self.objective, n_trials=self.n_trials, timeout=self.timeout)\n        \n        self.best_params = study.best_params\n        self.best_score = study.best_value\n        \n        print(f\"Best score: {self.best_score:.4f}\")\n        print(f\"Best parameters: {self.best_params}\")\n        \n        return self.best_params, self.best_score\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Dataset class for continent classification\nclass GrowNetClassificationTrainDataset(Dataset):\n    def __init__(self, features, n_targets):\n        self.features = features\n        self.n_targets = n_targets\n\n    def __len__(self):\n        return self.features.shape[0]\n    \n    def __getitem__(self, idx):\n        return {\n            'x': torch.tensor(self.features[idx], dtype=torch.float),\n            'n_classes': torch.tensor(self.n_targets[idx], dtype=torch.float)\n        }\n\n    \n# DynamicNet for classification\nclass GrowNetClassificationDynamicNet(nn.Module):\n    def __init__(self, c0_classes, lr):\n        super(GrowNetClassificationDynamicNet,self).__init__()\n        self.models = []\n        self.c0_classes = c0_classes\n        self.lr = lr\n\n        self.boost_rate = nn.Parameter(torch.tensor(lr,requires_grad=True,device=device))\n\n    def to(self,device):\n        self.c0_classes = self.c0_classes.to(device)\n        self.boost_rate = self.boost_rate.to(device)\n        for m in self.models:\n            m.to(device)\n    \n    def add(self,model):\n        self.models.append(model)\n\n    def parameters(self):\n        params = []\n        for m in self.models:\n            params.extend(m.parameters())\n        params.append(self.boost_rate)\n        return params\n    \n    def zero_grad(self):\n        for m in self.models:\n            m.zero_grad()\n\n    def to_eval(self):\n        for m in self.models:\n            m.eval()\n    \n    def to_train(self):\n        for m in self.models:\n            m.train(True)\n\n    def forward(self,x):\n        if len(self.models) == 0:\n            batch = x.shape[0]\n            c0_classes = self.c0_classes.repeat(batch,1)\n            return None, c0_classes\n        \n        middle_feat_cum = None\n        classes_pred = None\n\n        with torch.no_grad():\n            for m in self.models:\n                if middle_feat_cum is None:\n                    middle_feat_cum, classes_out = m(x,middle_feat_cum)\n                    classes_pred = classes_out\n                else:\n                    middle_feat_cum, classes_out = m(x,middle_feat_cum)\n                    classes_pred += classes_out\n\n        final_classes = self.c0_classes + self.boost_rate * classes_pred\n        return middle_feat_cum, final_classes\n    \n    def forward_grad(self,x):\n        if len(self.models) == 0:\n            batch = x.shape[0]\n            c0_classes = self.c0_classes.repeat(batch,1)\n            return None, c0_classes\n        \n        middle_feat_cum = None\n        classes_pred = None\n\n        for m in self.models:\n            if middle_feat_cum is None:\n                middle_feat_cum, classes_out = m(x,middle_feat_cum)\n                classes_pred = classes_out\n            else:\n                middle_feat_cum, classes_out = m(x,middle_feat_cum)\n                classes_pred += classes_out\n\n        final_classes = self.c0_classes + self.boost_rate * classes_pred\n        return middle_feat_cum, final_classes\n\n\nclass GrowNetClassificationMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n        super(GrowNetClassificationMLP,self).__init__()\n        self.bn = nn.BatchNorm1d(input_dim)\n\n        # Simple feedforward layers\n        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n        self.relu1 = nn.ReLU()\n        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n        self.dropout1 = nn.Dropout(0.2)\n        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n        self.relu2 = nn.ReLU()\n        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n        self.dropout2 = nn.Dropout(0.4)\n        self.class_head = nn.Linear(hidden_dim2, output_dim)\n\n    def forward(self,x, lower_f): # In a hierarchical network each new model can receive not just original input but also previously learned features\n        if lower_f is not None:\n            x = torch.cat([x,lower_f],dim=1)\n            x = self.bn(x)\n\n        # Simple feedforward\n        x = self.dropout1(self.relu1(self.fc1(x)))\n        x = self.bn1(x)\n        x = self.dropout2(self.relu2(self.fc2(x)))\n        x = self.bn2(x)\n        shared_features = x\n\n        # Prediction\n        n_classes = self.class_head(shared_features)\n\n        return shared_features, n_classes\n    \n    @classmethod\n    def get_model(cls,stage,params):\n        if stage == 0:\n            dim_in = params['feat_d']\n        else:\n            dim_in = params['feat_d'] + params['hidden_size']\n\n        model = cls(\n            dim_in,\n            params['hidden_size'],\n            params['hidden_size'],\n            params['n_classes']\n        )\n\n        return model\n\n\nclass GrowNetClassifierUnique:\n    def __init__(self, params = None, device=\"cpu\"):\n        if params is None:\n            self.params = grownet_classification_default_params()\n        else:\n            self.params = params\n        self.device = device\n        self.net_ensemble = None\n        self.class_weights_tensor = None\n\n    def _one_hot(self, y):\n        \"\"\"Ensure y is one-hot encoded.\"\"\"\n        if y.ndim == 1:\n            n_classes = np.max(y) + 1\n            return np.eye(n_classes)[y]\n        return y\n\n    def fit(self, X_train, y_train, X_val=None, y_val = None):\n\n        # Determine the input feature size\n        self.params['feat_d'] = X_train.shape[1]\n        \n        # One hot encode for this model\n        y_train = self._one_hot(y_train)\n        if y_val is not None:\n            y_val = self._one_hot(y_val)\n        self.params[\"n_classes\"] = y_train.shape[1]\n\n        # Determine the number of classes\n        self.params['n_classes'] = y_train.shape[1]\n        \n        # Class weighting to deal with imbalanced datasets\n        # Compute and store class weights tensor once\n        class_labels_flat = np.argmax(y_train, axis=1)\n        class_weights = compute_class_weight(\n            class_weight=\"balanced\",\n            classes=np.unique(class_labels_flat),\n            y=class_labels_flat\n        )\n        self.class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(self.device)\n\n\n        # Split validation if not provided\n        if X_val is None or y_val is None:\n            X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,test_size=self.params['val_split'],random_state=self.params['random_state'],stratify=class_labels_flat)\n\n        train_ds = GrowNetClassificationTrainDataset(X_train, y_train)\n        val_ds = GrowNetClassificationTrainDataset(X_val, y_val)\n        train_loader = DataLoader(train_ds, batch_size=self.params['batch_size'], shuffle=True)\n\n        print(f\"Train size: {len(train_ds)}, Val size: {len(val_ds)}\")\n\n        train_loader = DataLoader(train_ds, batch_size=self.params['batch_size'], shuffle=True)\n        \n        # Init ensemble\n        c0_classes = torch.tensor(np.log(np.mean(y_train, axis=0)), dtype=torch.float).unsqueeze(0).to(self.device)\n        self.net_ensemble = GrowNetClassificationDynamicNet(c0_classes, self.params['boost_rate'])\n        self.net_ensemble.to(self.device)\n        \n        best_val_loss = float(\"inf\")\n        best_stage = 0\n        early_stop = 0\n        lr = self.params[\"lr\"]\n        \n        for stage in range(self.params['num_nets']):\n            t0 = time.time()\n            \n            print(f\"\\nTraining weak learner {stage+1}/{self.params['num_nets']}\")\n            model = GrowNetClassificationMLP.get_model(stage, self.params).to(self.device)\n            optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay=self.params['weight_decay'])\n            self.net_ensemble.to_train()\n            \n            stage_train_losses = []\n            \n            for epoch in range(self.params[\"epochs_per_stage\"]):\n                for batch in train_loader:\n                    x = batch[\"x\"].to(self.device)\n                    targets = batch[\"n_classes\"].to(self.device)\n                    \n                    with torch.no_grad():\n                        _, prev_logits = self.net_ensemble.forward(x)\n                        prev_probs = torch.softmax(prev_logits, dim=1)\n                        grad = targets - prev_probs\n                        hessian = prev_probs * (1 - prev_probs)\n                        hessian = hessian.sum(dim=1, keepdim=True)\n                    \n                    middle_feat, logits = model(x, None if stage == 0 else self.net_ensemble.forward_grad(x)[0])\n                    loss_stagewise = nn.MSELoss(reduction=\"none\")\n                    boosting_loss = loss_stagewise(self.net_ensemble.boost_rate * logits, grad)\n                    boosting_loss = (boosting_loss * hessian).mean()\n                    class_loss = F.cross_entropy(logits, torch.argmax(targets, dim=1), weight=self.class_weights_tensor)\n                    total_loss = class_loss * boosting_loss # Optionally combine with boosting_loss\n                    \n                    model.zero_grad()\n                    total_loss.backward()\n                    clip_grad_norm_(model.parameters(), self.params['gradient_clip'])\n                    optimizer.step()\n                    stage_train_losses.append(total_loss.item())\n            \n            self.net_ensemble.add(model)\n            avg_stage_loss = np.mean(stage_train_losses)\n            print(f\"Stage {stage+1} finished | Avg Train Loss: {avg_stage_loss:.5f} | Time: {time.time() - t0:.1f}s\")\n            \n            val_metrics = self.evaluate(X_val, y_val)\n            val_loss = val_metrics['class_loss']\n            print(f\"Validation - Classification Acc: {val_metrics['class_accuracy']:.3f}\")\n            print(f\"Boost rate: {self.net_ensemble.boost_rate.item():.4f}\")\n            \n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                best_stage = stage\n                early_stop = 0\n            else:\n                early_stop += 1\n                if early_stop > self.params[\"early_stopping_steps\"]:\n                    print(\"Early stopping!\")\n                    break\n        print(f\"\\nBest model was at stage {best_stage+1} with Val Loss: {best_val_loss:.5f}\")\n\n\n    def evaluate(self, X, y):\n        self.net_ensemble.to_eval()\n        all_preds = []\n        all_preds_prob = []\n        all_targets = []\n        class_losses = []\n        y = self._one_hot(y)\n\n        loader = DataLoader(GrowNetClassificationTrainDataset(X, y), batch_size=self.params['batch_size'], shuffle=False)\n        with torch.no_grad():\n            for batch in loader:\n                x = batch[\"x\"].to(self.device)\n                targets = batch[\"n_classes\"].to(self.device)\n                _, logits = self.net_ensemble.forward(x)\n                loss = F.cross_entropy(logits, torch.argmax(targets, dim=1), weight=self.class_weights_tensor)\n                class_losses.append(loss.item())\n                preds = torch.argmax(logits, dim=1).cpu().numpy()\n                labs = torch.argmax(targets, dim=1).cpu().numpy()\n                probs = torch.softmax(logits,dim=1).cpu().numpy()\n                all_preds.extend(preds)\n                all_preds_prob.extend(probs)\n                all_targets.extend(labs)\n        acc = accuracy_score(all_targets, all_preds)\n        return {\n            'class_loss': np.mean(class_losses),\n            'class_accuracy': acc,\n            'predictions': all_preds,\n            'probabilities': np.array(all_preds_prob),\n            'targets': all_targets\n        }\n    \n    def predict(self, X):\n        self.net_ensemble.to_eval()\n        all_preds = []\n        all_preds_prob = []\n        loader = DataLoader(GrowNetClassificationTrainDataset(X, np.zeros((X.shape[0], self.params['n_classes']))), batch_size=self.params['batch_size'], shuffle=False)\n        with torch.no_grad():\n            for batch in loader:\n                x = batch[\"x\"].to(self.device)\n                _, logits = self.net_ensemble.forward(x)\n                probs = torch.softmax(logits,dim=1).cpu().numpy()\n                preds = np.argmax(probs, axis=1)\n                all_preds.extend(preds)\n                all_preds_prob.extend(probs)\n        return {\n            'predictions': np.array(all_preds),\n            'probabilities': np.array(all_preds_prob)        \n        }\n    \ndef run_grownet_classifier(X_train,y_train,X_test,y_test,params=None,\n                tune_hyperparams = False, n_trials=20,timeout=1200, verbose=False):\n    # Handle device detection internally\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # Use default if params not given\n    if params is None:\n        params = grownet_classification_default_params()\n    else:\n        default = grownet_classification_default_params()\n        default.update(params)\n        params = default\n    \n    if tune_hyperparams:\n        # Split validation set from training data\n        X_train_split, X_val, y_train_split, y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=42, stratify=y_train)\n\n        tuner = GrowNetClassificationTuner(X_train_split,y_train_split,X_val,y_val,params,device=device,n_trials=n_trials,timeout=timeout)\n        best_params, best_score = tuner.tune()\n        params.update(best_params)\n        if verbose:\n            print(\"Using best params:\", params)\n    \n    # Train final model on full training data\n    model = GrowNetClassifierUnique(params,device=device)\n    model.fit(X_train,y_train)\n    \n    results = model.evaluate(X_test,y_test)\n    if verbose:\n        print(\"\\nClassification Report:\")\n        print(classification_report(results['targets'], results['predictions']))\n        print(\"\\nAccuracy:\", results['class_accuracy'])\n    \n    return {\n        'model': model,\n        'predictions': results['predictions'],\n        'predicted_probabilities': results['probabilities'],\n        'accuracy': results['class_accuracy'],\n        'params': params\n    }\n\n\n\n# Set device globally\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n\ndef grownet_regression_default_params():\n    return {\n        \"hidden_size\": 256,\n        \"num_nets\": 10,\n        \"boost_rate\": 0.4,\n        \"lr\": 1e-3,\n        \"weight_decay\": 1e-5,\n        \"batch_size\": 128,\n        \"epochs_per_stage\": 30,\n        \"early_stopping_steps\": 7,\n        \"gradient_clip\": 1.0,\n        \"val_split\": 0.2,\n        \"test_split\": 0.2,\n        \"random_state\": 42,\n        \"n_outputs\":3\n    }\n\n\nclass GrowNetRegressionTuner:\n    def __init__(self, X_train, y_train, X_val, y_val, params, device=\"cpu\",n_trials = 20, timeout=1200):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_val = X_val\n        self.y_val = y_val\n        self.params = params\n        self.device = device\n        self.n_trials = n_trials\n        self.timeout = timeout\n        self.best_params = None\n        self.best_score = None \n\n    def objective(self,trial):\n        # Suggest hyperparameters\n        params = self.params.copy()\n        params.update({\n            \"hidden_size\": trial.suggest_categorical(\"hidden_size\", [128, 256, 512]),\n            \"num_nets\": trial.suggest_int(\"num_nets\", 10, 30),\n            \"boost_rate\": trial.suggest_float(\"boost_rate\", 0.1, 0.8),\n            \"lr\": trial.suggest_loguniform(\"lr\", 1e-4, 1e-2),\n            \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128, 256]),\n            \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3),\n            \"epochs_per_stage\": trial.suggest_int(\"epochs_per_stage\", 5, 10),\n            \"gradient_clip\": trial.suggest_float(\"gradient_clip\", 0.5, 2.0),\n        })\n\n        # Train model\n        model = GrowNetRegressorUnique(params, device=self.device)\n        model.fit(self.X_train, self.y_train, X_val=self.X_val, y_val=self.y_val)\n        val_metrics = model.evaluate(self.X_val, self.y_val)\n        return -val_metrics['rmse'] # minimize\n\n    def tune(self):\n        study = optuna.create_study(direction='maximize')\n        study.optimize(self.objective, n_trials=self.n_trials, timeout=self.timeout)\n        \n        self.best_params = study.best_params\n        self.best_score = study.best_value\n        \n        print(f\"Best score: {self.best_score:.4f}\")\n        print(f\"Best parameters: {self.best_params}\")\n        \n        return self.best_params, self.best_score\n\n\n# Dataset class for regression\nclass GrowNetRegressionTrainDataset(Dataset):\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return self.features.shape[0]\n    \n    def __getitem__(self, idx):\n        return {\n            'x': torch.tensor(self.features[idx], dtype=torch.float),\n            'y': torch.tensor(self.targets[idx], dtype=torch.float)\n        }\n\n    \n# DynamicNet for regression\nclass GrowNetRegressionDynamicNet(nn.Module):\n    def __init__(self, c0_coords, lr):\n        super(GrowNetRegressionDynamicNet, self).__init__()\n        self.models = nn.ModuleList()\n\n        self.c0_coords = c0_coords\n        self.lr = lr\n        self.boost_rate = nn.Parameter(torch.tensor(lr, requires_grad=True, device=device))\n\n    def to(self, device):\n        self.c0_coords = self.c0_coords.to(device)\n        self.boost_rate = self.boost_rate.to(device)\n        for m in self.models:\n            m.to(device)\n\n    def add(self, model):\n        self.models.append(model)\n\n    def parameters(self):\n        params = []\n        for m in self.models:\n            params.extend(m.parameters())\n        params.append(self.boost_rate)\n        return params\n\n    def zero_grad(self):\n        for m in self.models:\n            m.zero_grad()\n\n    def to_eval(self):\n        for m in self.models:\n            m.eval()\n\n    def to_train(self):\n        for m in self.models:\n            m.train(True)\n\n    def forward(self, x):\n        if len(self.models) == 0:\n            batch = x.shape[0]\n            c0_coords = self.c0_coords.repeat(batch, 1)\n            return None, c0_coords\n\n        middle_feat_cum = None\n        coords_pred = None\n        with torch.no_grad():\n            for m in self.models:\n                if middle_feat_cum is None:\n                    middle_feat_cum, coords_out = m(x, middle_feat_cum)\n                    coords_pred = coords_out\n                else:\n                    middle_feat_cum, coords_out = m(x, middle_feat_cum)\n                    coords_pred += coords_out\n        final_coords = self.c0_coords + self.boost_rate * coords_pred\n        return middle_feat_cum, final_coords\n\n    def forward_grad(self, x):\n        if len(self.models) == 0:\n            batch = x.shape[0]\n            c0_coords = self.c0_coords.repeat(batch, 1)\n            return None, c0_coords\n        middle_feat_cum = None\n        coords_pred = None\n        for m in self.models:\n            if middle_feat_cum is None:\n                middle_feat_cum, coords_out = m(x, middle_feat_cum)\n                coords_pred = coords_out\n            else:\n                middle_feat_cum, coords_out = m(x, middle_feat_cum)\n                coords_pred += coords_out\n        final_coords = self.c0_coords + self.boost_rate * coords_pred\n        return middle_feat_cum, final_coords\n\n\nclass GrowNetRegressionMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n        super(GrowNetRegressionMLP,self).__init__()\n        self.bn = nn.BatchNorm1d(input_dim)\n\n        # Simple feedforward layers\n        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n        self.relu1 = nn.ReLU()\n        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n        self.dropout1 = nn.Dropout(0.2)\n        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n        self.relu2 = nn.ReLU()\n        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n        self.dropout2 = nn.Dropout(0.4)\n        self.reg_head = nn.Linear(hidden_dim2, output_dim)\n\n    def forward(self,x, lower_f):\n        x = self.bn(x)\n        x = self.dropout1(self.relu1(self.fc1(x)))\n        x = self.bn1(x)\n        x = self.dropout2(self.relu2(self.fc2(x)))\n        x = self.bn2(x)\n        coord_out = self.reg_head(x)\n        return None, coord_out\n\n    @classmethod\n    def get_model(cls,stage,params):\n        dim_in = params['feat_d']\n        model = cls(\n            dim_in,\n            params['hidden_size'],\n            params['hidden_size'],\n            params['n_outputs']\n        )\n        return model\n\n\nclass GrowNetRegressorUnique:\n    def __init__(self, params = None, device=\"cpu\"):\n        if params is None:\n            self.params = grownet_regression_default_params()\n        else:\n            self.params = params\n        self.device = device\n        self.net_ensemble = None\n\n    def fit(self,X_train,y_train, X_val=None, y_val = None):\n        # Split validation if not provided\n        if X_val is None or y_val is None:\n            X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,test_size=self.params['val_split'],random_state=self.params['random_state'])\n\n        train_ds = GrowNetRegressionTrainDataset(X_train, y_train)\n        val_ds = GrowNetRegressionTrainDataset(X_val, y_val)\n        train_loader = DataLoader(train_ds, batch_size=self.params['batch_size'], shuffle=True)\n\n        print(f\"Train size: {len(train_ds)}, Val size: {len(val_ds)}\")\n\n        train_loader = DataLoader(train_ds, batch_size=self.params['batch_size'], shuffle=True)\n        \n        # Init ensemble\n        c0 = torch.tensor(np.mean(y_train, axis=0), dtype=torch.float).unsqueeze(0).to(self.device)\n\n        self.net_ensemble = GrowNetRegressionDynamicNet(c0, self.params['boost_rate'])\n        self.net_ensemble.to(self.device)\n        \n        best_val_loss = float(\"inf\")\n        best_stage = 0\n        early_stop = 0\n        lr = self.params[\"lr\"]\n        \n        for stage in range(self.params['num_nets']):\n            t0 = time.time()\n            \n            print(f\"\\nTraining weak learner {stage+1}/{self.params['num_nets']}\")\n            model = GrowNetRegressionMLP.get_model(stage, self.params).to(self.device)\n            optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay=self.params['weight_decay'])\n            self.net_ensemble.to_train()\n            \n            stage_train_losses = []\n            \n            for epoch in range(self.params[\"epochs_per_stage\"]):\n                for batch in train_loader:\n                    x = batch[\"x\"].to(self.device)\n                    targets = batch[\"y\"].to(self.device)\n\n                    with torch.no_grad():\n                        _, prev_preds = self.net_ensemble.forward(x)\n                        grad = targets - prev_preds\n\n                    # Always pass None for lower_f (no feature extraction)\n                    middle_feat, preds = model(x, None)\n                    loss_stagewise = nn.MSELoss(reduction=\"none\")\n                    boosting_loss = loss_stagewise(self.net_ensemble.boost_rate * preds, grad)\n                    boosting_loss = boosting_loss.mean()\n                    total_loss = boosting_loss\n                    \n                    model.zero_grad()\n                    total_loss.backward()\n                    clip_grad_norm_(model.parameters(), self.params['gradient_clip'])\n                    optimizer.step()\n                    stage_train_losses.append(total_loss.item())\n            self.net_ensemble.add(model)\n            avg_stage_loss = np.mean(stage_train_losses)\n            print(f\"Stage {stage+1} finished | Avg Train Loss: {avg_stage_loss:.5f} | Time: {time.time() - t0:.1f}s\")\n            val_metrics = self.evaluate(X_val, y_val)\n            val_loss = val_metrics['rmse']\n            print(f\"Validation - RMSE: {val_loss:.3f}\")\n            print(f\"Boost rate: {self.net_ensemble.boost_rate.item():.4f}\")\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                best_stage = stage\n                early_stop = 0\n            else:\n                early_stop += 1\n                if early_stop > self.params[\"early_stopping_steps\"]:\n                    print(\"Early stopping!\")\n                    break\n        print(f\"\\nBest model was at stage {best_stage+1} with Val RMSE: {best_val_loss:.5f}\")\n\n\n    def evaluate(self, X, y):\n        self.net_ensemble.to_eval()\n        all_preds = []\n        all_targets = []\n        losses = []\n        loader = DataLoader(GrowNetRegressionTrainDataset(X, y), batch_size=self.params['batch_size'], shuffle=False)\n        with torch.no_grad():\n            for batch in loader:\n                x = batch[\"x\"].to(self.device)\n                targets = batch[\"y\"].to(self.device)\n                _, preds = self.net_ensemble.forward(x)\n                loss = F.mse_loss(preds, targets)\n                losses.append(loss.item())\n                all_preds.append(preds.cpu().numpy())\n                all_targets.append(targets.cpu().numpy())\n        all_preds = np.concatenate(all_preds, axis=0)\n        all_targets = np.concatenate(all_targets, axis=0)\n        rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n        r2 = r2_score(all_targets, all_preds)\n        return {\n            'rmse': rmse,\n            'r2': r2,\n            'predictions': all_preds,\n            'targets': all_targets\n        }\n\n    def predict(self, X):\n        self.net_ensemble.to_eval()\n        all_preds = []\n        loader = DataLoader(GrowNetRegressionTrainDataset(X, np.zeros((X.shape[0], self.params['n_outputs']))), batch_size=self.params['batch_size'], shuffle=False)\n        with torch.no_grad():\n            for batch in loader:\n                x = batch[\"x\"].to(self.device)\n                _, preds = self.net_ensemble.forward(x)\n                all_preds.append(preds.cpu().numpy())\n        all_preds = np.concatenate(all_preds, axis=0)\n        return all_preds\n    \ndef run_grownet_regressor(X_train, y_train, X_test, y_test, params=None,\n                          tune_hyperparams=False, n_trials=20, timeout=1200, \n                          device=None, verbose=True):\n    \"\"\"Run GrowNet regressor with proper error handling and interface consistency\"\"\"\n    \n    try:\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            \n        if verbose:\n            print(f\"Running GrowNet regressor on device: {device}\")\n            \n        if params is None:\n            params = grownet_regression_default_params()\n        else:\n            default = grownet_regression_default_params()\n            default.update(params)\n            params = default\n            \n        # Handle both 1D and multi-dimensional targets\n        if len(y_train.shape) == 1:\n            y_train = y_train.reshape(-1, 1)\n            y_test = y_test.reshape(-1, 1)\n            \n        params['feat_d'] = X_train.shape[1]\n        params['n_outputs'] = y_train.shape[1]\n        \n        if tune_hyperparams:\n            X_train_split, X_val, y_train_split, y_val = train_test_split(\n                X_train, y_train, test_size=0.2, random_state=42\n            )\n            tuner = GrowNetRegressionTuner(X_train_split, y_train_split, X_val, y_val, \n                                         params, device=device, n_trials=n_trials, timeout=timeout)\n            best_params, best_score = tuner.tune()\n            params.update(best_params)\n            if verbose:\n                print(\"Using best params:\", params)\n                \n        model = GrowNetRegressorUnique(params, device=device)\n        model.fit(X_train, y_train)\n        results = model.evaluate(X_test, y_test)\n        \n        if verbose:\n            print(\"\\nRegression Report:\")\n            print(f\"RMSE: {results['rmse']:.4f}\")\n            print(f\"R2 Score: {results['r2']:.4f}\")\n            \n        return {\n            'model': model,\n            'predictions': results['predictions'],\n            'rmse': results['rmse'],\n            'r2_score': results['r2'],  # Use r2_score for consistency\n            'params': params,\n            'skipped': False\n        }\n        \n    except Exception as e:\n        if verbose:\n            print(f\"Error in GrowNet regressor: {e}\")\n        # Return dummy predictions on error\n        n_samples = X_test.shape[0]\n        output_dim = y_train.shape[1] if len(y_train.shape) > 1 else 1\n        dummy_preds = np.zeros((n_samples, output_dim))\n        \n        return {\n            'model': None,\n            'predictions': dummy_preds,\n            'rmse': float('inf'),\n            'r2_score': -float('inf'),\n            'params': params,\n            'skipped': True,\n            'error': str(e)\n        }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T14:20:34.777309Z","iopub.execute_input":"2025-08-06T14:20:34.777569Z","iopub.status.idle":"2025-08-06T14:20:34.929257Z","shell.execute_reply.started":"2025-08-06T14:20:34.777552Z","shell.execute_reply":"2025-08-06T14:20:34.928532Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nUsing device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"class LightGBMTuner:\n    def __init__(self, X_train, y_train, X_test, y_test, random_state=42, n_trials=20, timeout=1200):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        self.random_state = random_state\n        self.n_trials = n_trials\n        self.timeout = timeout\n        self.best_params = None\n        self.final_model = None\n\n    def default_params(self):\n        return {\n            'objective': 'multiclass',\n            'num_class': len(np.unique(self.y_train)),\n            'metric': 'multi_logloss',\n            'learning_rate': 0.1,\n            'max_depth': 6,\n            'num_leaves': 31,\n            'min_child_samples': 20,\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'reg_alpha': 0.1,\n            'reg_lambda': 1.0,\n            'n_estimators': 300,\n            'random_state': self.random_state,\n            'min_gain_to_split': 1e-3,  # Suppress split gain warning\n            'verbose': -1  # Suppress LightGBM output\n        }\n\n    def objective(self, trial):\n        params = {\n            'objective': 'multiclass',\n            'num_class': len(np.unique(self.y_train)),\n            'metric': 'multi_logloss',\n            'learning_rate': trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n            'max_depth': trial.suggest_int(\"max_depth\", 3, 12),\n            'num_leaves': trial.suggest_int(\"num_leaves\", 15, 256),\n            'min_child_samples': trial.suggest_int(\"min_child_samples\", 5, 100),\n            'subsample': trial.suggest_float(\"subsample\", 0.5, 1.0),\n            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n            'reg_lambda': trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n            'reg_alpha': trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\n            'n_estimators': trial.suggest_int(\"n_estimators\", 100, 400),\n            'random_state': self.random_state,\n            'min_gain_to_split': 1e-3,  # Suppress split gain warning\n            'verbose': -1  # Suppress LightGBM output\n        }\n        model = lgb.LGBMClassifier(**params)\n        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n        scores = cross_val_score(model, self.X_train, self.y_train, cv=skf, scoring='accuracy')\n        return scores.mean()\n\n    def tune(self):\n        study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2))\n        study.optimize(self.objective, n_trials=self.n_trials, timeout=self.timeout)\n        self.best_params = study.best_params\n        self.best_params.update({\n            'objective': 'multiclass',\n            'num_class': len(np.unique(self.y_train)),\n            'metric': 'multi_logloss',\n            'random_state': self.random_state,\n            'min_gain_to_split': 1e-3,  # Suppress split gain warning\n            'verbose': -1  # Suppress LightGBM output\n        })\n        return self.best_params\n\n    def train(self, params):\n        model = lgb.LGBMClassifier(**params)\n        model.fit(self.X_train, self.y_train)\n        self.final_model = model\n        return model\n\n    def evaluate(self, model=None):\n        if model is None:\n            model = self.final_model\n        preds = model.predict(self.X_test)\n        probs = model.predict_proba(self.X_test)\n        acc = accuracy_score(self.y_test, preds)\n        print(\"\\nClassification Report:\")\n        print(classification_report(self.y_test, preds))\n        print(f\"\\nAccuracy: {acc:.4f}\")\n        return preds, probs, acc\n\n\n\ndef run_lightgbm_classifier(X_train, y_train, X_test, y_test, \n                            tune_hyperparams=False, random_state=42,\n                            n_trials=20, timeout=1200, params=None, verbose=False):\n    tuner = LightGBMTuner(X_train, y_train, X_test, y_test, \n                          random_state=random_state, n_trials=n_trials, timeout=timeout)\n    if tune_hyperparams:\n        best_params = tuner.tune()\n        if verbose:\n            print(\"Using tuned parameters:\", best_params)\n    else:\n        best_params = tuner.default_params()\n        if params:\n            best_params.update(params)\n        if verbose:\n            print(\"Using default (or custom) parameters:\", best_params)\n\n    model = tuner.train(best_params)\n    preds, probs, acc = tuner.evaluate(model) if verbose else (model.predict(X_test), model.predict_proba(X_test), accuracy_score(y_test, model.predict(X_test)))\n    \n    if verbose:\n        return {\n            'model': model,\n            'predictions': preds,\n            'predicted_probabilities': probs,\n            'accuracy': acc,\n            'params': best_params\n        }\n    else:\n        return {\n            'model': model,\n            'predictions': preds,\n            'predicted_probabilities': probs,\n            'accuracy': acc,\n            'params': best_params\n        }\n\n\nclass LightGBMRegressorTuner:\n    def __init__(self, X_train, y_train, X_test, y_test,\n                 random_state=42, n_trials=20, timeout=1200):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        self.random_state = random_state\n        self.n_trials = n_trials\n        self.timeout = timeout\n        self.best_params = None\n        self.final_model = None\n\n    def default_params(self):\n        return {\n            'objective': 'regression',\n            'metric': 'rmse',\n            'boosting_type': 'gbdt',\n            'learning_rate': 0.1,\n            'max_depth': 6,\n            'min_child_samples': 20,\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'reg_lambda': 1.0,\n            'reg_alpha': 0.0,\n            'n_estimators': 300,\n        }\n\n    def objective(self, trial):\n        params = {\n            'objective': 'regression',\n            'metric': 'rmse',\n            'boosting_type': 'gbdt',\n            'learning_rate': trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n            'max_depth': trial.suggest_int(\"max_depth\", 3, 12),\n            'min_child_samples': trial.suggest_int(\"min_child_samples\", 5, 100),\n            'subsample': trial.suggest_float(\"subsample\", 0.5, 1.0),\n            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n            'reg_lambda': trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n            'reg_alpha': trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\n            'n_estimators': trial.suggest_int(\"n_estimators\", 100, 400),\n        }\n\n        model = lgb.LGBMRegressor(**params, random_state=self.random_state, verbose=-1)\n        kf = KFold(n_splits=5, shuffle=True, random_state=self.random_state)\n        scores = cross_val_score(model, self.X_train, self.y_train, cv=kf, scoring='neg_mean_absolute_error')\n        return np.mean(scores)\n\n    def tune(self):\n        study = optuna.create_study(direction='maximize',\n                                    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2))\n        study.optimize(self.objective, n_trials=self.n_trials, timeout=self.timeout)\n        self.best_params = study.best_params\n        self.best_params.update({\n            'objective': 'regression',\n            'metric': 'rmse',\n            'boosting_type': 'gbdt'\n        })\n        return self.best_params\n\n    def train(self, params):\n        model = lgb.LGBMRegressor(**params, random_state=self.random_state)\n        model.fit(self.X_train, self.y_train)\n        self.final_model = model\n        return model\n\n    def evaluate(self, model=None):\n        if model is None:\n            model = self.final_model\n        preds = model.predict(self.X_test)\n        mae = mean_absolute_error(self.y_test, preds)\n        r2 = r2_score(self.y_test, preds)\n        print(\"\\nRegression Report:\")\n        print(f\"MAE:  {mae:.4f}\")\n        print(f\"R2:   {r2:.4f}\")\n        return preds, mae, r2\n\n\ndef run_lightgbm_regressor(X_train, y_train, X_test, y_test,\n                          tune_hyperparams=False, random_state=42,\n                          n_trials=20, timeout=1200, params=None, verbose=True):\n    \"\"\"LightGBM regressor with proper error handling\"\"\"\n    \n    try:\n        # Handle multi-dimensional targets\n        if len(y_train.shape) > 1 and y_train.shape[1] > 1:\n            if verbose:\n                print(\"Warning: LightGBM doesn't support multi-output regression natively. Using first dimension only.\")\n            y_train = y_train[:, 0]\n            y_test = y_test[:, 0]\n\n        tuner = LightGBMRegressorTuner(X_train, y_train, X_test, y_test,\n                                      random_state=random_state, n_trials=n_trials, timeout=timeout)\n\n        if tune_hyperparams:\n            best_params = tuner.tune()\n            if verbose:\n                print(\"Using tuned parameters:\", best_params)\n        else:\n            best_params = tuner.default_params()\n            if params:\n                best_params.update(params)\n            if verbose:\n                print(\"Using default (or custom) parameters:\", best_params)\n\n        model = tuner.train(best_params)\n        preds, mae, r2 = tuner.evaluate(model) if verbose else (model.predict(X_test), None, None)\n\n        # Calculate additional metrics if not verbose\n        if not verbose:\n            mae = mean_absolute_error(y_test, preds)\n            r2 = r2_score(y_test, preds)\n\n        return {\n            'model': model,\n            'predictions': preds,\n            'mae': mae,\n            'r2_score': r2,  # Use r2_score for consistency\n            'params': best_params,\n            'skipped': False\n        }\n        \n    except Exception as e:\n        if verbose:\n            print(f\"Error in LightGBM regressor: {e}\")\n        # Return dummy predictions on error\n        n_samples = X_test.shape[0]\n        dummy_preds = np.zeros(n_samples)\n        \n        return {\n            'model': None,\n            'predictions': dummy_preds,\n            'mae': float('inf'),\n            'r2_score': -float('inf'),\n            'params': params,\n            'skipped': True,\n            'error': str(e)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T14:20:34.930145Z","iopub.execute_input":"2025-08-06T14:20:34.930404Z","iopub.status.idle":"2025-08-06T14:20:34.953675Z","shell.execute_reply.started":"2025-08-06T14:20:34.930386Z","shell.execute_reply":"2025-08-06T14:20:34.953104Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Neural Networks","metadata":{}},{"cell_type":"code","source":"def default_classification_params():\n    return {\n        \"input_dim\": 200,\n        \"hidden_dim\": [128, 64],\n        \"output_dim\": 7,\n        \"use_batch_norm\": True,\n        \"initial_dropout\": 0.3,\n        \"final_dropout\": 0.8,\n        \"lr\": 1e-3,\n        \"weight_decay\": 1e-5,\n        \"batch_size\": 128,\n        \"epochs\": 400,\n        \"early_stopping_steps\": 20,\n        \"gradient_clip\": 1.0,\n        \"val_split\": 0.2,\n        \"test_split\": 0.2,\n        \"random_state\": 42,\n    }\n\n\nclass NNClassificationTuner:\n    def __init__(self, X_train, y_train, X_val=None, y_val=None, params=None, device=\"cpu\", n_trials=20, timeout=1200):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_val = X_val\n        self.y_val = y_val\n        self.params = params\n        self.device = device\n        self.n_trials = n_trials\n        self.timeout = timeout\n        self.best_params = None\n        self.best_score = None\n\n    def objective(self, trial):\n        params = self.params.copy()\n        params.update({\n            \"hidden_dim\": trial.suggest_categorical(\n                \"hidden_dim\",\n                [\n                    [64],\n                    [128],\n                    [128, 64],\n                    [256, 128, 64],\n                    [256, 128],\n                    [512, 256, 128, 64]\n                ]\n            ),\n            \"initial_dropout\": trial.suggest_float(\"initial_dropout\", 0.1, 0.3),\n            \"final_dropout\": trial.suggest_float(\"final_dropout\", 0.5, 0.8),\n            \"lr\": trial.suggest_loguniform(\"lr\", 1e-4, 1e-2),\n            \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128, 256]),\n            \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3),\n            \"gradient_clip\": trial.suggest_float(\"gradient_clip\", 0.5, 2.0),\n        })\n\n        # Train model\n        model = NNClassifier(params, device=self.device)\n        model.fit(self.X_train, self.y_train, X_val=self.X_val, y_val=self.y_val)\n        val_metrics = model.evaluate(self.X_val, self.y_val)\n        val_acc = val_metrics['class_accuracy']\n        return val_acc\n\n    def tune(self):\n        study = optuna.create_study(direction='maximize')\n        study.optimize(self.objective, n_trials=self.n_trials, timeout=self.timeout)\n        \n        self.best_params = study.best_params\n        self.best_score = study.best_value\n        \n        print(f\"Best score: {self.best_score:.4f}\")\n        print(f\"Best parameters: {self.best_params}\")\n        \n        return self.best_params, self.best_score\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Dataset class for classification\nclass ClassificationTrainDataset(Dataset):\n    def __init__(self, features, n_targets):\n        self.features = features\n        self.n_targets = n_targets\n\n    def __len__(self):\n        return self.features.shape[0]\n    \n    def __getitem__(self, idx):\n        return {\n            'x': torch.tensor(self.features[idx], dtype=torch.float),\n            'n_classes': torch.tensor(self.n_targets[idx], dtype=torch.long)\n        }\n   \n\n# Neural Network\nclass ClassificationNeuralNetwork(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim = [128,64], use_batch_norm=True,\n                  initial_dropout:float = 0.2, final_dropout:float =0.7, random_state=42):\n        super(ClassificationNeuralNetwork,self).__init__()\n\n        \"\"\"\n        Initialize Continent architecture using Pytorch modules.\n\n        Parameters:\n        - input_size: Number of input features. In this case it is the GITs. # 200\n        - hidden_layers: List of hidden layers # 128, 64 are the default\n        - output_size: Number of classes\n        - dropout_rate: [0.2, 0.7]\n        - random_state: Random state for reporducibility\n        \n        \"\"\"\n        self.input_size = input_dim\n        self.hidden_dim = hidden_dim\n        self.output_size = output_dim\n        self.dropout_initial = initial_dropout\n        self.dropout_final = final_dropout\n        self.use_batch_norm = use_batch_norm\n\n        # Set random seeds\n        torch.manual_seed(random_state)\n        np.random.seed(random_state)\n       \n\n        # Build the neural network\n        self.layers = nn.ModuleList()\n        self.dropouts = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        # Create dynamic doprout rates\n        dropout_rates = np.linspace(initial_dropout,final_dropout, len(hidden_dim))\n\n        # Create the layer architecture\n        layer_sizes = [input_dim] + hidden_dim + [output_dim]\n\n        for i in range(len(layer_sizes)-1):\n            # Add the linear layers first\n            self.layers.append(nn.Linear(layer_sizes[i],layer_sizes[i+1]))\n\n            # Add batch normalization for hidden layers only and not for the output layers\n            if i < len(layer_sizes) - 2 and self.use_batch_norm:\n                self.batch_norms.append(nn.BatchNorm1d(layer_sizes[i+1]))\n\n            # Add dropout for hidden layers onyl and not for the output layers\n            if i < len(layer_sizes) - 2:\n                self.dropouts.append(nn.Dropout(dropout_rates[i]))\n\n    def forward(self,x):\n        \"\"\"\n        Forward propagations through the network\n        \n        Parameters:\n        - x: Input tensor        \n        \"\"\"\n\n\n        current_input = x\n\n        # Forward pass through the hidden layers\n        for i, (layer, dropout) in enumerate(zip(self.layers[:-1],self.dropouts)):\n            # Linear transformations\n            z = layer(current_input)\n\n            # Batch normalization if enabled\n            if self.use_batch_norm:\n                z = self.batch_norms[i](z)\n\n            # Acitvation function\n            a = F.relu(z)\n\n            # Apply dropout only during training\n            if i < len(self.dropouts):\n                a = dropout(a) if self.training else a # Apply dropout only during training\n            \n            current_input = a\n\n        # Output layer (no activation for regression)\n        output = self.layers[-1](current_input)\n\n        return output\n\n\nclass NNClassifier:\n    def __init__(self, params=None, device=\"cpu\"):\n        if params is None:\n            self.params = default_classification_params()\n        else:\n            self.params = params\n        self.device = device\n        self.model = None\n        self.class_weight_tensor = None\n        self.best_model_state = None\n    \n    def fit(self, X_train, y_train, X_val=None, y_val=None):\n        \"\"\" Train the model\"\"\"\n        print(\"Fit the model...\")\n\n        # Update the parameters to this input\n        self.params['input_dim'] = X_train.shape[1]\n        self.params['output_dim'] = len(np.unique(y_train))\n\n\n        # Compute the class weights\n        class_weights = compute_class_weight(class_weight=\"balanced\",classes=np.unique(y_train),y=y_train)\n        self.class_weight_tensor = torch.tensor(class_weights,dtype=torch.float32).to(self.device)\n\n        # Split if validation is not given\n        if X_val is None or y_val is None:\n            X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size=self.params['val_split'],\n                                                              random_state=self.params['random_state'], stratify=y_train)\n        \n        # Create datasets and dataloaders\n        train_dataset = ClassificationTrainDataset(X_train,y_train)\n        val_dataset = ClassificationTrainDataset(X_val,y_val)\n\n        train_loader = DataLoader(train_dataset,batch_size=self.params['batch_size'],shuffle=True)\n        val_loader = DataLoader(val_dataset,batch_size=self.params['batch_size'],shuffle=False)\n\n        print(f\"Train size {len(train_dataset)}, Val size {len(val_dataset)}\")\n        \n\n        # Initialize model\n        self.model = ClassificationNeuralNetwork(\n            input_dim=self.params['input_dim'],\n            output_dim=self.params['output_dim'],\n            hidden_dim=self.params['hidden_dim'],\n            use_batch_norm=self.params['use_batch_norm'],\n            initial_dropout=self.params['initial_dropout'],\n            final_dropout=self.params['final_dropout'],\n            random_state=self.params['random_state']\n        ).to(self.device)\n\n        # Loss function and evaluation for classification\n        criterion_classification = nn.CrossEntropyLoss(weight=self.class_weight_tensor)\n        optimizer = torch.optim.Adam(params=self.model.parameters(),\n                                     lr=self.params['lr'],\n                                     weight_decay=self.params['weight_decay'])\n        \n        best_val_loss = float('inf')\n        early_stopping_counter = 0\n\n        train_losses = []\n        val_losses = []\n\n        print(\"Strarting training.....\")\n        for epoch in range(self.params['epochs']):\n            # Training phase\n            self.model.train()\n            train_loss = 0.0\n            train_correct = 0\n            train_total = 0\n\n            for batch in train_loader:\n                features = batch['x'].to(self.device)\n                targets = batch['n_classes'].to(self.device)\n\n                # Set optimizer\n                optimizer.zero_grad()\n\n                # Forward pass\n                preds = self.model(features)\n\n                # Calculate loss\n                classification_loss = criterion_classification(preds,targets)\n\n                # Combined loss - adjust weight of the reconstruction loss\n                total_loss = classification_loss \n\n                # Backward pass\n                total_loss.backward()\n\n                # Gradient clipping\n                if self.params['gradient_clip'] > 0:\n                    clip_grad_norm_(self.model.parameters(), self.params['gradient_clip'])\n\n                optimizer.step()\n\n\n                train_loss += total_loss.item()\n\n                # Calcualte metrics for the epoch\n                _, predicted = torch.max(preds.data,1)\n                train_total += targets.size(0)\n                train_correct += (predicted == targets).sum().item()\n\n            # Validation phase\n            self.model.eval()\n            val_loss = 0.0\n            val_correct = 0\n            val_total = 0\n\n            with torch.no_grad():\n                for batch in val_loader:\n                    features = batch['x'].to(self.device)\n                    targets = batch['n_classes'].to(self.device)\n\n                    preds = self.model(features)\n\n                    classification_loss = criterion_classification(preds,targets)\n                    total_loss = classification_loss \n\n                    val_loss += total_loss.item()\n\n                    _, predicted = torch.max(preds.data,1)\n                    val_total += targets.size(0)\n                    val_correct += (predicted == targets).sum().item()\n\n            # Calculate averages\n            train_loss /= len(train_loader)\n            val_loss /= len(val_loader)\n            train_accuracy = 100 * train_correct / train_total\n            val_accuracy = 100 * val_correct / val_total\n\n            train_losses.append(train_loss)\n            val_losses.append(val_loss)\n\n\n            if epoch % 10 == 0:\n                print(f'Epoch [{epoch}/{self.params[\"epochs\"]}], '\n                      f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n                      f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n\n            # Early stopping\n                if val_loss < best_val_loss:\n                    best_val_loss = val_loss\n                    early_stopping_counter = 0\n                    # Save best model state\n                    self.best_model_state = copy.deepcopy(self.model.state_dict())\n                else:\n                    early_stopping_counter += 1\n                    if early_stopping_counter >= self.params['early_stopping_steps']:\n                        print(f\"Early stopping at epoch {epoch}\")\n                        break\n\n        print(f\"Training completed. Best validation loss: {best_val_loss:.4f}\")\n\n    def evaluate(self, X, y):\n        \"\"\"\n        Evaluate the model\n        \"\"\"\n\n        self.model.eval()\n        all_preds = []\n        all_targets = []\n        class_lossses = []\n        all_preds_prob = []\n\n        dataset = ClassificationTrainDataset(X, y)\n        loader = DataLoader(dataset, batch_size=self.params['batch_size'], shuffle=False)\n\n        criterion = nn.CrossEntropyLoss(weight=self.class_weight_tensor)\n\n        with torch.no_grad():\n            for batch in loader:\n                features = batch['x'].to(self.device)\n                targets = batch['n_classes'].to(self.device)\n\n                preds = self.model(features)\n                loss = criterion(preds,targets)\n                class_lossses.append(loss.item())\n\n                probs = F.softmax(preds, dim=1).cpu().numpy()\n                _, predicted = torch.max(preds,1)\n                all_preds.extend(predicted.cpu().numpy())\n                all_targets.extend(targets.cpu().numpy())\n                all_preds_prob.extend(probs)\n\n        acc = accuracy_score(all_targets, all_preds)\n\n        return {\n                'class_loss': np.mean(class_lossses),\n                'class_accuracy': acc,\n                'probabilities':np.array(all_preds_prob),\n                'predictions': all_preds,\n                'targets': all_targets\n            }\n    \n    def predict(self, X):\n        \"\"\"\n        Make predictions on new data\n        \"\"\"\n        \n        self.model.eval()\n        all_preds = []\n        all_preds_prob = []\n        \n        # Create dummy targets for dataset\n        dummy_targets = np.zeros(X.shape[0])\n        dataset = ClassificationTrainDataset(X, dummy_targets)\n        loader = DataLoader(dataset, batch_size=self.params['batch_size'], shuffle=False)\n        \n        with torch.no_grad():\n            for batch in loader:\n                features = batch['x'].to(self.device)\n                outputs = self.model(features)\n                \n                probs = F.softmax(outputs, dim=1).cpu().numpy()\n                preds = np.argmax(probs, axis=1)\n                \n                all_preds.extend(preds)\n                all_preds_prob.extend(probs)\n        \n        # Convert back to original labels\n        \n        return {\n            'predictions': all_preds,\n            'probabilities': np.array(all_preds_prob)\n        }\n    \n\ndef run_nn_classifier(X_train, y_train, X_test, y_test,\n                      tune_hyperparams=False, params=None,\n                      n_trials=20, timeout=1200, verbose=False):\n    \"\"\"Run the neural network classifier\"\"\"\n    \n    # Handle device detection internally\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    if verbose:\n        print(f\"Running neural network classifier on device: {device}\")\n    \n    # Use default if params not given\n    if params is None:\n        params = default_classification_params()\n    else:\n        default = default_classification_params()\n        default.update(params)\n        params = default\n\n    # Update input dimension based on actual data\n    params['input_dim'] = X_train.shape[1]\n    params['output_dim'] = len(np.unique(y_train))\n\n        \n    if tune_hyperparams:\n        # Split validation set from training data\n        X_train_split, X_val, y_train_split, y_val = train_test_split(\n            X_train,y_train, test_size=0.2, random_state=42, stratify=y_train\n        )\n\n        tuner = NNClassificationTuner(X_train_split, y_train_split, X_val, y_val, \n                               params, device=device, n_trials=n_trials, timeout=timeout)\n        best_params, best_score = tuner.tune()\n        params.update(best_params)\n        if verbose:\n            print(\"Using best params:\", params)\n\n    # Train final model on full training data\n    model = NNClassifier(params, device=device)\n    model.fit(X_train, y_train)\n\n    results = model.evaluate(X_test, y_test)\n    if verbose:\n        print(\"\\nClassification Report:\")\n        print(classification_report(results['targets'], results['predictions']))\n        print(\"\\nAccuracy:\", results['class_accuracy'])\n    \n    return {\n        'model': model,\n        'predictions': results['predictions'],\n        'predicted_probabilities': results['probabilities'],\n        'accuracy': results['class_accuracy'],\n        'params': params\n    }\n\n\n\ndef default_regression_params():\n    return {\n        \"input_dim\": 200,\n        \"hidden_dim\": [128, 64],\n        \"output_dim\": 3,\n        \"use_batch_norm\": True,\n        \"initial_dropout\": 0.2,\n        \"final_dropout\": 0.5,\n        \"lr\": 1e-3,\n        \"weight_decay\": 1e-5,\n        \"batch_size\": 128,\n        \"epochs\": 400,\n        \"early_stopping_steps\": 50,\n        \"gradient_clip\": 1.0,\n        \"val_split\": 0.2,\n        \"random_state\": 42,\n    }\n\n\nclass NNRegressionTuner:\n    def __init__(self, X_train, y_train, X_val=None, y_val=None, params=None, device=\"cpu\", n_trials=20, timeout=1200):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_val = X_val\n        self.y_val = y_val\n        self.params = params\n        self.device = device\n        self.n_trials = n_trials\n        self.timeout = timeout\n        self.best_params = None\n        self.best_score = None\n\n    def objective(self, trial):\n        params = self.params.copy()\n        params.update({\n            \"hidden_dim\": trial.suggest_categorical(\n                \"hidden_dim\",\n                [\n                    [64],\n                    [128],\n                    [128, 64],\n                    [256, 128, 64],\n                    [256, 128],\n                    [512, 256, 128, 64]\n                ]\n            ),\n            \"initial_dropout\": trial.suggest_float(\"initial_dropout\", 0.1, 0.3),\n            \"final_dropout\": trial.suggest_float(\"final_dropout\", 0.5, 0.8),\n            \"lr\": trial.suggest_loguniform(\"lr\", 1e-4, 1e-2),\n            \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128, 256]),\n            \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3),\n            \"gradient_clip\": trial.suggest_float(\"gradient_clip\", 0.5, 2.0),\n        })\n\n        # Train model\n        model = NNRegressor(params, device=self.device)\n        model.fit(self.X_train, self.y_train, X_val=self.X_val, y_val=self.y_val)\n        val_metrics = model.evaluate(self.X_val, self.y_val)\n        # Use negative MSE for maximization (Optuna maximizes by default)\n        val_mse = val_metrics['mse']\n        return -val_mse\n\n    def tune(self):\n        study = optuna.create_study(direction='maximize')\n        study.optimize(self.objective, n_trials=self.n_trials, timeout=self.timeout)\n        \n        self.best_params = study.best_params\n        self.best_score = study.best_value\n        \n        print(f\"Best score (negative MSE): {self.best_score:.4f}\")\n        print(f\"Best parameters: {self.best_params}\")\n        \n        return self.best_params, self.best_score\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nclass RegressionTrainDataset(Dataset):\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        return {\n            'x': torch.tensor(self.features[idx], dtype=torch.float),\n            'y': torch.tensor(self.targets[idx], dtype=torch.float)\n        }\n\n   \n\n# Neural Network for Regression\nclass RegressionNeuralNetwork(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim=[128, 64],\n                 use_batch_norm=True, initial_dropout=0.2, final_dropout=0.5, random_state=42):\n        super().__init__()\n        self.input_size = input_dim\n        self.output_size = output_dim\n        self.hidden_dim = hidden_dim\n        self.use_batch_norm = use_batch_norm\n\n        torch.manual_seed(random_state)\n\n        self.layers = nn.ModuleList()\n        self.dropouts = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        layer_sizes = [input_dim] + hidden_dim + [output_dim]\n        dropout_rates = np.linspace(initial_dropout, final_dropout, len(hidden_dim))\n\n        for i in range(len(layer_sizes) - 1):\n            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n            if i < len(layer_sizes) - 2 and use_batch_norm:\n                self.batch_norms.append(nn.BatchNorm1d(layer_sizes[i+1]))\n            if i < len(layer_sizes) - 2:\n                self.dropouts.append(nn.Dropout(dropout_rates[i]))\n\n    def forward(self, x):\n        for i in range(len(self.layers) - 1):\n            x = self.layers[i](x)\n            if self.use_batch_norm:\n                x = self.batch_norms[i](x)\n            x = F.relu(x)\n            x = self.dropouts[i](x)\n        return self.layers[-1](x)  # Output layer: no activation\n\n\n\nclass NNRegressor:\n    def __init__(self, params=None, device=\"cpu\"):\n        if params is None:\n            self.params = default_regression_params()\n        else:\n            self.params = params\n        self.device = device\n        self.model = None\n        self.best_model_state = None\n        self.target_scaler = None\n    \n    def fit(self, X_train, y_train, X_val=None, y_val=None):\n        \"\"\"Train the model\"\"\"\n        print(\"Fitting the model...\")\n\n        # Update the parameters to this input\n        self.params['input_dim'] = X_train.shape[1]\n        self.params['output_dim'] = y_train.shape[1]  \n\n        \n        # Split if validation is not given\n        if X_val is None or y_val is None:\n            X_train, X_val, y_train, y_val = train_test_split(\n                X_train, y_train, test_size=self.params['val_split'],\n                random_state=self.params['random_state']\n            )\n        \n        # Create datasets and dataloaders\n        train_dataset = RegressionTrainDataset(X_train, y_train)\n        val_dataset = RegressionTrainDataset(X_val, y_val)\n\n        train_loader = DataLoader(train_dataset, batch_size=self.params['batch_size'], shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=self.params['batch_size'], shuffle=False)\n\n        print(f\"Train size {len(train_dataset)}, Val size {len(val_dataset)}\")\n\n        # Initialize model\n        self.model = RegressionNeuralNetwork(\n            input_dim=self.params['input_dim'],\n            output_dim=self.params['output_dim'],\n            hidden_dim=self.params['hidden_dim'],\n            use_batch_norm=self.params['use_batch_norm'],\n            initial_dropout=self.params['initial_dropout'],\n            final_dropout=self.params['final_dropout'],\n            random_state=self.params['random_state']\n        ).to(self.device)\n\n        # Loss function and optimizer for regression\n        criterion = nn.MSELoss()\n        optimizer = torch.optim.Adam(\n            params=self.model.parameters(),\n            lr=self.params['lr'],\n            weight_decay=self.params['weight_decay']\n        )\n        \n        best_val_loss = float('inf')\n        early_stopping_counter = 0\n\n        train_losses = []\n        val_losses = []\n\n        print(\"Starting training...\")\n        for epoch in range(self.params['epochs']):\n            # Training phase\n            self.model.train()\n            train_loss = 0.0\n\n            for batch in train_loader:\n                features = batch['x'].to(self.device)\n                targets = batch['y'].to(self.device)\n\n                # Zero optimizer\n                optimizer.zero_grad()\n\n                # Forward pass\n                preds = self.model(features)\n\n                # Calculate loss\n                loss = criterion(preds, targets)\n\n                # Backward pass\n                loss.backward()\n\n                # Gradient clipping\n                if self.params['gradient_clip'] > 0:\n                    clip_grad_norm_(self.model.parameters(), self.params['gradient_clip'])\n\n                optimizer.step()\n\n                train_loss += loss.item()\n\n            # Validation phase\n            self.model.eval()\n            val_loss = 0.0\n\n            with torch.no_grad():\n                for batch in val_loader:\n                    features = batch['x'].to(self.device)\n                    targets = batch['y'].to(self.device)\n\n                    preds = self.model(features)\n                    loss = criterion(preds, targets)\n                    val_loss += loss.item()\n\n            # Calculate averages\n            train_loss /= len(train_loader)\n            val_loss /= len(val_loader)\n\n            train_losses.append(train_loss)\n            val_losses.append(val_loss)\n\n            if epoch % 10 == 0:\n                print(f'Epoch [{epoch}/{self.params[\"epochs\"]}], '\n                      f'Train Loss: {train_loss:.4f}, '\n                      f'Val Loss: {val_loss:.4f}')\n\n            # Early stopping\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                early_stopping_counter = 0\n                # Save best model state\n                self.best_model_state = copy.deepcopy(self.model.state_dict())\n            else:\n                early_stopping_counter += 1\n                if early_stopping_counter >= self.params['early_stopping_steps']:\n                    print(f\"Early stopping at epoch {epoch}\")\n                    break\n\n        print(f\"Training completed. Best validation loss: {best_val_loss:.4f}\")\n\n        # Load best model\n        if self.best_model_state is not None:\n            self.model.load_state_dict(self.best_model_state)\n\n    def evaluate(self, X, y):\n        \"\"\"Evaluate the model\"\"\"\n        self.model.eval()\n        all_preds = []\n        all_targets = []\n        losses = []\n\n        dataset = RegressionTrainDataset(X, y)\n        loader = DataLoader(dataset, batch_size=self.params['batch_size'], shuffle=False)\n        criterion = nn.MSELoss()\n\n        with torch.no_grad():\n            for batch in loader:\n                features = batch['x'].to(self.device)\n                targets = batch['y'].to(self.device)\n\n                preds = self.model(features)\n                loss = criterion(preds, targets)\n                losses.append(loss.item())\n\n                all_preds.extend(preds.cpu().numpy())\n                all_targets.extend(targets.cpu().numpy())\n\n        # Convert back to numpy arrays\n        all_preds = np.array(all_preds)\n        all_targets = np.array(all_targets)\n\n        # Calculate metrics\n        mse = mean_squared_error(all_targets, all_preds)\n        mae = mean_absolute_error(all_targets, all_preds)\n        r2 = r2_score(all_targets, all_preds)\n\n        return {\n            'mse': mse,\n            'mae': mae,\n            'r2': r2,\n            'rmse': np.sqrt(mse),\n            'predictions': all_preds,\n            'targets': all_targets\n        }\n    \n    def predict(self, X):\n        \"\"\"Make predictions on new data\"\"\"\n        self.model.eval()\n        all_preds = []\n        \n        # Create dummy targets for dataset\n        dummy_targets = np.zeros(X.shape[0])\n        dataset = RegressionTrainDataset(X, dummy_targets)\n        loader = DataLoader(dataset, batch_size=self.params['batch_size'], shuffle=False)\n        \n        with torch.no_grad():\n            for batch in loader:\n                features = batch['x'].to(self.device)\n                outputs = self.model(features).squeeze()\n                all_preds.extend(outputs.cpu().numpy())\n        \n        # Convert to numpy array and reshape\n        all_preds = np.array(all_preds)\n        \n        return all_preds\n\n\ndef run_nn_regressor(X_train, y_train, X_test, y_test, device=None,\n                     tune_hyperparams=False, params=None,\n                     n_trials=20, timeout=1200, verbose=True):\n    \"\"\"Run the neural network regressor\"\"\"\n    \n    try:\n        # Set device if not provided\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        \n        if verbose:\n            print(f\"Running neural network regressor on device: {device}\")\n        \n        # Use default if params not given\n        if params is None:\n            params = default_regression_params()\n        else:\n            default = default_regression_params()\n            default.update(params)\n            params = default\n\n        # Update input dimension based on actual data\n        params['input_dim'] = X_train.shape[1]\n        \n        # Handle both 1D and multi-dimensional targets\n        if len(y_train.shape) == 1:\n            y_train = y_train.reshape(-1, 1)\n            y_test = y_test.reshape(-1, 1)\n        \n        params['output_dim'] = y_train.shape[1]\n\n        if tune_hyperparams:\n            # Split validation set from training data\n            X_train_split, X_val, y_train_split, y_val = train_test_split(\n                X_train, y_train, test_size=0.2, random_state=42\n            )\n\n            tuner = NNRegressionTuner(X_train_split, y_train_split, X_val, y_val, \n                           params, device=device, n_trials=n_trials, timeout=timeout)\n            best_params, best_score = tuner.tune()\n            params.update(best_params)\n            if verbose:\n                print(\"Using best params:\", params)\n\n        # Train final model on full training data\n        model = NNRegressor(params, device=device)\n        model.fit(X_train, y_train)\n\n        # Evaluate on test set\n        results = model.evaluate(X_test, y_test)\n        \n        if verbose:\n            print(f\"\\nRegression Results:\")\n            print(f\"MSE: {results['mse']:.4f}\")\n            print(f\"MAE: {results['mae']:.4f}\")\n            print(f\"RMSE: {results['rmse']:.4f}\")\n            print(f\"R2: {results['r2']:.4f}\")\n        \n        return {\n            'model': model,\n            'predictions': results['predictions'],\n            'mse': results['mse'],\n            'mae': results['mae'],\n            'rmse': results['rmse'],\n            'r2_score': results['r2'],  # Add r2_score key for consistency\n            'params': params,\n            'skipped': False\n        }\n        \n    except Exception as e:\n        if verbose:\n            print(f\"Error in neural network regressor: {e}\")\n        # Return dummy predictions on error\n        n_samples = X_test.shape[0]\n        output_dim = y_train.shape[1] if len(y_train.shape) > 1 else 1\n        dummy_preds = np.zeros((n_samples, output_dim))\n        \n        return {\n            'model': None,\n            'predictions': dummy_preds,\n            'mse': float('inf'),\n            'mae': float('inf'),\n            'rmse': float('inf'),\n            'r2_score': -float('inf'),\n            'params': params,\n            'skipped': True,\n            'error': str(e)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T14:20:34.955110Z","iopub.execute_input":"2025-08-06T14:20:34.955375Z","iopub.status.idle":"2025-08-06T14:20:35.019145Z","shell.execute_reply.started":"2025-08-06T14:20:34.955353Z","shell.execute_reply":"2025-08-06T14:20:35.018363Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nUsing device: cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# TabPFN","metadata":{}},{"cell_type":"code","source":"def run_tabpfn_classifier(X_train, y_train, X_test, y_test, tune_hyperparams=False, max_time=60, params=None, random_state=42):\n    \"\"\"\n    Run TabPFN classifier with device and class count checks.\n    Uses AutoTabPFNClassifier for hyperparameter tuning.\n    \"\"\"\n    os.environ[\"TABPFN_ALLOW_CPU_LARGE_DATASET\"] = \"1\"\n    device = 'cpu'\n    if params and 'device' in params:\n        device = params['device']\n    elif torch.cuda.is_available():\n        device = 'cuda'\n\n    n_classes = len(np.unique(y_train))\n\n    # Skip if device is CPU\n    if device == 'cpu':\n        print(\"TabPFNClassifier skipped: device is CPU.\")\n        return {\n            'model': None,\n            'predictions': None,\n            'predicted_probabilities': None,\n            'accuracy': None,\n            'params': params,\n            'skipped': True,\n            'reason': 'cpu'\n        }\n\n    # Skip if too many classes\n    if n_classes > 30:\n        print(f\"TabPFNClassifier skipped: number of classes ({n_classes}) exceeds TabPFN's limit.\")\n        return {\n            'model': None,\n            'predictions': None,\n            'predicted_probabilities': None,\n            'accuracy': None,\n            'params': params,\n            'skipped': True,\n            'reason': 'too_many_classes'\n        }\n\n    try:\n        # Extract max_time from params if provided\n        if params and 'max_time' in params:\n            max_time = params['max_time']\n        \n        # Hyperparameter tuning with AutoTabPFNClassifier\n        if tune_hyperparams:\n            print(f\"Using AutoTabPFN for hyperparameter tuning with max_time={max_time}...\")\n            model = AutoTabPFNClassifier(device=device, max_time=max_time)\n            model.fit(X_train, y_train)\n            preds = model.predict(X_test)\n            probs = model.predict_proba(X_test)\n            acc = accuracy_score(y_test, preds)\n            \n            print(f\"AutoTabPFN accuracy: {acc:.4f}\")\n            print(\"\\nAutoTabPFN Classification Report:\")\n            print(classification_report(y_test, preds))\n            \n            return {\n                'model': model,\n                'predictions': preds,\n                'predicted_probabilities': probs,\n                'accuracy': acc,\n                'params': {'max_time': max_time, 'device': device}\n            }\n        \n        # Regular TabPFN usage\n        model = TabPFNClassifier(device=device, ignore_pretraining_limits=True)\n        model.fit(X_train, y_train)\n        preds = model.predict(X_test)\n        probs = model.predict_proba(X_test)\n        acc = accuracy_score(y_test, preds)\n        \n        print(\"\\nTabPFN Classification Report:\")\n        print(classification_report(y_test, preds))\n        print(f\"Accuracy: {acc:.4f}\")\n\n        return {\n            'model': model,\n            'predictions': preds,\n            'predicted_probabilities': probs,\n            'accuracy': acc,\n            'params': params\n        }\n        \n    except Exception as e:\n        print(f\"Error running TabPFN: {e}\")\n        return {\n            'model': None,\n            'predictions': None,\n            'predicted_probabilities': None,\n            'accuracy': None,\n            'params': params,\n            'skipped': True,\n            'reason': f'error: {str(e)}'\n        }\n\ndef run_tabpfn_regressor(X_train, y_train, X_test, y_test, tune_hyperparams=False, max_time=60, params=None):\n    \"\"\"\n    Runs TabPFNRegressor models to predict x, y, z coordinates.\n    Uses AutoTabPFNRegressor for hyperparameter tuning.\n    Skips if device is CPU.\n    \"\"\"\n    os.environ[\"TABPFN_ALLOW_CPU_LARGE_DATASET\"] = \"1\"\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    if params and 'device' in params:\n        device = params['device']\n\n    if device == 'cpu':\n        print(\"TabPFNRegressor skipped: device is CPU.\")\n        # Ensure predictions and lat_lon_predictions are arrays of NaN with correct shapes\n        n_samples = X_test.shape[0] if hasattr(X_test, \"shape\") and len(X_test.shape) > 0 else 0\n        preds = np.full((n_samples, 3), np.nan)\n        lat_lon_preds = np.full((n_samples, 2), np.nan)\n        return {\n            'models': None,\n            'predictions': preds,\n            'lat_lon_predictions': lat_lon_preds,\n            'metrics': None,\n            'skipped': True,\n            'reason': 'cpu'\n        }\n\n    # Extract max_time from params if provided\n    if params and 'max_time' in params:\n        max_time = params['max_time']\n\n    coord_names = ['x', 'y', 'z']\n    models = {}\n    preds = []\n    metrics = {}\n\n    try:\n        for i, coord in enumerate(coord_names):\n            print(f\"\\n----- Predicting {coord.upper()} -----\")\n            \n            if tune_hyperparams:\n                print(f\"Using AutoTabPFN for hyperparameter tuning with max_time={max_time}...\")\n                model = AutoTabPFNRegressor(device=device, max_time=max_time)\n            else:\n                model = TabPFNRegressor(device=device, ignore_pretraining_limits=True)\n                \n            model.fit(X_train, y_train[:, i])\n            y_pred = model.predict(X_test)\n            preds.append(y_pred)\n\n            mse = mean_squared_error(y_test[:, i], y_pred)\n            mae = mean_absolute_error(y_test[:, i], y_pred)\n            r2 = r2_score(y_test[:, i], y_pred)\n\n            print(f\"{coord.upper()} - MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n            metrics[coord] = {'mse': mse, 'mae': mae, 'r2': r2}\n            models[coord] = model\n\n        preds = np.stack(preds, axis=1)  # Shape: [n_samples, 3]\n        lat_pred_rad = np.arcsin(preds[:, 2])\n        lon_pred_rad = np.arctan2(preds[:, 1], preds[:, 0])\n        lat_pred_deg = np.degrees(lat_pred_rad)\n        lon_pred_deg = np.degrees(lon_pred_rad)\n\n        return {\n            'models': models,\n            'predictions': preds,\n            'lat_lon_predictions': np.stack([lat_pred_deg, lon_pred_deg], axis=1),\n            'metrics': metrics\n        }\n        \n    except Exception as e:\n        print(f\"Error running TabPFNRegressor: {e}\")\n        n_samples = X_test.shape[0] if hasattr(X_test, \"shape\") and len(X_test.shape) > 0 else 0\n        preds = np.full((n_samples, 3), np.nan)\n        lat_lon_preds = np.full((n_samples, 2), np.nan)\n        return {\n            'models': None,\n            'predictions': preds,\n            'lat_lon_predictions': lat_lon_preds,\n            'metrics': None,\n            'skipped': True,\n            'reason': f'error: {str(e)}'\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T14:20:35.020054Z","iopub.execute_input":"2025-08-06T14:20:35.020311Z","iopub.status.idle":"2025-08-06T14:20:35.038285Z","shell.execute_reply.started":"2025-08-06T14:20:35.020287Z","shell.execute_reply":"2025-08-06T14:20:35.037745Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"class XGBoostTuner:\n    def __init__(self, X_train, y_train, X_test, y_test, random_state=42, n_trials=20, timeout=1200):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        self.random_state = random_state\n        self.n_trials = n_trials\n        self.timeout = timeout\n        self.best_params = None\n        self.final_model = None\n\n    def default_params(self):\n        return {\n            'objective': 'multi:softprob',\n            'num_class': len(np.unique(self.y_train)),\n            'eval_metric': 'mlogloss',\n            'tree_method': 'hist',\n            'learning_rate': 0.1,\n            'max_depth': 6,\n            'min_child_weight': 1,\n            'gamma': 0,\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'lambda': 1.0,\n            'alpha': 0.0,\n            'n_estimators': 300,\n        }\n\n    def objective(self, trial):\n        params = {\n            'objective': 'multi:softprob',\n            'num_class': len(np.unique(self.y_train)),\n            'eval_metric': 'mlogloss',\n            'tree_method': 'hist',\n            'learning_rate': trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n            'max_depth': trial.suggest_int(\"max_depth\", 3, 12),\n            'min_child_weight': trial.suggest_int(\"min_child_weight\", 1, 10),\n            'gamma': trial.suggest_float(\"gamma\", 0, 5),\n            'subsample': trial.suggest_float(\"subsample\", 0.5, 1.0),\n            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n            'lambda': trial.suggest_float(\"lambda\", 1e-3, 10.0, log=True),\n            'alpha': trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True),\n            'n_estimators': trial.suggest_int(\"n_estimators\", 100, 400),\n        }\n\n        model = xgb.XGBClassifier(**params, random_state=self.random_state, verbosity=0, use_label_encoder=False)\n        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n        scores = cross_val_score(model, self.X_train, self.y_train, cv=skf, scoring='accuracy')\n        return scores.mean()\n\n    def tune(self):\n        study = optuna.create_study(direction='maximize',pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)) # Pruning helps the in stopping bad trials\n        study.optimize(self.objective, n_trials=self.n_trials, timeout=self.timeout)\n        self.best_params = study.best_params\n        self.best_params.update({\n            'objective': 'multi:softprob',\n            'num_class': len(np.unique(self.y_train)),\n            'eval_metric': 'mlogloss',\n            'tree_method': 'hist'\n        })\n        return self.best_params\n\n    def train(self, params):\n        model = xgb.XGBClassifier(**params, use_label_encoder=False)\n        model.fit(self.X_train, self.y_train)\n        self.final_model = model\n        return model\n\n    # Evaluate on the test dataset on how the model \n    def evaluate(self, model=None):\n        if model is None:\n            model = self.final_model\n        preds = model.predict(self.X_test)\n        probs = model.predict_proba(self.X_test)\n        acc = accuracy_score(self.y_test, preds)\n        print(\"\\nClassification Report:\")\n        print(classification_report(self.y_test, preds))\n        print(f\"\\nAccuracy: {acc:.4f}\")\n        return preds, probs, acc\n\n\n\ndef run_xgboost_classifier(X_train, y_train, X_test, y_test, \n                           tune_hyperparams=False, random_state=42, \n                           n_trials=20, timeout=1200, params=None, verbose=False):\n    \n    tuner = XGBoostTuner(X_train, y_train, X_test, y_test, \n                         random_state=random_state, n_trials=n_trials, timeout=timeout)\n\n    if tune_hyperparams:\n        best_params = tuner.tune()\n        if verbose:\n            print(\"Using tuned parameters:\", best_params)\n    else:\n        best_params = tuner.default_params()\n        if params:\n            best_params.update(params)\n        if verbose:\n            print(\"Using default (or custom) parameters:\", best_params)\n\n    model = tuner.train(best_params)\n    preds, probs, acc = tuner.evaluate(model) if verbose else (model.predict(X_test), model.predict_proba(X_test), accuracy_score(y_test, model.predict(X_test)))\n    \n    if verbose:\n        return {\n            'model': model,\n            'predictions': preds,\n            'predicted_probabilities': probs,\n            'accuracy': acc,\n            'params': best_params\n        }\n    else:\n        return {\n            'model': model,\n            'predictions': preds,\n            'predicted_probabilities': probs,\n            'accuracy': acc,\n            'params': best_params\n        }\n\n\n\n\n\nclass XGBoostRegressorTuner:\n    def __init__(self, X_train, y_train, X_test, y_test,\n                 random_state=42, n_trials=20, timeout=1200):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        self.random_state = random_state\n        self.n_trials = n_trials\n        self.timeout = timeout\n        self.best_params = None\n        self.final_model = None\n\n    def default_params(self):\n        return {\n            'objective': 'reg:squarederror',\n            'eval_metric': 'rmse',\n            'tree_method': 'hist',\n            'learning_rate': 0.1,\n            'max_depth': 6,\n            'min_child_weight': 1,\n            'gamma': 0,\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'lambda': 1.0,\n            'alpha': 0.0,\n            'n_estimators': 300,\n        }\n\n    def objective(self, trial):\n        params = {\n            'objective': 'reg:squarederror',\n            'eval_metric': 'rmse',\n            'tree_method': 'hist',\n            'learning_rate': trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n            'max_depth': trial.suggest_int(\"max_depth\", 3, 12),\n            'min_child_weight': trial.suggest_int(\"min_child_weight\", 1, 10),\n            'gamma': trial.suggest_float(\"gamma\", 0, 5),\n            'subsample': trial.suggest_float(\"subsample\", 0.5, 1.0),\n            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n            'lambda': trial.suggest_float(\"lambda\", 1e-3, 10.0, log=True),\n            'alpha': trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True),\n            'n_estimators': trial.suggest_int(\"n_estimators\", 100, 400),\n        }\n\n        model = xgb.XGBRegressor(**params, random_state=self.random_state, verbosity=0)\n        kf = KFold(n_splits=5, shuffle=True, random_state=self.random_state)\n        scores = cross_val_score(model, self.X_train, self.y_train, cv=kf, scoring='neg_mean_absolute_error')\n        return np.mean(scores)\n\n    def tune(self):\n        study = optuna.create_study(direction='maximize',\n                                    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2))\n        study.optimize(self.objective, n_trials=self.n_trials, timeout=self.timeout)\n        self.best_params = study.best_params\n        self.best_params.update({\n            'objective': 'reg:squarederror',\n            'eval_metric': 'rmse',\n            'tree_method': 'hist'\n        })\n        return self.best_params\n\n    def train(self, params):\n        model = xgb.XGBRegressor(**params)\n        model.fit(self.X_train, self.y_train)\n        self.final_model = model\n        return model\n\n    def evaluate(self, model=None):\n        if model is None:\n            model = self.final_model\n        preds = model.predict(self.X_test)\n        mae = mean_absolute_error(self.y_test, preds)\n        r2 = r2_score(self.y_test, preds)\n        print(\"\\nRegression Report:\")\n        print(f\"MAE:  {mae:.4f}\")\n        print(f\"R2:   {r2:.4f}\")\n        return preds, mae, r2\n\n\ndef run_xgboost_regressor(X_train, y_train, X_test, y_test,\n                          tune_hyperparams=False, random_state=42,\n                          n_trials=20, timeout=1200, params=None, verbose=True):\n    \"\"\"XGBoost regressor with proper error handling\"\"\"\n    \n    try:\n        # Handle multi-dimensional targets  \n        if len(y_train.shape) > 1 and y_train.shape[1] > 1:\n            if verbose:\n                print(\"Warning: XGBoost doesn't support multi-output regression natively. Using first dimension only.\")\n            y_train = y_train[:, 0]\n            y_test = y_test[:, 0]\n\n        tuner = XGBoostRegressorTuner(X_train, y_train, X_test, y_test,\n                                      random_state=random_state, n_trials=n_trials, timeout=timeout)\n\n        if tune_hyperparams:\n            best_params = tuner.tune()\n            if verbose:\n                print(\"Using tuned parameters:\", best_params)\n        else:\n            best_params = tuner.default_params()\n            if params:\n                best_params.update(params)\n            if verbose:\n                print(\"Using default (or custom) parameters:\", best_params)\n\n        model = tuner.train(best_params)\n        preds, mae, r2 = tuner.evaluate(model) if verbose else (model.predict(X_test), None, None)\n\n        # Calculate additional metrics if not verbose\n        if not verbose:\n            mae = mean_absolute_error(y_test, preds)\n            r2 = r2_score(y_test, preds)\n\n        return {\n            'model': model,\n            'predictions': preds,\n            'mae': mae,\n            'r2_score': r2,  # Use r2_score for consistency\n            'params': best_params,\n            'skipped': False\n        }\n        \n    except Exception as e:\n        if verbose:\n            print(f\"Error in XGBoost regressor: {e}\")\n        # Return dummy predictions on error\n        n_samples = X_test.shape[0]\n        dummy_preds = np.zeros(n_samples)\n        \n        return {\n            'model': None,\n            'predictions': dummy_preds,\n            'mae': float('inf'),\n            'r2_score': -float('inf'),\n            'params': params,\n            'skipped': True,\n            'error': str(e)\n        }\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T14:20:35.040124Z","iopub.execute_input":"2025-08-06T14:20:35.040318Z","iopub.status.idle":"2025-08-06T14:20:35.064645Z","shell.execute_reply.started":"2025-08-06T14:20:35.040302Z","shell.execute_reply":"2025-08-06T14:20:35.063909Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"# The main ensemble model\n\n\n\n# Load and process the dataset\n\n# Data processing function for hierarchical model\ndef process_data_hierarchical(df):\n    \"\"\"Process data for hierarchical prediction\"\"\"\n    # Process continuous features\n    cont_cols = [col for col in df.columns if col not in [\n        'latitude', 'longitude',\n        'latitude_rad', 'longitude_rad', 'x', 'y', 'z',\n        'scaled_x', 'scaled_y', 'scaled_z', 'continent', 'city'\n    ]]\n    \n    # Get the features\n    x_cont = df[cont_cols].values\n    \n    # Encode continent labels\n    continent_encoder = LabelEncoder()\n    y_continent = continent_encoder.fit_transform(df['continent'].values)\n    \n    # Encode city labels\n    city_encoder = LabelEncoder()\n    y_city = city_encoder.fit_transform(df['city'].values)\n    \n    # Calculate coordinates if not already present\n    if not all(col in df.columns for col in ['x', 'y', 'z']):\n        df['latitude_rad'] = np.deg2rad(df['latitude'])\n        df['longitude_rad'] = np.deg2rad(df['longitude'])\n        df['x'] = np.cos(df['latitude_rad']) * np.cos(df['longitude_rad'])\n        df['y'] = np.cos(df['latitude_rad']) * np.sin(df['longitude_rad'])\n        df['z'] = np.sin(df['latitude_rad'])\n    \n    # Scale coordinates\n    coord_scaler = StandardScaler()\n    y_coords = coord_scaler.fit_transform(df[['x', 'y', 'z']].values)\n    \n    continents = continent_encoder.classes_\n    cities = city_encoder.classes_\n    \n    print(f\"Continents: {len(continents)} ({continents})\")\n    print(f\"Cities: {len(cities)}\")\n    print(f\"Continuous features: {len(cont_cols)}\")\n    \n    return {\n        'x_cont': x_cont,\n        'y_continent': y_continent,\n        'y_city': y_city,\n        'y_coords': y_coords, # This is for neural networks. Scaling is required\n        'y_latitude': df['latitude'].values, # This is for XGBoost, we don't need to scale this\n        'y_longitude':df['longitude'].values, # This is for XGBoost, we don't need to scale this\n        'encoders': {\n            'continent': continent_encoder,\n            'city': city_encoder,\n            'coord': coord_scaler\n        },\n        'continents': continents,\n        'cities': cities\n    }\n\n# Hierarchial split to keep track of the indices\ndef hierarchical_split(X_cont, y_continent, y_city, y_coords, y_lat, y_lon, test_size=0.2, random_state=42):\n    \n    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    train_idx, test_idx = next(sss.split(X_cont, y_continent))\n\n    return {\n        'X_train': X_cont[train_idx],\n        'X_test': X_cont[test_idx],\n        'y_cont_train': y_continent[train_idx],\n        'y_cont_test': y_continent[test_idx],\n        'y_city_train': y_city[train_idx],\n        'y_city_test': y_city[test_idx],\n        'y_coords_train': y_coords[train_idx],\n        'y_coords_test': y_coords[test_idx],\n        'y_lat_train': y_lat[train_idx],\n        'y_lat_test': y_lat[test_idx],\n        'y_lon_train': y_lon[train_idx],\n        'y_lon_test': y_lon[test_idx],\n        'train_idx': train_idx,\n        'test_idx': test_idx\n    }\n\n# Distance between two points on the earth\ndef haversine_distance(lat1,lon1,lat2,lon2):\n    \"\"\"\n    Calculate the great circle distance between two points on the earth\n    \"\"\"\n    # Radius of the earth\n    R = 6371.0\n\n    # Convert from degrees to radians\n    lat1_rad = np.radians(lat1)\n    lon1_rad = np.radians(lon1)\n    lat2_rad = np.radians(lat2)\n    lon2_rad = np.radians(lon2)\n\n    dlat = lat2_rad - lat1_rad\n    dlon = lon2_rad - lon1_rad\n\n    a = np.sin(dlat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2) **2\n    c = 2 * np.arcsin(np.sqrt(a))\n\n    return R * c # in kilometers\n\n# Converting cartesian co-ordinates values to latitude and longitude\ndef xyz_to_latlon(xyz_coords):\n    \"\"\"\n    Convert the XYZ coordinates to latitude and longitude\n    \"\"\"\n    x,y,z = xyz_coords[:,0],xyz_coords[:,1],xyz_coords[:,2]\n\n    # Convert to latitude and longitude\n    lat_rad = np.arcsin(np.clip(z,-1,1)) # Clip to avoid numerical issues\n    lon_rad = np.arctan2(y,x)\n\n    # Convert to degrees\n    lat_deg = np.degrees(lat_rad)\n    lon_deg = np.degrees(lon_rad)\n\n    return np.stack([lat_deg,lon_deg],axis=1)\n\n# Plot the points on the world map for visualization\ndef plot_points_on_world_map(true_lat, true_long, predicted_lat, predicted_long, filename):\n    \"\"\"\n    Plots true and predicted latitude and longitude on a world map.\n    Args:\n        true_lat: True latitude value\n        true_long: True longitude value\n        predicted_lat: Prediction by the neural netwrok\n        predicted_long: Prediction by the neural network\n        filename: Path and the name of the file to save the plot.\n    Returns:\n        A figure is saved in the correct directory.\n    \"\"\"\n    # A file that is required to load the world map with proper countries\n    world = gpd.read_file(\"/home/chandru/binp37/data/geopandas/ne_110m_admin_0_countries.shp\") \n    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n    world.plot(ax=ax, color='lightgray', edgecolor='black')\n    # Plot true locations\n    geometry_true = [Point(xy) for xy in zip(true_long, true_lat)]\n    geo_df_true = gpd.GeoDataFrame(geometry_true, crs=world.crs, geometry=geometry_true) \n    geo_df_true.plot(ax=ax, marker='o', color='blue', markersize=15, label='True Locations')\n    # Plot predicted locations\n    geometry_predicted = [Point(xy) for xy in zip(predicted_long, predicted_lat)]\n    geo_df_predicted = gpd.GeoDataFrame(geometry_predicted, crs=world.crs, geometry=geometry_predicted) \n    geo_df_predicted.plot(ax=ax, marker='x', color='red', markersize=15, label='Predicted Locations')\n    ax.set_xlabel('Longitude')\n    ax.set_ylabel('Latitude')\n    ax.set_title('True vs. Predicted Locations on World Map')\n    ax.legend(loc='upper right')\n    plt.tight_layout()\n    plt.savefig(filename) # Save the plot as an image\n    plt.show()\n\n# Train the ensemble models on classification tasks -> Continent and city classification\ndef train_hierarchical_layer(\n        X_train,\n        X_test,\n        y_train,\n        y_test,\n        run_xgboost_classifier=None,\n        run_grownet_classifier=None,\n        run_nn_classifier=None,\n        run_tabpfn_classifier=None,\n        run_lightgbm_classifier=None,\n        run_catboost_classifier = None,\n        tune_hyperparams=False,\n        apply_smote = False,\n        random_state=42,\n        n_splits=3,\n        accuracy_threshold=0.8):\n    \"\"\"\n    Efficient single-stage hierarchical layer:\n    1. Run all models with default params in CV to filter AND generate meta-features\n    2. Tune hyperparameters only for filtered models\n    3. Train final ensemble\n    \"\"\"\n    \n    # Define all possible models with their configurations\n    model_configs = {\n        'xgb': {\n            'name': 'XGBoost',\n            'function': run_xgboost_classifier,\n            'enabled': run_xgboost_classifier is not None,\n            'tune_params': {'n_trials': 50, 'timeout': 1800}\n        },\n        'grownet': {\n            'name': 'GrowNet',\n            'function': run_grownet_classifier,\n            'enabled': run_grownet_classifier is not None,\n            'tune_params': {'n_trials': 50, 'timeout': 1800}\n        },\n        'nn': {\n            'name': 'Neural Network',\n            'function': run_nn_classifier,\n            'enabled': run_nn_classifier is not None,\n            'tune_params': {'n_trials': 50, 'timeout': 1800}\n        },\n        'tabpfn': {\n            'name': 'TabPFN',\n            'function': run_tabpfn_classifier,\n            'enabled': run_tabpfn_classifier is not None,\n            'tune_params': {'max_time_options': [30, 60, 120, 180]}\n        },\n        'lightgbm': {\n            'name': 'LightGBM',\n            'function':run_lightgbm_classifier,\n            'enabled':run_lightgbm_classifier is not None,\n            'tune_params': {'n_trials': 50, 'timeout': 1800}\n        },\n        'catboost': {\n            'name': 'CatBoost',\n            'function':run_catboost_classifier,\n            'enabled':run_catboost_classifier is not None,\n            'tune_params': {'n_trials': 50, 'timeout': 1800}\n        }\n    }\n    \n    # Filter to only enabled models\n    enabled_models = {k: v for k, v in model_configs.items() if v['enabled']}\n    \n    if not enabled_models:\n        raise ValueError(\"At least one model function must be provided (not None)\")\n    \n    print(f\"Enabled models: {list(enabled_models.keys())}\")\n    \n    # STAGE 1: Single CV loop to filter models AND generate meta-features\n    print(\"STAGE 1: Running cross-validation to filter models and generate meta-features...\")\n    \n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    n_train_samples = X_train.shape[0]\n    n_classes = len(np.unique(y_train))\n\n    # Track accuracies and out-of-fold predictions\n    model_fold_accuracies = {model_key: [] for model_key in enabled_models.keys()}\n    oof_probs = {model_key: np.zeros((n_train_samples, n_classes)) for model_key in enabled_models.keys()}\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n        print(f\"Processing Fold {fold+1}/{n_splits}\")\n        \n        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n        \n        if apply_smote:\n            X_fold_train, y_fold_train = SMOTE(random_state=42).fit_resample(X_fold_train, y_fold_train)\n        \n        for model_key, config in enabled_models.items():\n            print(f\"  Running {config['name']} on fold {fold+1}...\")\n            try:\n                fold_result = config['function'](\n                    X_fold_train, y_fold_train, X_fold_val, y_fold_val,\n                    tune_hyperparams=False, params=None, verbose=True\n                )\n                \n                if fold_result.get('skipped', False):\n                    print(f\"  {config['name']} was skipped on fold {fold+1}\")\n                    model_fold_accuracies[model_key].append(0.0)\n                    oof_probs[model_key][val_idx] = np.full((len(val_idx), n_classes), 1.0/n_classes)\n                else:\n                    accuracy = fold_result['accuracy']\n                    print(f\"  {config['name']} fold {fold+1} accuracy: {accuracy:.4f}\")\n                    model_fold_accuracies[model_key].append(accuracy)\n                    oof_probs[model_key][val_idx] = fold_result['predicted_probabilities']\n                    \n            except Exception as e:\n                logging.error(f\"Error running {config['name']} on fold {fold+1}: {e}\")\n                model_fold_accuracies[model_key].append(0.0)\n                oof_probs[model_key][val_idx] = np.full((len(val_idx), n_classes), 1.0/n_classes)\n\n    # Calculate average accuracies and filter models\n    model_avg_accuracies = {k: np.mean(v) for k, v in model_fold_accuracies.items()}\n    passed_models = [k for k, acc in model_avg_accuracies.items() if acc >= accuracy_threshold]\n    \n    print(f\"Model average accuracies: {model_avg_accuracies}\")\n    print(f\"Models passing threshold ({accuracy_threshold*100:.1f}%): {passed_models}\")\n    \n    if not passed_models:\n        raise ValueError(f\"No models met the accuracy threshold of {accuracy_threshold*100:.1f}%.\")\n\n    # STAGE 2: Hyperparameter tuning only for passed models\n    best_params = {}\n    if tune_hyperparams:\n        print(\"STAGE 2: Tuning hyperparameters for filtered models...\")\n        \n        X_train_hyper, X_test_hyper, y_train_hyper, y_test_hyper = train_test_split(\n            X_train, y_train, test_size=0.2, random_state=101, stratify=y_train\n        )\n        \n        for model_key in passed_models:\n            config = enabled_models[model_key]\n            print(f\"Tuning {config['name']} hyperparameters...\")\n            \n            try:\n                if model_key == 'tabpfn':\n                    # Special handling for TabPFN\n                    best_params[model_key] = _tune_tabpfn_hyperparams(\n                        config['function'], X_train_hyper, y_train_hyper, \n                        X_test_hyper, y_test_hyper, config['tune_params']['max_time_options']\n                    )\n                else:\n                    # Standard tuning for other models\n                    result = config['function'](\n                        X_train_hyper, y_train_hyper, X_test_hyper, y_test_hyper,\n                        tune_hyperparams=True, verbose=True, **config['tune_params']\n                    )\n                    best_params[model_key] = result['params']\n                \n                print(f\"Best {config['name']} params: {best_params[model_key]}\")\n                \n            except Exception as e:\n                logging.error(f\"Error tuning {config['name']}: {e}\")\n                best_params[model_key] = None\n    else:\n        best_params = {model_key: None for model_key in passed_models}\n\n    # Create meta training features from out-of-fold predictions\n    meta_feature_list = [oof_probs[model_key] for model_key in passed_models]\n    meta_X_train = np.hstack(meta_feature_list)\n    print(f\"Meta training features shape: {meta_X_train.shape}\")\n\n    # STAGE 3: Train final models on full training data\n    print(\"STAGE 3: Training final models on full training data...\")\n    test_results = {}\n    for model_key in passed_models:\n        config = enabled_models[model_key]\n        print(f\"Training final {config['name']} model on full training data...\")\n        \n        try:\n            result = config['function'](\n                X_train, y_train, X_test, y_test,\n                tune_hyperparams=False,\n                params=best_params[model_key],\n                verbose=True\n            )\n            test_results[model_key] = result['predicted_probabilities']\n            print(f\"Successfully trained final {config['name']} model\")\n            \n        except Exception as e:\n            logging.error(f\"Error training final {config['name']} model: {e}\")\n            test_results[model_key] = np.full((X_test.shape[0], n_classes), 1.0/n_classes)\n\n    # Create meta test features\n    meta_test_feature_list = [test_results[model_key] for model_key in passed_models]\n    meta_X_test = np.hstack(meta_test_feature_list)\n    print(f\"Meta test features shape: {meta_X_test.shape}\")\n\n    # Train meta model\n    print(\"Training meta model...\")\n    meta_model = xgb.XGBClassifier(objective='multi:softprob', random_state=random_state)\n    meta_model.fit(meta_X_train, y_train)\n\n    # Make predictions\n    train_preds = meta_model.predict(meta_X_train)\n    test_preds = meta_model.predict(meta_X_test)\n\n    # Print summary\n    print(f\"\\nSummary:\")\n    print(f\"- Used models: {passed_models}\")\n    print(f\"- Meta features: {meta_X_train.shape[1]}\")\n    print(f\"- Meta model train accuracy: {accuracy_score(y_train, train_preds):.4f}\")\n    print(f\"- Meta model test accuracy: {accuracy_score(y_test, test_preds):.4f}\")\n\n    return meta_model, meta_X_train, meta_X_test, train_preds, test_preds\n\n\ndef _tune_tabpfn_hyperparams(tabpfn_function, X_train, y_train, X_test, y_test, max_time_options):\n    \"\"\"Special hyperparameter tuning for TabPFN using different max_time values\"\"\"\n    best_accuracy = 0.0\n    best_max_time = max_time_options[0]\n    \n    print(f\"Tuning TabPFN with max_time options: {max_time_options}\")\n    \n    for max_time in max_time_options:\n        try:\n            result = tabpfn_function(\n                X_train, y_train, X_test, y_test,\n                tune_hyperparams=True, max_time=max_time\n            )\n            \n            if result.get('skipped', False):\n                continue\n                \n            accuracy = result['accuracy']\n            print(f\"TabPFN with max_time={max_time}: accuracy={accuracy:.4f}\")\n            \n            if accuracy > best_accuracy:\n                best_accuracy = accuracy\n                best_max_time = max_time\n                \n        except Exception as e:\n            logging.error(f\"Error testing TabPFN with max_time={max_time}: {e}\")\n    \n    return {'max_time': best_max_time}\n\n# Train the ensemble models on regression tasks -> Co-ordinates predictions\ndef train_hierarchical_coordinate_layer(\n        X_train, X_test, y_train_lat, y_train_lon,\n        y_test_lat, y_test_lon, y_train_coords,\n        y_test_coords, coord_scaler,\n        run_xgboost_regressor = None,\n        run_grownet_regressor = None,\n        run_nn_regressor = None,\n        run_tabpfn_regressor = None,\n        run_lightgbm_regressor = None,\n        run_catboost_regressor = None,\n        tune_hyperparams = False,\n        random_state = 42,\n        n_splits = 3\n    ):\n    \"\"\"\n    Two-stage hierarchical coordinate prediction:\n    1. Run all models with default params, select best by average median distance\n    2. Tune hyperparameters only for the best model\n    \"\"\"\n\n    model_configs = {\n        'xgb':{\n            'name':'XGBoost',\n            'function':run_xgboost_regressor,\n            'enabled': run_xgboost_regressor is not None,\n            'prediction_type':'sequential',\n            'tune_params': {'n_trials': 50, 'timeout': 1800}\n        },\n        'grownet':{\n            'name':'GrowNet',\n            'function':run_grownet_regressor,\n            'enabled':run_grownet_regressor is not None,\n            'prediction_type':'xyz',\n            'tune_params': {'n_trials': 50, 'timeout': 1800}\n        },\n        'nn':{\n            'name':'Neural Network',\n            'function': run_nn_regressor,\n            'enabled':run_nn_regressor is not None,\n            'prediction_type':'xyz',\n            'tune_params': {'n_trials': 50, 'timeout': 1800}\n        },\n        'tabpfn':{\n            'name':'TabPFN',\n            'function':run_tabpfn_regressor,\n            'enabled':run_tabpfn_regressor is not None,\n            'prediction_type':'xyz',\n            'tune_params': {'n_trials': 20, 'max_time_options': [30, 60, 120]}\n        },\n        'lightgbm':{\n            'name':'LightGBM',\n            'function':run_lightgbm_regressor,\n            'enabled':run_lightgbm_regressor is not None,\n            'prediction_type':'sequential',\n            'tune_params': {'n_trials': 50, 'timeout': 1800}\n        },\n        'catboost':{\n            'name':'CatBoost',\n            'function':run_catboost_regressor,\n            'enabled':run_catboost_regressor is not None,\n            'prediction_type':'sequential',\n            'tune_params': {'n_trials': 50, 'timeout': 1800}\n        }\n    }\n\n    enabled_models = {k: v for k,v in model_configs.items() if v['enabled']}\n\n    if not enabled_models:\n        raise ValueError(\"At least one model function must be provided (not None)\")\n    \n    print(f\"Enabled models: {list(enabled_models.keys())}\")\n\n    # STAGE 1: Run all models with default parameters to calculate average median distance\n    print(\"STAGE 1: Running all models with default parameters...\")\n    \n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    n_train_samples = X_train.shape[0]\n    y_train_combined = np.stack([y_train_lat, y_train_lon], axis=1)\n\n    # Track average median distances across folds for each model\n    model_avg_median_distances = {}\n    \n    for model_key, config in enabled_models.items():\n        print(f\"Running {config['name']} with default parameters...\")\n        fold_median_distances = []\n        \n        for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n            y_fold_train_lat, y_fold_val_lat = y_train_lat[train_idx], y_train_lat[val_idx]\n            y_fold_train_lon, y_fold_val_lon = y_train_lon[train_idx], y_train_lon[val_idx]\n            y_fold_train_coords, y_fold_val_coords = y_train_coords[train_idx], y_train_coords[val_idx]\n            y_fold_val_combined = np.stack([y_fold_val_lat, y_fold_val_lon], axis=1)\n            \n            try:\n                if config['prediction_type'] == \"sequential\":\n                    # Predict latitude first\n                    lat_result = config['function'](\n                        X_fold_train, y_fold_train_lat, X_fold_val, y_fold_val_lat,\n                        tune_hyperparams=False, params=None, verbose=False\n                    )\n                    \n                    if lat_result.get('skipped', False):\n                        fold_median_distances.append(float('inf'))\n                        continue\n                        \n                    lat_pred_train = lat_result['model'].predict(X_fold_train)\n                    lat_pred_val = lat_result['predictions']\n\n                    # Augment features with latitude predictions\n                    X_fold_train_aug = np.hstack([X_fold_train, lat_pred_train.reshape(-1, 1)])\n                    X_fold_val_aug = np.hstack([X_fold_val, lat_pred_val.reshape(-1, 1)])\n\n                    # Predict longitude\n                    lon_result = config['function'](\n                        X_fold_train_aug, y_fold_train_lon, X_fold_val_aug, y_fold_val_lon,\n                        tune_hyperparams=False, params=None, verbose=False\n                    )\n                    \n                    if lon_result.get('skipped', False):\n                        fold_median_distances.append(float('inf'))\n                        continue\n                        \n                    lon_pred_val = lon_result['predictions']\n                    val_predictions = np.stack([lat_pred_val, lon_pred_val], axis=1)\n\n                elif config['prediction_type'] == 'xyz':\n                    fold_result = config['function'](\n                        X_fold_train, y_fold_train_coords, X_fold_val, y_fold_val_coords,\n                        tune_hyperparams=False, params=None, verbose=False\n                    )\n                    \n                    if fold_result.get('skipped', False):\n                        fold_median_distances.append(float('inf'))\n                        continue\n                    \n                    xyz_pred = fold_result['predictions']\n                    # Ensure predictions are 2D\n                    if xyz_pred.ndim == 1:\n                        xyz_pred = xyz_pred.reshape(-1, 3)\n                    xyz_rescaled = coord_scaler.inverse_transform(xyz_pred)\n                    val_predictions = xyz_to_latlon(xyz_rescaled)\n                \n                # Calculate median distance for this fold\n                distances = haversine_distance(\n                    y_fold_val_combined[:, 0], y_fold_val_combined[:, 1],\n                    val_predictions[:, 0], val_predictions[:, 1]\n                )\n                fold_median_distances.append(np.median(distances))\n                print(f\"  {config['name']} fold {fold+1} median distance: {np.median(distances):.2f} km\")\n                \n            except Exception as e:\n                logging.error(f\"Error running {config['name']} on fold {fold+1}: {e}\")\n                fold_median_distances.append(float('inf'))\n        \n        model_avg_median_distances[model_key] = np.mean(fold_median_distances)\n        print(f\"{config['name']} average median distance: {model_avg_median_distances[model_key]:.2f} km\")\n    \n    # Select best model by lowest average median distance\n    best_model = min(model_avg_median_distances, key=model_avg_median_distances.get)\n    print(f\"Best model by average median distance: {best_model}\")\n    print(f\"Model average median distances: {model_avg_median_distances}\")\n    \n    # STAGE 2: Hyperparameter tuning only for the best model\n    best_params = None\n    if tune_hyperparams:\n        print(\"STAGE 2: Tuning hyperparameters for the best model...\")\n        \n        X_train_hyper, X_test_hyper, y_train_hyper_lat, y_test_hyper_lat = train_test_split(\n            X_train, y_train_lat, test_size=0.2, random_state=101\n        )\n        _, _, y_train_hyper_lon, y_test_hyper_lon = train_test_split(\n            X_train, y_train_lon, test_size=0.2, random_state=101\n        )\n        _, _, y_train_hyper_coords, y_test_hyper_coords = train_test_split(\n            X_train, y_train_coords, test_size=0.2, random_state=101\n        )\n        \n        config = enabled_models[best_model]\n        print(f\"Tuning {config['name']} hyperparameters...\")\n        \n        try:\n            if config['prediction_type'] == 'sequential':\n                # For sequential models, tune on latitude prediction\n                tune_params = config['tune_params'].copy()\n                tune_params.update({\n                    'tune_hyperparams': True,\n                    'verbose': True\n                })\n                \n                result = config['function'](\n                    X_train_hyper, y_train_hyper_lat, X_test_hyper, y_test_hyper_lat,\n                    **tune_params\n                )\n                \n            elif config['prediction_type'] == 'xyz':\n                tune_params = config['tune_params'].copy()\n                tune_params.update({\n                    'tune_hyperparams': True,\n                    'verbose': True\n                })\n                \n                if best_model == 'tabpfn':\n                    # Special handling for TabPFN\n                    best_params = _tune_tabpfn_regressor_hyperparams(\n                        config['function'], X_train_hyper, y_train_hyper_coords,\n                        X_test_hyper, y_test_hyper_coords, tune_params['max_time_options']\n                    )\n                else:\n                    result = config['function'](\n                        X_train_hyper, y_train_hyper_coords, X_test_hyper, y_test_hyper_coords,\n                        **tune_params\n                    )\n                    best_params = result.get('params')\n            \n            if best_params is None and 'result' in locals():\n                best_params = result.get('params')\n            print(f\"Best {config['name']} params: {best_params}\")\n            \n        except Exception as e:\n            logging.error(f\"Error tuning {config['name']}: {e}\")\n            best_params = None\n\n    # STAGE 3: Final training with tuned parameters\n    print(\"STAGE 3: Final training with tuned parameters...\")\n    \n    config = enabled_models[best_model]\n    \n    try:\n        if config['prediction_type'] == 'sequential':\n            # Sequential prediction with tuned params\n            lat_result = config['function'](\n                X_train, y_train_lat, X_test, y_test_lat,\n                tune_hyperparams=False, params=best_params, verbose=True\n            )\n            \n            if lat_result.get('skipped', False):\n                raise ValueError(f\"Latitude prediction failed for {config['name']}\")\n                \n            lat_pred_train = lat_result['model'].predict(X_train)\n            lat_pred_test = lat_result['predictions']\n            \n            # Augment features with latitude predictions\n            X_train_aug = np.hstack([X_train, lat_pred_train.reshape(-1, 1)])\n            X_test_aug = np.hstack([X_test, lat_pred_test.reshape(-1, 1)])\n            \n            lon_result = config['function'](\n                X_train_aug, y_train_lon, X_test_aug, y_test_lon,\n                tune_hyperparams=False, params=best_params, verbose=True\n            )\n            \n            if lon_result.get('skipped', False):\n                raise ValueError(f\"Longitude prediction failed for {config['name']}\")\n                \n            lon_pred_test = lon_result['predictions']\n            test_preds = np.stack([lat_pred_test, lon_pred_test], axis=1)\n\n        elif config['prediction_type'] == 'xyz':\n            result = config['function'](\n                X_train, y_train_coords, X_test, y_test_coords,\n                tune_hyperparams=False, params=best_params, verbose=True\n            )\n            \n            if result.get('skipped', False):\n                raise ValueError(f\"XYZ prediction failed for {config['name']}\")\n            \n            xyz_pred = result['predictions']\n            # Ensure predictions are 2D\n            if xyz_pred.ndim == 1:\n                xyz_pred = xyz_pred.reshape(-1, 3)\n            xyz_rescaled = coord_scaler.inverse_transform(xyz_pred)\n            test_preds = xyz_to_latlon(xyz_rescaled)\n            \n    except Exception as e:\n        logging.error(f\"Error training final {config['name']} model: {e}\")\n        raise\n\n    # Calculate distance metrics\n    def calculate_distance_metrics(y_true, y_pred):\n        distances = haversine_distance(y_true[:, 0], y_true[:, 1], y_pred[:, 0], y_pred[:, 1])\n        return {\n            'median_distance': np.median(distances),\n            'mean_distance': np.mean(distances),\n            'percentile_95': np.percentile(distances, 95),\n            'percentile_99': np.percentile(distances, 99),\n            'distances': distances\n        }\n\n    y_test_combined = np.stack([y_test_lat, y_test_lon], axis=1)\n    test_metrics = calculate_distance_metrics(y_test_combined, test_preds)\n\n    print(f\"\\nSummary:\")\n    print(f\"- Used model: {best_model}\")\n    print(f\"- Test median distance: {test_metrics['median_distance']:.2f} km\")\n    print(f\"- Test mean distance: {test_metrics['mean_distance']:.2f} km\")\n    print(f\"- Test 95th percentile: {test_metrics['percentile_95']:.2f} km\")\n\n    return {\n        'test_preds': test_preds,\n        'test_metrics': test_metrics,\n        'enabled_models': [best_model],\n        'best_model': best_model,\n        'best_params': best_params\n    }\n\ndef _tune_tabpfn_regressor_hyperparams(tabpfn_function, X_train, y_train, X_test, y_test, max_time_options):\n    \"\"\"Special hyperparameter tuning for TabPFN regressor using different max_time values\"\"\"\n    best_r2 = -float('inf')\n    best_max_time = max_time_options[0]\n    \n    print(f\"Tuning TabPFN regressor with max_time options: {max_time_options}\")\n    \n    for max_time in max_time_options:\n        try:\n            result = tabpfn_function(\n                X_train, y_train, X_test, y_test,\n                tune_hyperparams=True, max_time=max_time, verbose=False\n            )\n            \n            if result.get('skipped', False):\n                continue\n                \n            r2 = result.get('r2_score', -float('inf'))\n            print(f\"TabPFN regressor with max_time={max_time}: R²={r2:.4f}\")\n            \n            if r2 > best_r2:\n                best_r2 = r2\n                best_max_time = max_time\n                \n        except Exception as e:\n            logging.error(f\"Error testing TabPFN regressor with max_time={max_time}: {e}\")\n    \n    return {'max_time': best_max_time}\n\n# Process data\ndf = pd.read_csv(\"/kaggle/input/metasub-data/metasub_training_testing_data.csv\")\nprocessed_data = process_data_hierarchical(df)\n\nX_cont = processed_data['x_cont']\ny_cont = processed_data['y_continent']\ny_cities = processed_data['y_city']\ny_coords = processed_data['y_coords']\ny_latitude = processed_data['y_latitude']\ny_longitude = processed_data['y_longitude']\n\n\nsplit_data = hierarchical_split(\n    X_cont,\n    y_cont,\n    y_cities,\n    y_coords,\n    processed_data['y_latitude'],\n    processed_data['y_longitude']\n)\n\n# Original feautres\nX_train_cont, X_test_cont = split_data['X_train'], split_data['X_test']\n# Train and test for continent\ny_train_cont, y_test_cont = split_data['y_cont_train'], split_data['y_cont_test']\n# Train and test for cities\ny_train_city, y_test_city = split_data['y_city_train'], split_data['y_city_test']\n# Train and test for latitude\ny_train_lat, y_test_lat = split_data['y_lat_train'], split_data['y_lat_test']\n# Train and test for longitude\ny_train_lon, y_test_lon = split_data['y_lon_train'], split_data['y_lon_test']\n# Train and test for co-ordinates\ny_train_coords, y_test_coords = split_data['y_coords_train'],  split_data['y_coords_test']\n\n# Continent layer\ncontinent_model, meta_X_train_cont, meta_X_test_cont, cont_train_preds, cont_test_preds = train_hierarchical_layer(\n    X_train=X_train_cont,\n    X_test=X_test_cont,\n    y_train=y_train_cont,\n    y_test=y_test_cont,\n    run_xgboost_classifier=None,\n    run_grownet_classifier=None,\n    run_nn_classifier=None,\n    run_tabpfn_classifier=run_tabpfn_classifier,\n    run_lightgbm_classifier=None,\n    run_catboost_classifier=None,\n    tune_hyperparams=False,\n    apply_smote=True,\n    n_splits=5,\n    accuracy_threshold=0.90  # 91% for continent\n)\n\nexit()\n\n# City layer \nX_train_city = np.hstack([X_train_cont,meta_X_train_cont])\nX_test_city = np.hstack([X_test_cont,meta_X_test_cont])\n\ncity_model, meta_X_train_city, meta_X_test_city, city_train_preds, city_test_preds = train_hierarchical_layer(\n    X_train=X_train_city,\n    X_test=X_test_city,\n    y_train=y_train_city,\n    y_test=y_test_city,\n    run_xgboost_classifier=run_xgboost_classifier,\n    run_grownet_classifier=None,\n    run_lightgbm_classifier=run_lightgbm_classifier,\n    run_catboost_classifier=None,\n    run_nn_classifier=None,\n    run_tabpfn_classifier=None,  # Now handles GPU/CPU automatically\n    tune_hyperparams=False,\n    apply_smote=False,\n    n_splits=5,\n    accuracy_threshold=0.91  # 89% for city\n)\n\n# Coordinate layer\n\nX_train_coord = np.hstack([X_train_city,meta_X_train_city])\nX_test_coord = np.hstack([X_test_city,meta_X_test_city])\n\n\ncoords_results = train_hierarchical_coordinate_layer(\n    X_train=X_train_coord,\n    X_test=X_test_coord,\n    y_train_lat=y_train_lat,\n    y_train_lon = y_train_lon,\n    y_test_lat=y_test_lat,\n    y_test_lon=y_test_lon,\n    y_train_coords=y_train_coords,\n    y_test_coords=y_test_coords,\n    coord_scaler=processed_data['encoders']['coord'],\n    run_xgboost_regressor=run_xgboost_regressor,\n    run_tabpfn_regressor=run_tabpfn_regressor,\n    run_nn_regressor=None,\n    run_grownet_regressor=None,\n    run_lightgbm_regressor=None,\n    run_catboost_regressor=None,\n    tune_hyperparams=False,\n    n_splits=5\n)\n\n\n# All metrics\nsave_dir = \"saved_results/\"\nos.makedirs(save_dir,exist_ok=True)\n# Continent Layer\n\nprint(\"\\nContinent Prediction - Test Set:\")\nprint(classification_report(y_test_cont, cont_test_preds,target_names=processed_data['continents']))\n# Save the test predictions\nnp.save(os.path.join(save_dir, \"x_test.npy\"), X_test_cont)\nnp.save(os.path.join(save_dir, \"y_test_cont.npy\"),y_test_cont)\nnp.save(os.path.join(save_dir, \"y_pred_cont.npy\"),cont_test_preds)\n\n# City Layer\n\nprint(\"\\nCity Prediction - Test Set:\")\nprint(classification_report(y_test_city,city_test_preds))\n# Save the test predictions\nnp.save(os.path.join(save_dir,\"y_test_city.npy\"),y_test_city)\nnp.save(os.path.join(save_dir,\"y_pred_city.npy\"),city_test_preds)\n\n# Co-ordinate Layer\nprint(\"Coordinate prediction results:\")\nprint(f\"Test Median Distance: {coords_results['test_metrics']['median_distance']:.2f} km\")\nprint(f\"Test Mean Distance: {coords_results['test_metrics']['mean_distance']:.2f} km\")\nprint(f\"Test 95th Percentile: {coords_results['test_metrics']['percentile_95']:.2f} km\")\n\n# Save the test predictions\nnp.save(os.path.join(save_dir,\"y_test_coord.npy\"),np.stack([y_test_lat,y_test_lon],axis=1).astype(np.float32))\nnp.save(os.path.join(save_dir,\"y_pred_coord.npy\"),coords_results['test_preds'])\n\n\n# Error calculations\ndef error_calc(test_conts,pred_conts,test_city,pred_city,test_lat,pred_lat,test_lon,pred_lon):\n    error_df = pd.DataFrame({\n        'true_cont': test_conts,\n        'pred_cont': pred_conts,\n        'true_city': test_city,\n        'pred_city': pred_city,\n        'true_lat': test_lat,\n        'true_lon': test_lon,\n        'pred_lat': pred_lat,\n        'pred_lon': pred_lon\n    })\n\n\n    # Assign true contient and city names\n    error_df['true_cont_name'] = error_df['true_cont'].map(lambda i: processed_data['continents'][i])\n    error_df['pred_cont_name'] = error_df['pred_cont'].map(lambda i: processed_data['continents'][i])\n\n    error_df['true_city_name'] = error_df['true_city'].map(lambda i: processed_data['cities'][i])\n    error_df['pred_city_name'] = error_df['pred_city'].map(lambda i: processed_data['cities'][i])\n\n    cont_support_map = dict(zip(np.unique(error_df['true_cont_name'],return_counts=True)[0],np.unique(error_df['true_cont_name'],return_counts=True)[1]))\n    city_support_map = dict(zip(np.unique(error_df['true_city_name'],return_counts=True)[0],np.unique(error_df['true_city_name'],return_counts=True)[1]))\n\n    # Step 1: Compute the correctness\n    error_df['continent_correct'] = error_df['true_cont'] == error_df['pred_cont']\n    error_df['city_correct'] = error_df['true_city'] == error_df['pred_city']\n\n    # Step 2: Calculate the haversine distance\n    error_df['coord_error'] = haversine_distance(error_df['true_lat'],error_df['true_lon'],error_df['pred_lat'],error_df['pred_lon'])\n\n    # Print the distance error statistics\n    print(f\"The median distance error is {np.median(error_df['coord_error'].values)}\")\n    print(f\"The mean distance error is {np.mean(error_df['coord_error'].values)}\")\n    print(f\"The max distance error is {np.max(error_df['coord_error'].values)}\")\n\n    # Step 3: Group into 4 categories\n    def group_label(row):\n        if row['continent_correct'] and row['city_correct']:\n            return 'C_correct Z_correct'\n        elif row['continent_correct'] and not row['city_correct']:\n            return 'C_correct Z_wrong'\n        elif not row['continent_correct'] and row['city_correct']:\n            return 'C_wrong Z_correct'\n        else:\n            return 'C_wrong Z_wrong'\n        \n    # Create the error group column\n    error_df['error_group'] = error_df.apply(group_label, axis=1)\n\n    # Now we proceed with grouping\n    group_stats = error_df.groupby('error_group')['coord_error'].agg([\n        ('count','count'),\n        ('mean_error_km','mean'),\n        ('median_error_km','median')\n    ])\n\n    # Step 5: Calculate proportion and expected error.\n    \"\"\"\n    P(C=C*) : Probability of contient predicting correct continent\n    P(Z=Z*) : Probability of ciry predicting correct city\n    E(D|condition) : Expected distance error under that condition\n\n    E(D) = P(C=C*,Z=Z*)*E(D|C=C*,Z=Z*)+ -> ideal condition continent is correct and city is also correct\n            P(C=C*,Z!=Z*)*E(D|C=C*,Z!=Z*)+ -> continent is correct and city is wrong\n            P(C!=C*,Z=Z*)*E(D|C!=C*,Z=Z*)+ -> city is correct but continent is wrong\n            P(C!=C*,Z!=Z*)*E(D|C!=C*,Z!=Z*) -> both cotinent and city are wrong\n    \"\"\"\n    total = len(error_df)\n    group_stats['proportion'] = group_stats['count'] / total\n    group_stats['weighted_error'] = group_stats['mean_error_km'] * group_stats['proportion']\n    expected_total_error = group_stats['weighted_error'].sum()\n    print(group_stats)\n    print(f\"Expected Coordinate Error E[D]: {expected_total_error:.2f} km\")\n\n    def compute_in_radius_metrics(y_true, y_pred, thresholds=None):\n        \"\"\"\n        Compute % of predictions within given distance thresholds\n        y_true, y_pred: numpy arrays of shape (N, 2) for [lat, lon]\n        thresholds: List of distance thresholds in kilometers (default: [1, 5, 50, 100, 250, 500, 1000, 5000])\n        \"\"\"\n        if thresholds is None:\n            thresholds = [1, 5, 50, 100, 250, 500, 1000, 5000]\n\n        distances = haversine_distance(\n            y_true[:, 0], y_true[:, 1], y_pred[:, 0], y_pred[:, 1]\n        )\n\n        results = {}\n        for r in thresholds:\n            percent = np.mean(distances <= r) * 100\n            results[f\"<{r} km\"] = percent\n\n        return results\n\n    metrics = compute_in_radius_metrics(y_true=np.stack([test_lat,test_lon],axis=1), y_pred=np.stack([pred_lat,pred_lon],axis=1))\n\n    print(\"In-Radius Accuracy Metrics:\")\n    for k, v in metrics.items():\n        print(f\"{k:>8}: {v:.2f}%\")\n        \n    def in_radius_by_group(df, group_col, thresholds=[1, 5, 50, 100, 250, 500, 1000, 5000]):\n        \"\"\"\n        Compute in-radius accuracy for a group column (continent, city, or continent+city)\n        \"\"\"\n        df = df.copy()\n        df['coord_error'] = haversine_distance(\n            df['true_lat'].values, df['true_lon'].values,\n            df['pred_lat'].values, df['pred_lon'].values\n        )\n\n        results = {}\n        grouped = df.groupby(group_col)\n\n        for group_name, group_df in grouped:\n            res = {}\n            errors = group_df['coord_error'].values\n            for r in thresholds:\n                res[f\"<{r} km\"] = np.mean(errors <= r) * 100  # in %\n            results[group_name] = res\n\n        return pd.DataFrame(results).T  # Transpose for better readability\n    \n    continent_metrics = in_radius_by_group(error_df, group_col='true_cont_name')\n    print(\"In-Radius Accuracy per Continent\")\n    continent_metrics['continent_support'] = continent_metrics.index.map(cont_support_map)\n    print(continent_metrics.round(2))\n\n    city_metrics = in_radius_by_group(error_df, group_col='true_city_name')\n    print(\"In-Radius Accuracy per City\")\n    city_metrics['city_support'] = city_metrics.index.map(city_support_map)\n    print(city_metrics.round(2))\n\n    error_df['continent_city'] = error_df['true_cont_name'] + \" / \" + error_df['true_city_name']\n    cont_city_metrics = in_radius_by_group(error_df, group_col='continent_city')\n    cont_city_metrics['continent_support'] = cont_city_metrics.index.map(lambda x :x.split(\"/\")[-1].strip()).map(city_support_map)\n    print(\"In-Radius Accuracy per Continent-City\")\n    print(cont_city_metrics.round(2))\n\n\n# Error calculations for all the predictions\nprint(\"Starting error calculations...\")\nerror_calc(test_conts=y_test_cont,pred_conts=cont_test_preds,\n           test_city=y_test_city,pred_city = city_test_preds,\n           test_lat=y_test_lat,pred_lat=coords_results['test_preds'][:,0],\n           test_lon=y_test_lon,pred_lon=coords_results['test_preds'][:,1])\n\n# Plot the points on the world map\n#print(\"Plotting points on world map...\")\n#plot_points_on_world_map(true_lat = y_test_lat,\n#                         true_long=y_test_lon,\n#                         predicted_lat=coords_results['test_preds'][:,0],\n#                         predicted_long=coords_results['test_preds'][:,1],\n#                         filename=\"test.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T14:20:35.065722Z","iopub.execute_input":"2025-08-06T14:20:35.065972Z","iopub.status.idle":"2025-08-06T14:20:35.627235Z","shell.execute_reply.started":"2025-08-06T14:20:35.065945Z","shell.execute_reply":"2025-08-06T14:20:35.626438Z"}},"outputs":[{"name":"stdout","text":"Continents: 7 (['east_asia' 'europe' 'middle_east' 'north_america' 'oceania'\n 'south_america' 'sub_saharan_africa'])\nCities: 40\nContinuous features: 200\nEnabled models: ['tabpfn']\nSTAGE 1: Running cross-validation to filter models and generate meta-features...\nProcessing Fold 1/5\n  Running TabPFN on fold 1...\nProcessing Fold 2/5\n  Running TabPFN on fold 2...\nProcessing Fold 3/5\n  Running TabPFN on fold 3...\nProcessing Fold 4/5\n  Running TabPFN on fold 4...\nProcessing Fold 5/5\n  Running TabPFN on fold 5...\nModel average accuracies: {'tabpfn': 0.0}\nModels passing threshold (90.0%): []\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3191732898.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;31m# Continent layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m continent_model, meta_X_train_cont, meta_X_test_cont, cont_train_preds, cont_test_preds = train_hierarchical_layer(\n\u001b[0m\u001b[1;32m    768\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_cont\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test_cont\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3191732898.py\u001b[0m in \u001b[0;36mtrain_hierarchical_layer\u001b[0;34m(X_train, X_test, y_train, y_test, run_xgboost_classifier, run_grownet_classifier, run_nn_classifier, run_tabpfn_classifier, run_lightgbm_classifier, run_catboost_classifier, tune_hyperparams, apply_smote, random_state, n_splits, accuracy_threshold)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpassed_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No models met the accuracy threshold of {accuracy_threshold*100:.1f}%.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;31m# STAGE 2: Hyperparameter tuning only for passed models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: No models met the accuracy threshold of 90.0%."],"ename":"ValueError","evalue":"No models met the accuracy threshold of 90.0%.","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}