{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metasub data mGPS algorithm - 31/03/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am just trying to get the data pre-processing steps right. The idea for doing this is to get the dataset in the right format for easier analysis using neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "os.chdir(\"/home/chandru/binp37/\")\n",
    "# Read the metadata for the metasub data.\n",
    "complete_meta = pd.read_csv(\"./data/metasub/complete_metadata.csv\")\n",
    "taxa_abund = pd.read_csv(\"./data/metasub/metasub_taxa_abundance.csv\")\n",
    "taxa_abund = taxa_abund.drop_duplicates(subset=['uuid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4288, 3711)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the bacterial and metadata\n",
    "metasub_data = pd.merge(complete_meta,taxa_abund,on='uuid')\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4157, 3711)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove control samples\n",
    "control_cities = {'control','other_control','neg_control','other','pos_control'}\n",
    "control_types = {'ctrl cities','negative_control','positive_control'}\n",
    "\n",
    "mask = metasub_data['city'].isin(control_cities) | metasub_data['control_type'].isin(control_types)\n",
    "metasub_data = metasub_data[~mask].copy()\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4157, 3711)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Re-label london boroughs\n",
    "metasub_data.loc[metasub_data['city'].isin(['kensington','islington']),'city'] = 'london'\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove sparse sample locations and doubtful samples\n",
    "city_counts = metasub_data['city'].value_counts()\n",
    "small_cities = city_counts[city_counts<8].index.tolist()\n",
    "remove_samples = metasub_data['city'].isin(['antarctica']+small_cities)\n",
    "metasub_data = metasub_data[~remove_samples]\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct the identified mislabeling of data\n",
    "kyiv_filter = metasub_data['city'] == 'kyiv'\n",
    "metasub_data.loc[kyiv_filter,'latitude'] = metasub_data.loc[kyiv_filter,'city_latitude'] # Set all the latitude to the city_latitude\n",
    "metasub_data.loc[kyiv_filter,'longitude'] = metasub_data.loc[kyiv_filter,'city_longitude'] # Set all the latitude to the city_longitutde\n",
    "\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing latitude and longitude values with city-level data\n",
    "missing_lat = metasub_data[\"latitude\"].isna()\n",
    "missing_lon = metasub_data[\"longitude\"].isna()\n",
    "metasub_data.loc[missing_lat, \"latitude\"] = metasub_data.loc[missing_lat, \"city_latitude\"]\n",
    "metasub_data.loc[missing_lon, \"longitude\"] = metasub_data.loc[missing_lon, \"city_longitude\"]\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correction for incorrect London co-ordinates\n",
    "london_filter = metasub_data['city'] == 'london'\n",
    "metasub_data.loc[london_filter,'city_latitude'] = 51.50853\n",
    "metasub_data.loc[london_filter,'city_longitude'] = -0.12574\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination - 02/04/2025\n",
    "\n",
    "Here we use RFE as a part of a pipeline to get the best set of parameters for fitting the deep learning model. We need a suitable set of parameters for that are infromative enough to apply a deep learning model. These are called as geographically informative taxa (GITs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import multiprocessing\n",
    "import time\n",
    "#import ray\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "*Explanation of the Code*\n",
    "\n",
    "This function is designed to perform feature selection by:\n",
    "\n",
    "1. **Removing Highly Correlated Features:**\n",
    "   - Computes a correlation matrix of the features.\n",
    "   - Identifies features with correlation greater than 0.98 and removes them.\n",
    "\n",
    "2. **Recursive Feature Elimination (RFE) for Feature Selection:**\n",
    "   - Uses a Random Forest Classifier to determine feature importance.\n",
    "   - Applies RFE (Recursive Feature Elimination) to iteratively remove the least important features.\n",
    "   - The number of features to keep is determined based on predefined subset sizes.\n",
    "\n",
    "3. **Parallel Processing Support:**  \n",
    "   Uses multiple CPU cores when specified for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection algorithm\n",
    "def species_select(X,y,remove_correlated=True,subsets=None,cores=1):\n",
    "    \"\"\"\n",
    "    Feature selection algorithm for species classification.\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.DataFrame): Feature matrix\n",
    "    y (pd.Series): Target variable\n",
    "    remove_correlated (bool): Whether we need to remoce highly correlated variables. (default is set to True)\n",
    "    subsets (list): List of feature subset sizes to evaluate. If None, it is determined automatically.\n",
    "    cores (int): Number of CPU cores to use for parallel computation. (default is set to 1)\n",
    "    \n",
    "    Returns:\n",
    "    RFE object: Trained Recursive Feature Elimination (RFE) model.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()  # Track execution time\n",
    "\n",
    "    # Set parallel processing\n",
    "    num_cores = multiprocessing.cpu_count() if cores > 1 else 1\n",
    "    print(f\"Using {num_cores} CPU cores for computation.\")\n",
    "\n",
    "    if remove_correlated:\n",
    "        # Compute correlation matrix\n",
    "        print(\"Calculating correlation matrix...\")\n",
    "        corr_matrix = X.corr()\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "        # Identify correlated features (above 0.98)\n",
    "        correlated_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.98)]\n",
    "\n",
    "        # Drop correlated features\n",
    "        X = X.drop(columns=correlated_features)\n",
    "        print(f\"Correlated feature removed: {len(correlated_features)}\")\n",
    "\n",
    "    # Determine default subset sizes if not provided\n",
    "    num_features = X.shape[1]\n",
    "    if subsets is None:\n",
    "        subsets = [num_features // 2, num_features // 4, num_features // 8, num_features // 16, num_features // 32, num_features // 64]\n",
    "        subsets = [s for s in subsets  if s > 0] # Remove non-positive values\n",
    "\n",
    "    print(f\"Feature selection subsets: {subsets}\")\n",
    "\n",
    "    # Define model (Random Forrest for fearure ranking)\n",
    "    model = RandomForestClassifier(n_jobs=num_cores, random_state=123)\n",
    "    print(\"Initialized RandomForestClassifier.\")\n",
    "\n",
    "    # Recursive Feature Elimination (RFE)\n",
    "    for subset in subsets:\n",
    "        print(f\"\\nStarting RFE with {subset} features....\")\n",
    "        start_rfe = time.time()\n",
    "        rfe = RFE(estimator=model, n_features_to_select=min(subsets),step=20)\n",
    "        # Fit RFE to the data\n",
    "        rfe.fit(X,y)\n",
    "        print(f\"Completed RFE with {subset} features in {time.time() - start_rfe:.2f} seconds.\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nFeature selection completed in {total_time:.2f} seconds.\")\n",
    "    return rfe\n",
    "\n",
    "featureElim = species_select(X=metasub_data.iloc[:,42:500],\n",
    "                             y=metasub_data['city'],\n",
    "                             remove_correlated=False,\n",
    "                             subsets=[50,100,200,500],\n",
    "                             cores=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You Can Do If It's Still Too Slow\n",
    "\n",
    "1. **Reduce Feature Set Before Running RFE:**\n",
    "    - If remove_correlated=False, you still have 3711 features.\n",
    "    - Try using only top 500-1000 features based on variance or importance.\n",
    "\n",
    "2. **Increase Step Size in RFE:**\n",
    "    - Default step=1 removes one feature per iteration, which is slow.\n",
    "    - Try step=5 or step=10 to remove multiple features at once:\n",
    "    - rfe = RFE(estimatoe=model, n_features_to_select=subset, step=5)\n",
    "3. **Use XGBoost Instead of Random Forest:**\n",
    "    - model = XGBClassifier(n_jobs=num_cores, random_state=123)\n",
    "4. **Sequential Feature Selector:**\n",
    "    - from sklearn.feature_selection import SequentialFeatureSelector\n",
    "    sfs = SequentialFeatureSelector(model, n_features_to_select=100, direction='backward', n_jobs=num_cores)\n",
    "    sfs.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_rfe_feature_selection(X: pd.DataFrame, y: pd.Series, n_jobs: int = 1, random_state: int = 123,\n",
    "                                   cv: int = 10, subsets: list = None, remove_correlated: bool = True,\n",
    "                                   correlation_threshold: float = 0.98, num_cpus: int = None):\n",
    "    \"\"\"\n",
    "    Performs parallel Recursive Feature Elimination (RFE) with cross-validation to select the best feature subset.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame of features.\n",
    "        y (pd.Series): Series of the target variable.\n",
    "        n_jobs (int): Number of jobs for the base estimator (RandomForestClassifier).\n",
    "        random_state (int): Random state for reproducibility.\n",
    "        cv (int): Number of cross-validation folds.\n",
    "        subsets (list, optional): List of feature subset sizes to evaluate. If None, default subsets are used. Defaults to None.\n",
    "        remove_correlated (bool, optional): Whether to remove highly correlated features before RFE. Defaults to True.\n",
    "        correlation_threshold (float, optional): Threshold for identifying highly correlated features. Defaults to 0.98.\n",
    "        num_cpus (int, optional): Number of CPUs to use for Ray. If None, Ray will auto-detect. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - best_params (dict): Dictionary with the best RFE parameters.\n",
    "            - best_accuracy (float): The best mean cross-validation accuracy achieved.\n",
    "            - results_df (pd.DataFrame): DataFrame containing the mean accuracy for each feature subset size.\n",
    "            - elapsed_time (float): Total time taken for the feature selection process.\n",
    "    \"\"\"\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "    ray.init(ignore_reinit_error=True, num_cpus=num_cpus)\n",
    "\n",
    "    model = RandomForestClassifier(n_jobs=n_jobs, random_state=random_state)\n",
    "\n",
    "    if remove_correlated:\n",
    "        # Compute correlation matrix\n",
    "        print(\"Calculating correlation matrix...\")\n",
    "        corr_matrix = X.corr()\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "        # Identify correlated features (above threshold)\n",
    "        correlated_features = [column for column in upper_tri.columns if any(upper_tri[column] > correlation_threshold)]\n",
    "\n",
    "        # Drop correlated features\n",
    "        X = X.drop(columns=correlated_features)\n",
    "        print(f\"Correlated features removed: {len(correlated_features)}\")\n",
    "\n",
    "    # Determine default subset sizes if not provided\n",
    "    num_features = X.shape[1]\n",
    "    if subsets is None:\n",
    "        subsets = [num_features // 2, num_features // 4, num_features // 8, num_features // 16, num_features // 32, num_features // 64]\n",
    "        subsets = [s for s in subsets if s > 0]  # Remove non-positive values\n",
    "\n",
    "    n_features_options = sorted(list(set(subsets))) # Ensure unique and sorted subset sizes\n",
    "    total_iterations = len(n_features_options) * cv\n",
    "\n",
    "    print(f\"\\nStarting RFE with subsets of features: {n_features_options}\")\n",
    "\n",
    "    # Define remote function for parallel execution\n",
    "    @ray.remote\n",
    "    def evaluate_rfe_remote(n_features, fold, X_remote, y_remote):\n",
    "        \"\"\"Performs RFE feature selection and evaluates performance for a given fold.\"\"\"\n",
    "        pipe = make_pipeline(RFE(estimator=model, n_features_to_select=n_features, step=10))\n",
    "\n",
    "        # We use the stratified K fold to split the data into training and validation sets\n",
    "        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=fold)\n",
    "        train_index, test_index = list(skf.split(X_remote, y_remote))[fold]\n",
    "\n",
    "        # train_index and test_index contain the index values for extracting training and testing data\n",
    "        X_train = X_remote.iloc[train_index, :]\n",
    "        X_test = X_remote.iloc[test_index, :]\n",
    "        y_train = y_remote.iloc[train_index]\n",
    "        y_test = y_remote.iloc[test_index]\n",
    "\n",
    "        # Fit the model using the training data and then evaluate the score based on the testing data\n",
    "        pipe.fit(X_train, y_train)\n",
    "        score = pipe.score(X_test, y_test)\n",
    "\n",
    "        return n_features, fold, score\n",
    "\n",
    "    start_time = time.time()\n",
    "    X_ray = ray.put(X)\n",
    "    y_ray = ray.put(y)\n",
    "    tasks = [evaluate_rfe_remote.remote(n_features, fold, X_ray, y_ray)\n",
    "             for n_features in n_features_options for fold in range(cv)]\n",
    "\n",
    "    results = []\n",
    "    with tqdm(total=total_iterations, desc='Parallel RFE + Cross-validation') as pbar:\n",
    "        while tasks:\n",
    "            done, tasks = ray.wait(tasks, num_returns=1)\n",
    "            result = ray.get(done[0])\n",
    "            results.append((result[0], result[2]))  # (n_features, score)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Aggregate mean accuracy for each feature subset\n",
    "    results_df = pd.DataFrame(results, columns=[\"n_features\", \"accuracy\"])\n",
    "    results_df = results_df.groupby(\"n_features\").mean().reset_index()\n",
    "\n",
    "    # Find best feature subset\n",
    "    best_row = results_df.loc[results_df[\"accuracy\"].idxmax()]\n",
    "    best_n_features = int(best_row[\"n_features\"])\n",
    "    best_accuracy = best_row[\"accuracy\"]\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    best_params = {\"rfe__n_features_to_select\": best_n_features}\n",
    "\n",
    "    ray.shutdown()\n",
    "\n",
    "    return best_params, best_accuracy, results_df, elapsed_time\n",
    "\n",
    "X = metasub_data.iloc[:,42:400]\n",
    "y= metasub_data['city']\n",
    "\n",
    "best_parameters, best_score, all_results, time_taken = parallel_rfe_feature_selection(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        n_jobs=-1,  # Use all available cores for RandomForest within each Ray task\n",
    "        random_state=123,\n",
    "        cv=5,\n",
    "        subsets=[50, 100, 200, 300, 500],\n",
    "        remove_correlated=True,\n",
    "        correlation_threshold=0.95,\n",
    "        num_cpus=50  # Limit Ray to 4 CPUs for this example\n",
    "    )\n",
    "\n",
    "print(f'\\nBest params: {best_parameters}')\n",
    "print(f'Best accuracy: {best_score:.6f}')\n",
    "print(f'Mean accuracy for all tested feature subsets:\\n{all_results}')\n",
    "print(f'Total time taken: {time_taken:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since I have lost the model, but I know that 200 features gives the best accuracy of 0.89. \n",
    "X = metasub_data.iloc[:,42:]\n",
    "y = metasub_data['city']\n",
    "model_200 = RandomForestClassifier(n_jobs=24,random_state=123)\n",
    "rfe = RFE(estimator=model_200,n_features_to_select=200,step=20)\n",
    "rfe.fit(X,y)\n",
    "\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(selected_features)\n",
    "\n",
    "print(len(rfe.support_))\n",
    "# All the accuracy results from the previous runs\n",
    "#results_df = pd.DataFrame(results,columns=['n_vars','accuracy'])\n",
    "#results_df.to_csv('mgps_git_taxa.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binp37_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
