{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metasub data mGPS algorithm - 31/03/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am just trying to get the data pre-processing steps right. The idea for doing this is to get the dataset in the right format for easier analysis using neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "os.chdir(\"/home/inf-21-2024/binp37/\")\n",
    "# Read the metadata for the metasub data.\n",
    "complete_meta = pd.read_csv(\"./data/metasub/complete_metadata.csv\")\n",
    "taxa_abund = pd.read_csv(\"./data/metasub/metasub_taxa_abundance.csv\")\n",
    "taxa_abund = taxa_abund.drop_duplicates(subset=['uuid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4288, 3711)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the bacterial and metadata\n",
    "metasub_data = pd.merge(complete_meta,taxa_abund,on='uuid')\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4157, 3711)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove control samples\n",
    "control_cities = {'control','other_control','neg_control','other','pos_control'}\n",
    "control_types = {'ctrl cities','negative_control','positive_control'}\n",
    "\n",
    "mask = metasub_data['city'].isin(control_cities) | metasub_data['control_type'].isin(control_types)\n",
    "metasub_data = metasub_data[~mask].copy()\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4157, 3711)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Re-label london boroughs\n",
    "metasub_data.loc[metasub_data['city'].isin(['kensington','islington']),'city'] = 'london'\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove sparse sample locations and doubtful samples\n",
    "city_counts = metasub_data['city'].value_counts()\n",
    "small_cities = city_counts[city_counts<8].index.tolist()\n",
    "remove_samples = metasub_data['city'].isin(['antarctica']+small_cities)\n",
    "metasub_data = metasub_data[~remove_samples]\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct the identified mislabeling of data\n",
    "kyiv_filter = metasub_data['city'] == 'kyiv'\n",
    "metasub_data.loc[kyiv_filter,'latitude'] = metasub_data.loc[kyiv_filter,'city_latitude'] # Set all the latitude to the city_latitude\n",
    "metasub_data.loc[kyiv_filter,'longitude'] = metasub_data.loc[kyiv_filter,'city_longitude'] # Set all the latitude to the city_longitutde\n",
    "\n",
    "porto_filter = metasub_data['city'] == 'porto'\n",
    "metasub_data.loc[porto_filter,'city'] = \"europe\"\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing latitude and longitude values with city-level data\n",
    "missing_lat = metasub_data[\"latitude\"].isna()\n",
    "missing_lon = metasub_data[\"longitude\"].isna()\n",
    "metasub_data.loc[missing_lat, \"latitude\"] = metasub_data.loc[missing_lat, \"city_latitude\"]\n",
    "metasub_data.loc[missing_lon, \"longitude\"] = metasub_data.loc[missing_lon, \"city_longitude\"]\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correction for incorrect London co-ordinates\n",
    "london_filter = metasub_data['city'] == 'london'\n",
    "metasub_data.loc[london_filter,'city_latitude'] = 51.50853\n",
    "metasub_data.loc[london_filter,'city_longitude'] = -0.12574\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uuid', 'metasub_name', 'core_project', 'project', 'city', 'city_code',\n",
       "       'latitude', 'longitude', 'surface_material', 'control_type',\n",
       "       'elevation', 'line', 'station', 'surface', 'temperature', 'traffic',\n",
       "       'setting', 'num_reads', 'library_post_PCR_Qubit',\n",
       "       'library_QC_concentration', 'city_latitude', 'city_longitude',\n",
       "       'coastal_city', 'city_total_population', 'city_population_density',\n",
       "       'city_land_area_km2', 'city_ave_june_temp_c', 'city_elevation_meters',\n",
       "       'continent', 'city_koppen_climate', 'barcode', 'ha_id',\n",
       "       'hudson_alpha_flowcell', 'hudson_alpha_project', 'index_sequence',\n",
       "       'location_type', 'hudson_alpha_uid', 'other_project_uid',\n",
       "       'plate_number', 'plate_pos', 'sample_type', 'sl_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metasub_data.iloc[:,:42].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination - 02/04/2025\n",
    "\n",
    "Here we use RFE as a part of a pipeline to get the best set of parameters for fitting the deep learning model. We need a suitable set of parameters for that are infromative enough to apply a deep learning model. These are called as geographically informative taxa (GITs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inf-21-2024/miniconda3/envs/ai_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-07 19:17:18,283\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import multiprocessing\n",
    "import time\n",
    "import ray\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "*Explanation of the Code*\n",
    "\n",
    "This function is designed to perform feature selection by:\n",
    "\n",
    "1. **Removing Highly Correlated Features:**\n",
    "   - Computes a correlation matrix of the features.\n",
    "   - Identifies features with correlation greater than 0.98 and removes them.\n",
    "\n",
    "2. **Recursive Feature Elimination (RFE) for Feature Selection:**\n",
    "   - Uses a Random Forest Classifier to determine feature importance.\n",
    "   - Applies RFE (Recursive Feature Elimination) to iteratively remove the least important features.\n",
    "   - The number of features to keep is determined based on predefined subset sizes.\n",
    "\n",
    "3. **Parallel Processing Support:**  \n",
    "   Uses multiple CPU cores when specified for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection algorithm\n",
    "def species_select(X,y,remove_correlated=True,subsets=None,cores=1):\n",
    "    \"\"\"\n",
    "    Feature selection algorithm for species classification.\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.DataFrame): Feature matrix\n",
    "    y (pd.Series): Target variable\n",
    "    remove_correlated (bool): Whether we need to remoce highly correlated variables. (default is set to True)\n",
    "    subsets (list): List of feature subset sizes to evaluate. If None, it is determined automatically.\n",
    "    cores (int): Number of CPU cores to use for parallel computation. (default is set to 1)\n",
    "    \n",
    "    Returns:\n",
    "    RFE object: Trained Recursive Feature Elimination (RFE) model.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()  # Track execution time\n",
    "\n",
    "    # Set parallel processing\n",
    "    num_cores = multiprocessing.cpu_count() if cores > 1 else 1\n",
    "    print(f\"Using {num_cores} CPU cores for computation.\")\n",
    "\n",
    "    if remove_correlated:\n",
    "        # Compute correlation matrix\n",
    "        print(\"Calculating correlation matrix...\")\n",
    "        corr_matrix = X.corr()\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "        # Identify correlated features (above 0.98)\n",
    "        correlated_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.98)]\n",
    "\n",
    "        # Drop correlated features\n",
    "        X = X.drop(columns=correlated_features)\n",
    "        print(f\"Correlated feature removed: {len(correlated_features)}\")\n",
    "\n",
    "    # Determine default subset sizes if not provided\n",
    "    num_features = X.shape[1]\n",
    "    if subsets is None:\n",
    "        subsets = [num_features // 2, num_features // 4, num_features // 8, num_features // 16, num_features // 32, num_features // 64]\n",
    "        subsets = [s for s in subsets  if s > 0] # Remove non-positive values\n",
    "\n",
    "    print(f\"Feature selection subsets: {subsets}\")\n",
    "\n",
    "    # Define model (Random Forrest for fearure ranking)\n",
    "    model = RandomForestClassifier(n_jobs=num_cores, random_state=123)\n",
    "    print(\"Initialized RandomForestClassifier.\")\n",
    "\n",
    "    # Recursive Feature Elimination (RFE)\n",
    "    for subset in subsets:\n",
    "        print(f\"\\nStarting RFE with {subset} features....\")\n",
    "        start_rfe = time.time()\n",
    "        rfe = RFE(estimator=model, n_features_to_select=min(subsets),step=20)\n",
    "        # Fit RFE to the data\n",
    "        rfe.fit(X,y)\n",
    "        print(f\"Completed RFE with {subset} features in {time.time() - start_rfe:.2f} seconds.\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nFeature selection completed in {total_time:.2f} seconds.\")\n",
    "    return rfe\n",
    "\n",
    "featureElim = species_select(X=metasub_data.iloc[:,42:500],\n",
    "                             y=metasub_data['city'],\n",
    "                             remove_correlated=False,\n",
    "                             subsets=[50,100,200,500],\n",
    "                             cores=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You Can Do If It's Still Too Slow\n",
    "\n",
    "1. **Reduce Feature Set Before Running RFE:**\n",
    "    - If remove_correlated=False, you still have 3711 features.\n",
    "    - Try using only top 500-1000 features based on variance or importance.\n",
    "\n",
    "2. **Increase Step Size in RFE:**\n",
    "    - Default step=1 removes one feature per iteration, which is slow.\n",
    "    - Try step=5 or step=10 to remove multiple features at once:\n",
    "    - rfe = RFE(estimatoe=model, n_features_to_select=subset, step=5)\n",
    "3. **Use XGBoost Instead of Random Forest:**\n",
    "    - model = XGBClassifier(n_jobs=num_cores, random_state=123)\n",
    "4. **Sequential Feature Selector:**\n",
    "    - from sklearn.feature_selection import SequentialFeatureSelector\n",
    "    sfs = SequentialFeatureSelector(model, n_features_to_select=100, direction='backward', n_jobs=num_cores)\n",
    "    sfs.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 16:49:03,058\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating correlation matrix...\n",
      "Correlated features removed: 34\n",
      "\n",
      "Starting RFE with subsets of features: [50, 100, 200, 300, 500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parallel RFE + Cross-validation:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[36m(evaluate_rfe_remote pid=3202616)\u001b[0m /home/inf-21-2024/miniconda3/envs/binp37_env/lib/python3.9/site-packages/sklearn/feature_selection/_rfe.py:300: UserWarning: Found n_features_to_select=500 > n_features=324. There will be no feature selection and all features will be kept.\n",
      "\u001b[36m(evaluate_rfe_remote pid=3202616)\u001b[0m   warnings.warn(\n",
      "Parallel RFE + Cross-validation: 100%|██████████| 25/25 [00:36<00:00,  1.46s/it]\n",
      "\u001b[36m(evaluate_rfe_remote pid=3202620)\u001b[0m /home/inf-21-2024/miniconda3/envs/binp37_env/lib/python3.9/site-packages/sklearn/feature_selection/_rfe.py:300: UserWarning: Found n_features_to_select=500 > n_features=324. There will be no feature selection and all features will be kept.\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(evaluate_rfe_remote pid=3202620)\u001b[0m   warnings.warn(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744037384.589459 3202071 chttp2_transport.cc:1182] ipv4:130.235.8.214:64042: Got goaway [2] err=UNAVAILABLE:GOAWAY received; Error code: 2; Debug Text: Cancelling all calls {grpc_status:14, http2_error:2, created_time:\"2025-04-07T16:49:44.589417313+02:00\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best params: {'rfe__n_features_to_select': 50}\n",
      "Best accuracy: 0.754545\n",
      "Mean accuracy for all tested feature subsets:\n",
      "   n_features  accuracy\n",
      "0          50  0.754545\n",
      "1         100  0.740786\n",
      "2         200  0.740295\n",
      "3         300  0.734889\n",
      "4         500  0.735135\n",
      "Total time taken: 37.16 seconds\n"
     ]
    }
   ],
   "source": [
    "def parallel_rfe_feature_selection(X: pd.DataFrame, y: pd.Series, n_jobs: int = 1, random_state: int = 123,\n",
    "                                   cv: int = 10, subsets: list = None, remove_correlated: bool = True,\n",
    "                                   correlation_threshold: float = 0.98, num_cpus: int = None):\n",
    "    \"\"\"\n",
    "    Performs parallel Recursive Feature Elimination (RFE) with cross-validation to select the best feature subset.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame of features.\n",
    "        y (pd.Series): Series of the target variable.\n",
    "        n_jobs (int): Number of jobs for the base estimator (RandomForestClassifier).\n",
    "        random_state (int): Random state for reproducibility.\n",
    "        cv (int): Number of cross-validation folds.\n",
    "        subsets (list, optional): List of feature subset sizes to evaluate. If None, default subsets are used. Defaults to None.\n",
    "        remove_correlated (bool, optional): Whether to remove highly correlated features before RFE. Defaults to True.\n",
    "        correlation_threshold (float, optional): Threshold for identifying highly correlated features. Defaults to 0.98.\n",
    "        num_cpus (int, optional): Number of CPUs to use for Ray. If None, Ray will auto-detect. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - best_params (dict): Dictionary with the best RFE parameters.\n",
    "            - best_accuracy (float): The best mean cross-validation accuracy achieved.\n",
    "            - results_df (pd.DataFrame): DataFrame containing the mean accuracy for each feature subset size.\n",
    "            - elapsed_time (float): Total time taken for the feature selection process.\n",
    "    \"\"\"\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "    ray.init(ignore_reinit_error=True, num_cpus=num_cpus)\n",
    "\n",
    "    model = RandomForestClassifier(n_jobs=n_jobs, random_state=random_state)\n",
    "\n",
    "    if remove_correlated:\n",
    "        # Compute correlation matrix\n",
    "        print(\"Calculating correlation matrix...\")\n",
    "        corr_matrix = X.corr()\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "        # Identify correlated features (above threshold)\n",
    "        correlated_features = [column for column in upper_tri.columns if any(upper_tri[column] > correlation_threshold)]\n",
    "\n",
    "        # Drop correlated features\n",
    "        X = X.drop(columns=correlated_features)\n",
    "        print(f\"Correlated features removed: {len(correlated_features)}\")\n",
    "\n",
    "    # Determine default subset sizes if not provided\n",
    "    num_features = X.shape[1]\n",
    "    if subsets is None:\n",
    "        subsets = [num_features // 2, num_features // 4, num_features // 8, num_features // 16, num_features // 32, num_features // 64]\n",
    "        subsets = [s for s in subsets if s > 0]  # Remove non-positive values\n",
    "\n",
    "    n_features_options = sorted(list(set(subsets))) # Ensure unique and sorted subset sizes\n",
    "    total_iterations = len(n_features_options) * cv\n",
    "\n",
    "    print(f\"\\nStarting RFE with subsets of features: {n_features_options}\")\n",
    "\n",
    "    # Define remote function for parallel execution\n",
    "    @ray.remote\n",
    "    def evaluate_rfe_remote(n_features, fold, X_remote, y_remote):\n",
    "        \"\"\"Performs RFE feature selection and evaluates performance for a given fold.\"\"\"\n",
    "        pipe = make_pipeline(RFE(estimator=model, n_features_to_select=n_features, step=10))\n",
    "\n",
    "        # We use the stratified K fold to split the data into training and validation sets\n",
    "        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=fold)\n",
    "        train_index, test_index = list(skf.split(X_remote, y_remote))[fold]\n",
    "\n",
    "        # train_index and test_index contain the index values for extracting training and testing data\n",
    "        X_train = X_remote.iloc[train_index, :]\n",
    "        X_test = X_remote.iloc[test_index, :]\n",
    "        y_train = y_remote.iloc[train_index]\n",
    "        y_test = y_remote.iloc[test_index]\n",
    "\n",
    "        # Fit the model using the training data and then evaluate the score based on the testing data\n",
    "        pipe.fit(X_train, y_train)\n",
    "        score = pipe.score(X_test, y_test)\n",
    "\n",
    "        return n_features, fold, score\n",
    "\n",
    "    start_time = time.time()\n",
    "    X_ray = ray.put(X)\n",
    "    y_ray = ray.put(y)\n",
    "    tasks = [evaluate_rfe_remote.remote(n_features, fold, X_ray, y_ray)\n",
    "             for n_features in n_features_options for fold in range(cv)]\n",
    "\n",
    "    results = []\n",
    "    with tqdm(total=total_iterations, desc='Parallel RFE + Cross-validation') as pbar:\n",
    "        while tasks:\n",
    "            done, tasks = ray.wait(tasks, num_returns=1)\n",
    "            result = ray.get(done[0])\n",
    "            results.append((result[0], result[2]))  # (n_features, score)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Aggregate mean accuracy for each feature subset\n",
    "    results_df = pd.DataFrame(results, columns=[\"n_features\", \"accuracy\"])\n",
    "    results_df = results_df.groupby(\"n_features\").mean().reset_index()\n",
    "\n",
    "    # Find best feature subset\n",
    "    best_row = results_df.loc[results_df[\"accuracy\"].idxmax()]\n",
    "    best_n_features = int(best_row[\"n_features\"])\n",
    "    best_accuracy = best_row[\"accuracy\"]\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    best_params = {\"rfe__n_features_to_select\": best_n_features}\n",
    "\n",
    "    ray.shutdown()\n",
    "\n",
    "    return best_params, best_accuracy, results_df, elapsed_time\n",
    "\n",
    "X = metasub_data.iloc[:,42:400]\n",
    "y= metasub_data['city']\n",
    "\n",
    "best_parameters, best_score, all_results, time_taken = parallel_rfe_feature_selection(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        n_jobs=-1,  # Use all available cores for RandomForest within each Ray task\n",
    "        random_state=123,\n",
    "        cv=5,\n",
    "        subsets=[50, 100, 200, 300, 500],\n",
    "        remove_correlated=True,\n",
    "        correlation_threshold=0.95,\n",
    "        num_cpus=50  # Limit Ray to 4 CPUs for this example\n",
    "    )\n",
    "\n",
    "print(f'\\nBest params: {best_parameters}')\n",
    "print(f'Best accuracy: {best_score:.6f}')\n",
    "print(f'Mean accuracy for all tested feature subsets:\\n{all_results}')\n",
    "print(f'Total time taken: {time_taken:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Acidovorax ebreus', 'Acidovorax sp. JS42', 'Acidovorax sp. KKS102',\n",
      "       'Acinetobacter baumannii', 'Acinetobacter haemolyticus',\n",
      "       'Acinetobacter johnsonii', 'Acinetobacter junii',\n",
      "       'Acinetobacter pittii', 'Acinetobacter schindleri',\n",
      "       'Acinetobacter sp. LoGeW2-3',\n",
      "       ...\n",
      "       'Thermothelomyces thermophila', 'Thielavia terrestris',\n",
      "       'Truepera radiovictrix', 'Tsukamurella sp. MH1',\n",
      "       'Variovorax boronicumulans', 'Variovorax paradoxus',\n",
      "       'Variovorax sp. PAMC 28711', 'Veillonella parvula', 'Weissella cibaria',\n",
      "       'Xanthomonas campestris'],\n",
      "      dtype='object', length=200)\n",
      "3669\n"
     ]
    }
   ],
   "source": [
    "# Since I have lost the model, but I know that 200 features gives the best accuracy of 0.89. \n",
    "X = metasub_data.iloc[:,42:]\n",
    "y = metasub_data['city']\n",
    "model_200 = RandomForestClassifier(n_jobs=24,random_state=123)\n",
    "rfe = RFE(estimator=model_200,n_features_to_select=200,step=20)\n",
    "rfe.fit(X,y)\n",
    "\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(selected_features)\n",
    "\n",
    "print(len(rfe.support_))\n",
    "# All the accuracy results from the previous runs\n",
    "#results_df = pd.DataFrame(results,columns=['n_vars','accuracy'])\n",
    "#results_df.to_csv('mgps_git_taxa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Neural Networks Tests - 03/04/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acidovorax ebreus</th>\n",
       "      <th>Acidovorax sp. JS42</th>\n",
       "      <th>Acidovorax sp. KKS102</th>\n",
       "      <th>Acinetobacter baumannii</th>\n",
       "      <th>Acinetobacter haemolyticus</th>\n",
       "      <th>Acinetobacter johnsonii</th>\n",
       "      <th>Acinetobacter junii</th>\n",
       "      <th>Acinetobacter pittii</th>\n",
       "      <th>Acinetobacter schindleri</th>\n",
       "      <th>Acinetobacter sp. LoGeW2-3</th>\n",
       "      <th>...</th>\n",
       "      <th>Variovorax boronicumulans</th>\n",
       "      <th>Variovorax paradoxus</th>\n",
       "      <th>Variovorax sp. PAMC 28711</th>\n",
       "      <th>Veillonella parvula</th>\n",
       "      <th>Weissella cibaria</th>\n",
       "      <th>Xanthomonas campestris</th>\n",
       "      <th>continent</th>\n",
       "      <th>city</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00480</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00028</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>0.00142</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00091</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00181</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.00274</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>0.00191</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00208</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00397</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Acidovorax ebreus  Acidovorax sp. JS42  Acidovorax sp. KKS102  \\\n",
       "0            0.00000              0.00000                0.00023   \n",
       "1            0.00000              0.00001                0.00003   \n",
       "2            0.00003              0.00000                0.00011   \n",
       "3            0.00000              0.00000                0.00000   \n",
       "4            0.00000              0.00000                0.00000   \n",
       "\n",
       "   Acinetobacter baumannii  Acinetobacter haemolyticus  \\\n",
       "0                  0.00015                     0.00000   \n",
       "1                  0.00028                     0.00016   \n",
       "2                  0.00181                     0.00060   \n",
       "3                  0.00002                     0.00001   \n",
       "4                  0.00003                     0.00000   \n",
       "\n",
       "   Acinetobacter johnsonii  Acinetobacter junii  Acinetobacter pittii  \\\n",
       "0                  0.00006              0.00001               0.00007   \n",
       "1                  0.00142              0.00017               0.00013   \n",
       "2                  0.00274              0.00030               0.00110   \n",
       "3                  0.00003              0.00000               0.00000   \n",
       "4                  0.00000              0.00000               0.00002   \n",
       "\n",
       "   Acinetobacter schindleri  Acinetobacter sp. LoGeW2-3  ...  \\\n",
       "0                   0.00010                     0.00005  ...   \n",
       "1                   0.00262                     0.00140  ...   \n",
       "2                   0.00191                     0.00132  ...   \n",
       "3                   0.00003                     0.00001  ...   \n",
       "4                   0.00009                     0.00001  ...   \n",
       "\n",
       "   Variovorax boronicumulans  Variovorax paradoxus  Variovorax sp. PAMC 28711  \\\n",
       "0                    0.00031               0.00075                    0.00021   \n",
       "1                    0.00013               0.00024                    0.00003   \n",
       "2                    0.00010               0.00025                    0.00001   \n",
       "3                    0.00003               0.00002                    0.00000   \n",
       "4                    0.00004               0.00008                    0.00003   \n",
       "\n",
       "   Veillonella parvula  Weissella cibaria  Xanthomonas campestris  continent  \\\n",
       "0                  0.0                0.0                 0.00480    oceania   \n",
       "1                  0.0                0.0                 0.00091    oceania   \n",
       "2                  0.0                0.0                 0.00208    oceania   \n",
       "3                  0.0                0.0                 0.00137    oceania   \n",
       "4                  0.0                0.0                 0.00397    oceania   \n",
       "\n",
       "       city  latitude  longitude  \n",
       "0  hamilton -37.78333  175.28333  \n",
       "1  hamilton -37.78333  175.28333  \n",
       "2  hamilton -37.78333  175.28333  \n",
       "3  hamilton -37.78333  175.28333  \n",
       "4  hamilton -37.78333  175.28333  \n",
       "\n",
       "[5 rows x 204 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_data = pd.concat([metasub_data[selected_features],metasub_data[['continent','city','latitude','longitude']]],axis=1)\n",
    "nn_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the city and continent names into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique cities in the metasub dataset are 40.\n",
      "The unique continents in the metasub dataset are ['oceania', 'south_america', 'east_asia', 'sub_saharan_africa', 'middle_east', 'north_america', 'europe']\n"
     ]
    }
   ],
   "source": [
    "print(f\"The unique cities in the metasub dataset are {len(list(nn_data['city'].unique()))}.\")\n",
    "print(f\"The unique continents in the metasub dataset are {list(nn_data['continent'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acidovorax ebreus</th>\n",
       "      <th>Acidovorax sp. JS42</th>\n",
       "      <th>Acidovorax sp. KKS102</th>\n",
       "      <th>Acinetobacter baumannii</th>\n",
       "      <th>Acinetobacter haemolyticus</th>\n",
       "      <th>Acinetobacter johnsonii</th>\n",
       "      <th>Acinetobacter junii</th>\n",
       "      <th>Acinetobacter pittii</th>\n",
       "      <th>Acinetobacter schindleri</th>\n",
       "      <th>Acinetobacter sp. LoGeW2-3</th>\n",
       "      <th>...</th>\n",
       "      <th>Variovorax boronicumulans</th>\n",
       "      <th>Variovorax paradoxus</th>\n",
       "      <th>Variovorax sp. PAMC 28711</th>\n",
       "      <th>Veillonella parvula</th>\n",
       "      <th>Weissella cibaria</th>\n",
       "      <th>Xanthomonas campestris</th>\n",
       "      <th>city_encoding</th>\n",
       "      <th>continent_encoding</th>\n",
       "      <th>lat_scaled</th>\n",
       "      <th>long_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00480</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00028</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>0.00142</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00091</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00181</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.00274</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>0.00191</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00208</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00397</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Acidovorax ebreus  Acidovorax sp. JS42  Acidovorax sp. KKS102  \\\n",
       "0            0.00000              0.00000                0.00023   \n",
       "1            0.00000              0.00001                0.00003   \n",
       "2            0.00003              0.00000                0.00011   \n",
       "3            0.00000              0.00000                0.00000   \n",
       "4            0.00000              0.00000                0.00000   \n",
       "\n",
       "   Acinetobacter baumannii  Acinetobacter haemolyticus  \\\n",
       "0                  0.00015                     0.00000   \n",
       "1                  0.00028                     0.00016   \n",
       "2                  0.00181                     0.00060   \n",
       "3                  0.00002                     0.00001   \n",
       "4                  0.00003                     0.00000   \n",
       "\n",
       "   Acinetobacter johnsonii  Acinetobacter junii  Acinetobacter pittii  \\\n",
       "0                  0.00006              0.00001               0.00007   \n",
       "1                  0.00142              0.00017               0.00013   \n",
       "2                  0.00274              0.00030               0.00110   \n",
       "3                  0.00003              0.00000               0.00000   \n",
       "4                  0.00000              0.00000               0.00002   \n",
       "\n",
       "   Acinetobacter schindleri  Acinetobacter sp. LoGeW2-3  ...  \\\n",
       "0                   0.00010                     0.00005  ...   \n",
       "1                   0.00262                     0.00140  ...   \n",
       "2                   0.00191                     0.00132  ...   \n",
       "3                   0.00003                     0.00001  ...   \n",
       "4                   0.00009                     0.00001  ...   \n",
       "\n",
       "   Variovorax boronicumulans  Variovorax paradoxus  Variovorax sp. PAMC 28711  \\\n",
       "0                    0.00031               0.00075                    0.00021   \n",
       "1                    0.00013               0.00024                    0.00003   \n",
       "2                    0.00010               0.00025                    0.00001   \n",
       "3                    0.00003               0.00002                    0.00000   \n",
       "4                    0.00004               0.00008                    0.00003   \n",
       "\n",
       "   Veillonella parvula  Weissella cibaria  Xanthomonas campestris  \\\n",
       "0                  0.0                0.0                 0.00480   \n",
       "1                  0.0                0.0                 0.00091   \n",
       "2                  0.0                0.0                 0.00208   \n",
       "3                  0.0                0.0                 0.00137   \n",
       "4                  0.0                0.0                 0.00397   \n",
       "\n",
       "   city_encoding  continent_encoding  lat_scaled  long_scaled  \n",
       "0             10                   4   -3.548641     1.899948  \n",
       "1             10                   4   -3.548641     1.899948  \n",
       "2             10                   4   -3.548641     1.899948  \n",
       "3             10                   4   -3.548641     1.899948  \n",
       "4             10                   4   -3.548641     1.899948  \n",
       "\n",
       "[5 rows x 204 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize label and scalers\n",
    "le = LabelEncoder()\n",
    "stdscaler = StandardScaler() # I can try MinMaxScaler as well\n",
    "# Convert all the categorical variables into numbers\n",
    "nn_data['city_encoding'] = nn_data[['city']].apply(le.fit_transform)\n",
    "nn_data['continent_encoding'] = nn_data[['continent']].apply(le.fit_transform)\n",
    "nn_data['lat_scaled'] = stdscaler.fit_transform(nn_data[['latitude']])\n",
    "nn_data['long_scaled'] = stdscaler.fit_transform(nn_data[['longitude']])\n",
    "# Store all the new scaled and encoded data in a new dataframe\n",
    "encoded_nn_data = nn_data.drop(columns=['city','continent','latitude','longitude'],axis=1)\n",
    "encoded_nn_data.to_csv('./results/metasub_training_testing_data.csv',index=False)\n",
    "encoded_nn_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3256, 200)\n",
      "(3256, 4)\n",
      "(814, 200)\n",
      "(814, 4)\n"
     ]
    }
   ],
   "source": [
    "# KFold - Shuffle=True, I will do this later. For now, I will just use the train_test_split.\n",
    "kf = KFold(n_splits=5,shuffle=True, random_state=123)\n",
    "\n",
    "X = encoded_nn_data.iloc[:,:200].values\n",
    "y = encoded_nn_data[['continent_encoding','city_encoding','lat_scaled','long_scaled']].values \n",
    "\n",
    "for train_idx, val_idx in kf.split(X,y[:,1]): # We will use only the city column to create the split. Based on the ordering of the columns in the previous cell.\n",
    "    X_train = pd.DataFrame(X[train_idx])\n",
    "    y_train = pd.DataFrame(y[train_idx])\n",
    "\n",
    "    X_test = pd.DataFrame(X[val_idx])\n",
    "    y_test = pd.DataFrame(y[val_idx])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_nn_data.iloc[:,:200].values,\n",
    "                                                    encoded_nn_data[['continent_encoding','city_encoding','lat_scaled','long_scaled']].values,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustDat(torch.utils.data.Dataset):\n",
    "    def __init__(self,df,target):\n",
    "        self.df = df.astype(np.float32)\n",
    "        self.target = target\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        dp = self.df[idx]\n",
    "        targ = self.target[idx]\n",
    "        dp = torch.from_numpy(dp)\n",
    "        continent_city = torch.tensor(targ[0:2],dtype=torch.long)\n",
    "        lat_lon = torch.tensor(targ[2:],dtype=torch.float32)\n",
    "        targ_combined = torch.cat((continent_city,lat_lon)) \n",
    "        return dp,continent_city, lat_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0, 12]), tensor([-0.5329,  1.1160]))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CustDat(X_train,y_train).__getitem__(23)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(CustDat(X_train,y_train),\n",
    "                                       batch_size=64,shuffle=True,num_workers=4,pin_memory=False)\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(CustDat(X_test,y_test),\n",
    "                                       batch_size=64,shuffle=True,num_workers=4,pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Model - 07/04/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the class for the netural network continent, citites and latitude and longitutude\n",
    "# This is one architechture that I have  created a sequential architecture where the prediction of each level (continent -> city -> latitude -> longitude) depends on the output of the previous level. \n",
    "# The initial features are progressively enriched with the predictions from the preceding models to make the subsequent predictions. \n",
    "# Each individual model is a feedforward neural network with a consistent structure of three hidden layers with ReLU activations. \n",
    "# The output layer's size and the choice of activation (or lack thereof) depend on whether the task is classification (continent, city) or regression (latitude, longitude).\n",
    "\n",
    "class NeuralNetContinent(nn.Module):\n",
    "    def __init__(self, input_size_continents, num_continents):\n",
    "        # Initializes the NeuralNetContinent class, inheriting from nn.Module (PyTorch's base class for neural network modules).\n",
    "        super(NeuralNetContinent, self).__init__()\n",
    "        # Defines the first linear layer: input size is 'input_size_continents' (assumed to be 200, representing the number of initial features),\n",
    "        # and the output size is 400 (the number of neurons in this layer). This layer performs a linear transformation on the input data.\n",
    "        self.layer1 = nn.Linear(input_size_continents, 400) # 200 GITs\n",
    "        # Defines the second linear layer: input size is 400 (output of the previous layer), and the output size is also 400.\n",
    "        self.layer2 = nn.Linear(400, 400)\n",
    "        # Defines the third linear layer: input size is 400, and the output size is 200.\n",
    "        self.layer3 = nn.Linear(400, 200)\n",
    "        # Defines the final linear layer: input size is 200, and the output size is 'num_continents' (assumed to be 7, representing the number of continent classes).\n",
    "        # The output of this layer will represent the raw scores (logits) for each continent.\n",
    "        self.layer4 = nn.Linear(200, num_continents) # 7 continents\n",
    "        # Initializes the ReLU (Rectified Linear Unit) activation function. ReLU introduces non-linearity to the network, allowing it to learn complex patterns.\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Defines the forward pass of the neural network. 'x' is the input tensor.\n",
    "\n",
    "        # The input 'x' is passed through the first linear layer, and then the ReLU activation function is applied element-wise to the output.\n",
    "        out = self.relu(self.layer1(x))\n",
    "        # The output of the first activation is passed through the second linear layer, followed by ReLU.\n",
    "        out = self.relu(self.layer2(out))\n",
    "        # The output of the second activation is passed through the third linear layer, followed by ReLU.\n",
    "        out = self.relu(self.layer3(out))\n",
    "        # The output of the third activation is passed through the final linear layer. No activation function is typically applied here for classification tasks,\n",
    "        # as the raw scores (logits) are used by the CrossEntropyLoss.\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        # Returns the output tensor, which contains the raw scores for each continent class.\n",
    "        return out\n",
    "\n",
    "class NeuralNetCities(nn.Module):\n",
    "    def __init__(self, input_size_cities, num_cities):\n",
    "        # Initializes the NeuralNetCities class, inheriting from nn.Module.\n",
    "        super(NeuralNetCities, self).__init__()\n",
    "        # Defines the first linear layer: input size is 'input_size_cities' (assumed to be 207, including initial features and continent probabilities),\n",
    "        # and the output size is 400.\n",
    "        self.layer1 = nn.Linear(input_size_cities, 400) # 207 (continent probabilities also included)\n",
    "        # Defines the second linear layer: input size is 400, and the output size is also 400.\n",
    "        self.layer2 = nn.Linear(400, 400)\n",
    "        # Defines the third linear layer: input size is 400, and the output size is 200.\n",
    "        self.layer3 = nn.Linear(400, 200)\n",
    "        # Defines the final linear layer: input size is 200, and the output size is 'num_cities' (assumed to be 40, representing the number of city classes).\n",
    "        # The output of this layer will represent the raw scores (logits) for each city.\n",
    "        self.layer4 = nn.Linear(200, num_cities) # 40 continents\n",
    "        # Initializes the ReLU activation function.\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Defines the forward pass of the cities neural network. 'x' is the input tensor.\n",
    "\n",
    "        # The input 'x' is passed through the first linear layer, followed by ReLU.\n",
    "        out = self.relu(self.layer1(x))\n",
    "        # The output of the first activation is passed through the second linear layer, followed by ReLU.\n",
    "        out = self.relu(self.layer2(out))\n",
    "        # The output of the second activation is passed through the third linear layer, followed by ReLU.\n",
    "        out = self.relu(self.layer3(out))\n",
    "        # The output of the third activation is passed through the final linear layer (no activation for logits).\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        # Returns the output tensor, containing raw scores for each city class.\n",
    "        return out\n",
    "\n",
    "class NeuralNetLat(nn.Module):\n",
    "    def __init__(self, input_size_lat, lat_size):\n",
    "        # Initializes the NeuralNetLat class for latitude prediction, inheriting from nn.Module.\n",
    "        super(NeuralNetLat, self).__init__()\n",
    "        # Defines the first linear layer: input size is 'input_size_lat' (assumed to be 247, including initial features, continent probabilities, and city probabilities),\n",
    "        # and the output size is 400.\n",
    "        self.layer1 = nn.Linear(input_size_lat, 400) # 247 input shape (continent and cities probabilities also included)\n",
    "        # Defines the second linear layer: input size is 400, and the output size is also 400.\n",
    "        self.layer2 = nn.Linear(400, 400)\n",
    "        # Defines the third linear layer: input size is 400, and the output size is 200.\n",
    "        self.layer3 = nn.Linear(400, 200)\n",
    "        # Defines the final linear layer: input size is 200, and the output size is 'lat_size' (set to 1, for predicting a single latitude value).\n",
    "        # For regression tasks like predicting a single numerical value, no activation function is typically applied to the final layer.\n",
    "        self.layer4 = nn.Linear(200, lat_size)\n",
    "        # Initializes the ReLU activation function for the hidden layers.\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Defines the forward pass for the latitude prediction network.\n",
    "\n",
    "        # Input 'x' through the first linear layer and ReLU.\n",
    "        out = self.relu(self.layer1(x))\n",
    "        # Output of the first activation through the second linear layer and ReLU.\n",
    "        out = self.relu(self.layer2(out))\n",
    "        # Output of the second activation through the third linear layer and ReLU.\n",
    "        out = self.relu(self.layer3(out))\n",
    "        # Output of the third activation through the final linear layer (no activation for regression output).\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        # Returns the predicted latitude value.\n",
    "        return out\n",
    "\n",
    "class NeuralNetLong(nn.Module):\n",
    "    def __init__(self, input_size_long, long_size):\n",
    "        # Initializes the NeuralNetLong class for longitude prediction, inheriting from nn.Module.\n",
    "        super(NeuralNetLong, self).__init__()\n",
    "        # Defines the first linear layer: input size is 'input_size_long' (assumed to be 248, including initial features, continent probabilities, city probabilities, and the latitude value),\n",
    "        # and the output size is 400.\n",
    "        self.layer1 = nn.Linear(input_size_long, 400) # 248 input shape (continent, cities probabilites and the latitude values as well)\n",
    "        # Defines the second linear layer: input size is 400, and the output size is also 400.\n",
    "        self.layer2 = nn.Linear(400, 400)\n",
    "        # Defines the third linear layer: input size is 400, and the output size is 200.\n",
    "        self.layer3 = nn.Linear(400, 200)\n",
    "        # Defines the final linear layer: input size is 200, and the output size is 'long_size' (set to 1, for predicting a single longitude value).\n",
    "        # Similar to latitude, no activation is typically used on the final layer for regression.\n",
    "        self.layer4 = nn.Linear(200, long_size)\n",
    "        # Initializes the ReLU activation function for the hidden layers.\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Defines the forward pass for the longitude prediction network.\n",
    "\n",
    "        # Input 'x' through the first linear layer and ReLU.\n",
    "        out = self.relu(self.layer1(x))\n",
    "        # Output of the first activation through the second linear layer and ReLU.\n",
    "        out = self.relu(self.layer2(out))\n",
    "        # Output of the second activation through the third linear layer and ReLU.\n",
    "        out = self.relu(self.layer3(out))\n",
    "        # Output of the third activation through the final linear layer (no activation for regression output).\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        # Returns the predicted longitude value.\n",
    "        return out\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "# Defines the size of the input features for the continent prediction model.\n",
    "input_size_continents = 200\n",
    "# Defines the size of the input features for the city prediction model (original features + continent probabilities).\n",
    "input_size_cities = 207\n",
    "# Defines the size of the input features for the latitude prediction model (original features + continent probabilities + city probabilities).\n",
    "input_size_lat = 247\n",
    "# Defines the size of the input features for the longitude prediction model (original features + continent probabilities + city probabilities + latitude).\n",
    "input_size_long = 248\n",
    "\n",
    "# Defines the number of continent classes.\n",
    "num_continents = 7\n",
    "# Defines the number of city classes.\n",
    "num_cities = 40\n",
    "# Defines the output size for latitude prediction (a single value).\n",
    "lat_size = 1\n",
    "# Defines the output size for longitude prediction (a single value).\n",
    "long_size = 1\n",
    "\n",
    "# Defines the learning rate for the optimizers, controlling the step size during weight updates.\n",
    "learning_rate = 0.001\n",
    "# Defines the batch size for training, the number of samples processed in one iteration.\n",
    "batch_size = 64\n",
    "# Defines the number of training epochs, the number of times the entire training dataset is passed through the network.\n",
    "num_epochs = 20\n",
    "\n",
    "# Initialize the neural network models and move them to the specified device (CPU or GPU).\n",
    "nn_continent_model = NeuralNetContinent(input_size_continents=input_size_continents, num_continents=num_continents).to(device)\n",
    "nn_cities_model = NeuralNetCities(input_size_cities=input_size_cities, num_cities=num_cities).to(device)\n",
    "nn_lat_model = NeuralNetLat(input_size_lat=input_size_lat, lat_size=lat_size).to(device)\n",
    "nn_long_model = NeuralNetLong(input_size_long=input_size_long, long_size=long_size).to(device)\n",
    "\n",
    "\n",
    "# Define the loss functions. CrossEntropyLoss is used for multi-class classification (continents and cities),\n",
    "# and MSELoss (Mean Squared Error Loss) is used for regression (latitude and longitude).\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_lat_lon = nn.MSELoss()\n",
    "# Define the optimizers for each model. Adam is used for continent and city classification,\n",
    "# and SGD (Stochastic Gradient Descent) is used for latitude and longitude regression.\n",
    "optimizer_continent = torch.optim.Adam(nn_continent_model.parameters(), lr=learning_rate)\n",
    "optimizer_cities = torch.optim.Adam(nn_cities_model.parameters(), lr=learning_rate)\n",
    "optimizer_lat = torch.optim.SGD(nn_lat_model.parameters(), lr=learning_rate)\n",
    "optimizer_long = torch.optim.SGD(nn_long_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20, Loss Continents: 1.5316, Epoch Time: 5.69 seconds\n",
      "Epoch 1/20, Loss Cities: 3.1838, Epoch Time: 5.69 seconds\n",
      "Epoch 1/20, Loss Latitudes: 1.3016, Epoch Time: 5.69 seconds\n",
      "Epoch 1/20, Loss Longitudes: 1.0050, Epoch Time: 5.69 seconds\n",
      "\n",
      "Epoch 2/20, Loss Continents: 1.1433, Epoch Time: 5.82 seconds\n",
      "Epoch 2/20, Loss Cities: 2.9700, Epoch Time: 5.82 seconds\n",
      "Epoch 2/20, Loss Latitudes: 0.8933, Epoch Time: 5.82 seconds\n",
      "Epoch 2/20, Loss Longitudes: 1.2439, Epoch Time: 5.82 seconds\n",
      "\n",
      "Epoch 3/20, Loss Continents: 1.0886, Epoch Time: 5.66 seconds\n",
      "Epoch 3/20, Loss Cities: 2.3779, Epoch Time: 5.66 seconds\n",
      "Epoch 3/20, Loss Latitudes: 1.1289, Epoch Time: 5.66 seconds\n",
      "Epoch 3/20, Loss Longitudes: 1.0153, Epoch Time: 5.66 seconds\n",
      "\n",
      "Epoch 4/20, Loss Continents: 0.9880, Epoch Time: 5.61 seconds\n",
      "Epoch 4/20, Loss Cities: 2.1531, Epoch Time: 5.61 seconds\n",
      "Epoch 4/20, Loss Latitudes: 1.1696, Epoch Time: 5.61 seconds\n",
      "Epoch 4/20, Loss Longitudes: 0.6440, Epoch Time: 5.61 seconds\n",
      "\n",
      "Epoch 5/20, Loss Continents: 1.0436, Epoch Time: 5.32 seconds\n",
      "Epoch 5/20, Loss Cities: 2.1030, Epoch Time: 5.32 seconds\n",
      "Epoch 5/20, Loss Latitudes: 1.0413, Epoch Time: 5.32 seconds\n",
      "Epoch 5/20, Loss Longitudes: 1.1242, Epoch Time: 5.32 seconds\n",
      "\n",
      "Epoch 6/20, Loss Continents: 0.6179, Epoch Time: 5.58 seconds\n",
      "Epoch 6/20, Loss Cities: 1.4719, Epoch Time: 5.58 seconds\n",
      "Epoch 6/20, Loss Latitudes: 0.6530, Epoch Time: 5.58 seconds\n",
      "Epoch 6/20, Loss Longitudes: 0.8672, Epoch Time: 5.58 seconds\n",
      "\n",
      "Epoch 7/20, Loss Continents: 0.6155, Epoch Time: 5.33 seconds\n",
      "Epoch 7/20, Loss Cities: 1.8081, Epoch Time: 5.33 seconds\n",
      "Epoch 7/20, Loss Latitudes: 0.6853, Epoch Time: 5.33 seconds\n",
      "Epoch 7/20, Loss Longitudes: 0.5702, Epoch Time: 5.33 seconds\n",
      "\n",
      "Epoch 8/20, Loss Continents: 0.6814, Epoch Time: 5.48 seconds\n",
      "Epoch 8/20, Loss Cities: 1.8022, Epoch Time: 5.48 seconds\n",
      "Epoch 8/20, Loss Latitudes: 0.5845, Epoch Time: 5.48 seconds\n",
      "Epoch 8/20, Loss Longitudes: 0.8364, Epoch Time: 5.48 seconds\n",
      "\n",
      "Epoch 9/20, Loss Continents: 0.7378, Epoch Time: 5.28 seconds\n",
      "Epoch 9/20, Loss Cities: 1.6525, Epoch Time: 5.28 seconds\n",
      "Epoch 9/20, Loss Latitudes: 1.1694, Epoch Time: 5.28 seconds\n",
      "Epoch 9/20, Loss Longitudes: 0.6039, Epoch Time: 5.28 seconds\n",
      "\n",
      "Epoch 10/20, Loss Continents: 0.5709, Epoch Time: 5.70 seconds\n",
      "Epoch 10/20, Loss Cities: 1.2945, Epoch Time: 5.70 seconds\n",
      "Epoch 10/20, Loss Latitudes: 0.6691, Epoch Time: 5.70 seconds\n",
      "Epoch 10/20, Loss Longitudes: 0.6796, Epoch Time: 5.70 seconds\n",
      "\n",
      "Epoch 11/20, Loss Continents: 0.6950, Epoch Time: 5.50 seconds\n",
      "Epoch 11/20, Loss Cities: 2.0745, Epoch Time: 5.50 seconds\n",
      "Epoch 11/20, Loss Latitudes: 0.7417, Epoch Time: 5.50 seconds\n",
      "Epoch 11/20, Loss Longitudes: 0.5819, Epoch Time: 5.50 seconds\n",
      "\n",
      "Epoch 12/20, Loss Continents: 0.5658, Epoch Time: 5.32 seconds\n",
      "Epoch 12/20, Loss Cities: 1.5122, Epoch Time: 5.32 seconds\n",
      "Epoch 12/20, Loss Latitudes: 0.9235, Epoch Time: 5.32 seconds\n",
      "Epoch 12/20, Loss Longitudes: 0.8291, Epoch Time: 5.32 seconds\n",
      "\n",
      "Epoch 13/20, Loss Continents: 0.6644, Epoch Time: 5.37 seconds\n",
      "Epoch 13/20, Loss Cities: 1.4686, Epoch Time: 5.37 seconds\n",
      "Epoch 13/20, Loss Latitudes: 0.7446, Epoch Time: 5.37 seconds\n",
      "Epoch 13/20, Loss Longitudes: 0.6647, Epoch Time: 5.37 seconds\n",
      "\n",
      "Epoch 14/20, Loss Continents: 0.6073, Epoch Time: 5.82 seconds\n",
      "Epoch 14/20, Loss Cities: 1.2666, Epoch Time: 5.82 seconds\n",
      "Epoch 14/20, Loss Latitudes: 0.6480, Epoch Time: 5.82 seconds\n",
      "Epoch 14/20, Loss Longitudes: 0.5117, Epoch Time: 5.82 seconds\n",
      "\n",
      "Epoch 15/20, Loss Continents: 0.4824, Epoch Time: 5.31 seconds\n",
      "Epoch 15/20, Loss Cities: 1.2151, Epoch Time: 5.31 seconds\n",
      "Epoch 15/20, Loss Latitudes: 0.4096, Epoch Time: 5.31 seconds\n",
      "Epoch 15/20, Loss Longitudes: 0.5433, Epoch Time: 5.31 seconds\n",
      "\n",
      "Epoch 16/20, Loss Continents: 0.5136, Epoch Time: 5.18 seconds\n",
      "Epoch 16/20, Loss Cities: 1.1716, Epoch Time: 5.18 seconds\n",
      "Epoch 16/20, Loss Latitudes: 0.3928, Epoch Time: 5.18 seconds\n",
      "Epoch 16/20, Loss Longitudes: 0.4394, Epoch Time: 5.18 seconds\n",
      "\n",
      "Epoch 17/20, Loss Continents: 0.5206, Epoch Time: 5.41 seconds\n",
      "Epoch 17/20, Loss Cities: 1.2421, Epoch Time: 5.41 seconds\n",
      "Epoch 17/20, Loss Latitudes: 0.6714, Epoch Time: 5.41 seconds\n",
      "Epoch 17/20, Loss Longitudes: 0.6062, Epoch Time: 5.41 seconds\n",
      "\n",
      "Epoch 18/20, Loss Continents: 0.5934, Epoch Time: 5.42 seconds\n",
      "Epoch 18/20, Loss Cities: 1.1231, Epoch Time: 5.42 seconds\n",
      "Epoch 18/20, Loss Latitudes: 0.5392, Epoch Time: 5.42 seconds\n",
      "Epoch 18/20, Loss Longitudes: 0.6351, Epoch Time: 5.42 seconds\n",
      "\n",
      "Epoch 19/20, Loss Continents: 0.5482, Epoch Time: 5.47 seconds\n",
      "Epoch 19/20, Loss Cities: 1.1230, Epoch Time: 5.47 seconds\n",
      "Epoch 19/20, Loss Latitudes: 0.4668, Epoch Time: 5.47 seconds\n",
      "Epoch 19/20, Loss Longitudes: 0.6614, Epoch Time: 5.47 seconds\n",
      "\n",
      "Epoch 20/20, Loss Continents: 0.3709, Epoch Time: 5.69 seconds\n",
      "Epoch 20/20, Loss Cities: 1.0554, Epoch Time: 5.69 seconds\n",
      "Epoch 20/20, Loss Latitudes: 0.3666, Epoch Time: 5.69 seconds\n",
      "Epoch 20/20, Loss Longitudes: 0.4046, Epoch Time: 5.69 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time() # start time of each epoch\n",
    "\n",
    "    for batch_idx, (data,continent_city,lat_long) in enumerate(train_dl):\n",
    "        data = data.to(device=device)\n",
    "\n",
    "\n",
    "        # scores continent\n",
    "        scores_continent = nn_continent_model(data)\n",
    "        in_data_cities = torch.cat((data,scores_continent),1)\n",
    "\n",
    "        # scores cities\n",
    "        scores_cities = nn_cities_model(in_data_cities)\n",
    "        in_data_lat = torch.cat((in_data_cities,scores_cities),1)\n",
    "        \n",
    "        # scores latititude\n",
    "        scores_lat = nn_lat_model(in_data_lat)\n",
    "        in_data_long = torch.cat((in_data_lat,scores_lat),1)\n",
    "\n",
    "        # scores longitude\n",
    "        scores_long = nn_long_model(in_data_long)\n",
    " \n",
    "\n",
    "        # loss\n",
    "        loss_cities = criterion(scores_cities, continent_city[:,1])\n",
    "        loss_continents = criterion(scores_continent,continent_city[:,0])\n",
    "        loss_lat = criterion_lat_lon(scores_lat,lat_long[:,0].unsqueeze(1))\n",
    "        loss_long = criterion_lat_lon(scores_long,lat_long[:,-1].unsqueeze(1))\n",
    "\n",
    "        # backward propogation\n",
    "        optimizer_cities.zero_grad()\n",
    "        optimizer_continent.zero_grad()\n",
    "        optimizer_lat.zero_grad()\n",
    "        optimizer_long.zero_grad()\n",
    "        \n",
    "        loss_long.backward(retain_graph=True)\n",
    "        loss_lat.backward(retain_graph=True)\n",
    "        loss_cities.backward(retain_graph=True)\n",
    "        loss_continents.backward()\n",
    "\n",
    "\n",
    "        # gradient descent\n",
    "        optimizer_cities.step()\n",
    "        optimizer_continent.step()\n",
    "    \n",
    "\n",
    "    epoch_end_time = time.time() # end time of each epoch\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}, Loss Continents: {loss_continents.item():.4f}, Epoch Time: {epoch_duration:.2f} seconds\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss Cities: {loss_cities.item():.4f}, Epoch Time: {epoch_duration:.2f} seconds\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss Latitudes: {loss_lat.item():.4f}, Epoch Time: {epoch_duration:.2f} seconds\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss Longitudes: {loss_long.item():.4f}, Epoch Time: {epoch_duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy:\n",
      "Continent Model Accuracy: 83.78%\n",
      "Cities Model Accuracy: 66.00%\n",
      "Latitude Model Mean Absolute Error: 0.4658\n",
      "Longitude Model Mean Absolute Error: 0.4733\n",
      "\n",
      "Test Accuracy:\n",
      "Continent Model Accuracy: 80.84%\n",
      "Cities Model Accuracy: 63.27%\n",
      "Latitude Model Mean Absolute Error: 0.4848\n",
      "Longitude Model Mean Absolute Error: 0.5089\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy on different levels\n",
    "def check_accuracy(loader, continent_model, cities_model=None, lat_model=None, long_model=None, tolerance_lat=0.1, tolerance_long=0.1):\n",
    "    num_correct_continent = 0\n",
    "    num_samples = 0\n",
    "    num_correct_cities = 0\n",
    "    total_absolute_error_lat = 0.0\n",
    "    total_absolute_error_long = 0.0\n",
    "\n",
    "    continent_model.eval()\n",
    "    if cities_model:\n",
    "        cities_model.eval()\n",
    "    if lat_model:\n",
    "        lat_model.eval()\n",
    "    if long_model:\n",
    "        long_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (data, continent_city, lat_long) in loader:\n",
    "            data = data.to(device=device)\n",
    "            target_continent = continent_city[:, 0].to(device=device)\n",
    "            target_cities = continent_city[:, 1].to(device=device)\n",
    "            target_lat = lat_long[:, 0].to(device=device).unsqueeze(1) # Ensure correct shape for comparison\n",
    "            target_long = lat_long[:, 1].to(device=device).unsqueeze(1) # Ensure correct shape for comparison\n",
    "\n",
    "            # Continent predictions\n",
    "            scores_continent = continent_model(data)\n",
    "            _, predictions_continent = scores_continent.max(1)\n",
    "            num_correct_continent += (predictions_continent == target_continent).sum()\n",
    "\n",
    "            num_samples += predictions_continent.size(0)\n",
    "\n",
    "            # Cities predictions (if cities model is provided)\n",
    "            if cities_model:\n",
    "                scores_continent_for_cities = continent_model(data) # Get continent scores again\n",
    "                in_data_cities = torch.cat((data, scores_continent_for_cities), 1)\n",
    "                scores_cities = cities_model(in_data_cities)\n",
    "                _, predictions_cities = scores_cities.max(1)\n",
    "                num_correct_cities += (predictions_cities == target_cities).sum()\n",
    "\n",
    "            # Latitude predictions (if latitude model is provided)\n",
    "            if lat_model:\n",
    "                scores_continent_for_lat = continent_model(data)\n",
    "                in_data_cities_for_lat = torch.cat((data, scores_continent_for_lat), 1)\n",
    "                scores_cities_for_lat = cities_model(in_data_cities_for_lat)\n",
    "                in_data_lat = torch.cat((in_data_cities_for_lat, scores_cities_for_lat), 1)\n",
    "                predicted_lat = lat_model(in_data_lat)\n",
    "                absolute_error_lat = torch.abs(predicted_lat - target_lat)\n",
    "                total_absolute_error_lat += torch.sum(absolute_error_lat).item()\n",
    "\n",
    "            # Longitude predictions (if longitude model is provided)\n",
    "            if long_model:\n",
    "                scores_continent_for_long = continent_model(data)\n",
    "                scores_cities_for_long = cities_model(torch.cat((data, scores_continent_for_long), 1))\n",
    "                in_data_lat_for_long = torch.cat((torch.cat((data, scores_continent_for_long), 1), scores_cities_for_long), 1)\n",
    "                predicted_lat_for_long = lat_model(in_data_lat_for_long)\n",
    "                in_data_long = torch.cat((in_data_lat_for_long, predicted_lat_for_long), 1)\n",
    "                predicted_long = long_model(in_data_long)\n",
    "                absolute_error_long = torch.abs(predicted_long - target_long)\n",
    "                total_absolute_error_long += torch.sum(absolute_error_long).item()\n",
    "\n",
    "\n",
    "\n",
    "    accuracy_continent = float(num_correct_continent) / float(num_samples) * 100\n",
    "    print(f'Continent Model Accuracy: {accuracy_continent:.2f}%')\n",
    "\n",
    "    if cities_model:\n",
    "        accuracy_cities = float(num_correct_cities) / float(num_samples) * 100\n",
    "        print(f'Cities Model Accuracy: {accuracy_cities:.2f}%')\n",
    "\n",
    "    if lat_model:\n",
    "        mean_absolute_error_lat = total_absolute_error_lat / num_samples\n",
    "        print(f'Latitude Model Mean Absolute Error: {mean_absolute_error_lat:.4f}')\n",
    "\n",
    "    if long_model:\n",
    "        mean_absolute_error_long = total_absolute_error_long / num_samples\n",
    "        print(f'Longitude Model Mean Absolute Error: {mean_absolute_error_long:.4f}')\n",
    "\n",
    "    continent_model.train()\n",
    "    if cities_model:\n",
    "        cities_model.train()\n",
    "    if lat_model:\n",
    "        lat_model.train()\n",
    "    if long_model:\n",
    "        long_model.train()\n",
    "\n",
    "# Check accuracy for all models\n",
    "print(\"\\nTraining Accuracy:\")\n",
    "check_accuracy(train_dl, nn_continent_model, nn_cities_model, nn_lat_model, nn_long_model)\n",
    "print(\"\\nTest Accuracy:\")\n",
    "check_accuracy(test_dl, nn_continent_model, nn_cities_model, nn_lat_model, nn_long_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model - 07/04/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedNeuralNet(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(CombinedNeuralNet,self).__init__()\n",
    "        # Initial layers shared by all the tasks\n",
    "        self.layer1 = nn.Linear(input_size,400)\n",
    "        self.layer2 = nn.Linear(400,400)\n",
    "        self.layer3 = nn.Linear(400,200)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Branch for Continent Prediction\n",
    "        self.continent_layer = nn.Linear(200,7) # Output for 7 continents\n",
    "        \n",
    "        # Layers after continent branch\n",
    "        self.layer_after_continent = nn.Linear(207,400) # Concatenate previous layers\n",
    "        self.layer_cities1 = nn.Linear(400,400)\n",
    "        self.layer_cities2 = nn.Linear(400,200) \n",
    "        self.city_layer = nn.Linear(200, 40) # Output for 40 cities\n",
    "\n",
    "        # Layers after city branch\n",
    "        self.layer_after_cities = nn.Linear(247,400)  # Concatenate previous layers\n",
    "        self.layer_lat1 = nn.Linear(400,400)\n",
    "        self.layer_lat2 = nn.Linear(400,200)\n",
    "        self.latitude_layer = nn.Linear(200,1) # Ouput for latitude\n",
    "\n",
    "        # Layers after latitude branch\n",
    "        self.layer_after_lat = nn.Linear(248, 400) # Concatenate previous layers\n",
    "        self.layer_long1 = nn.Linear(400,400)\n",
    "        self.layer_long2 = nn.Linear(400,200)\n",
    "        self.longitude_layer = nn.Linear(200,1) # Outptu for longitude\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        # Shared layers\n",
    "        out = self.relu(self.layer1(x))\n",
    "        out = self.relu(self.layer2(out))\n",
    "        shared_out = self.relu(self.layer3(out))\n",
    "\n",
    "        # Continent branch\n",
    "        continent_logits = self.continent_layer(shared_out)\n",
    "\n",
    "        # City branch\n",
    "        concat_continent = torch.cat((shared_out, continent_logits), dim=1)\n",
    "        out_cities = self.relu(self.layer_after_continent(concat_continent))\n",
    "        out_cities = self.relu(self.layer_cities1(out_cities))\n",
    "        out_cities = self.relu(self.layer_cities2(out_cities))\n",
    "        city_logits = self.city_layer(out_cities)\n",
    "\n",
    "        # Latitude branch\n",
    "        concat_cities = torch.cat((concat_continent, city_logits), dim=1) # Include continent info as well\n",
    "        out_lat = self.relu(self.layer_after_cities(concat_cities))\n",
    "        out_lat = self.relu(self.layer_lat1(out_lat))\n",
    "        out_lat = self.relu(self.layer_lat2(out_lat))\n",
    "        latitude_prediction = self.latitude_layer(out_lat)\n",
    "\n",
    "        # Longitude branch\n",
    "        concat_lat = torch.cat((concat_cities, latitude_prediction), dim=1) # Include continent and city info\n",
    "        out_long = self.relu(self.layer_after_lat(concat_lat))\n",
    "        out_long = self.relu(self.layer_long1(out_long))\n",
    "        out_long = self.relu(self.layer_long2(out_long))\n",
    "        longitude_prediction = self.longitude_layer(out_long)\n",
    "\n",
    "        return continent_logits, city_logits, latitude_prediction, longitude_prediction\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 200\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "\n",
    "# Initialize the combined network\n",
    "combined_model = CombinedNeuralNet(input_size).to(device)\n",
    "\n",
    "# Loss functions and optimizers\n",
    "criterion_continent = nn.CrossEntropyLoss()\n",
    "criterion_cities = nn.CrossEntropyLoss()\n",
    "criterion_lat_lon = nn.MSELoss()\n",
    "optimizer_combined = torch.optim.Adam(combined_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss (Continent): 1.5871, Loss (Cities): 3.0838, Loss (Lat): 1.1349, Loss (Long): 1.2983, Epoch Time: 4.42 seconds\n",
      "Epoch 2/20, Loss (Continent): 1.1989, Loss (Cities): 2.8471, Loss (Lat): 0.8669, Loss (Long): 0.8780, Epoch Time: 4.51 seconds\n",
      "Epoch 3/20, Loss (Continent): 0.9039, Loss (Cities): 2.1539, Loss (Lat): 1.0387, Loss (Long): 0.5097, Epoch Time: 4.27 seconds\n",
      "Epoch 4/20, Loss (Continent): 0.8719, Loss (Cities): 1.9786, Loss (Lat): 0.4834, Loss (Long): 0.4824, Epoch Time: 4.52 seconds\n",
      "Epoch 5/20, Loss (Continent): 0.9425, Loss (Cities): 1.9934, Loss (Lat): 0.8179, Loss (Long): 0.3748, Epoch Time: 4.43 seconds\n",
      "Epoch 6/20, Loss (Continent): 0.8427, Loss (Cities): 1.7413, Loss (Lat): 0.6649, Loss (Long): 0.5281, Epoch Time: 4.38 seconds\n",
      "Epoch 7/20, Loss (Continent): 0.6994, Loss (Cities): 1.6707, Loss (Lat): 0.6340, Loss (Long): 0.2840, Epoch Time: 4.08 seconds\n",
      "Epoch 8/20, Loss (Continent): 0.6382, Loss (Cities): 1.4611, Loss (Lat): 0.3097, Loss (Long): 0.2180, Epoch Time: 4.72 seconds\n",
      "Epoch 9/20, Loss (Continent): 0.6730, Loss (Cities): 1.6721, Loss (Lat): 0.6918, Loss (Long): 0.3739, Epoch Time: 4.30 seconds\n",
      "Epoch 10/20, Loss (Continent): 0.6238, Loss (Cities): 1.6914, Loss (Lat): 0.1941, Loss (Long): 0.4337, Epoch Time: 4.47 seconds\n",
      "Epoch 11/20, Loss (Continent): 0.6668, Loss (Cities): 1.8130, Loss (Lat): 0.3119, Loss (Long): 0.5125, Epoch Time: 4.41 seconds\n",
      "Epoch 12/20, Loss (Continent): 0.5075, Loss (Cities): 1.2810, Loss (Lat): 0.2207, Loss (Long): 0.2866, Epoch Time: 4.32 seconds\n",
      "Epoch 13/20, Loss (Continent): 0.4709, Loss (Cities): 1.1816, Loss (Lat): 0.3429, Loss (Long): 0.1850, Epoch Time: 4.09 seconds\n",
      "Epoch 14/20, Loss (Continent): 0.7430, Loss (Cities): 1.3716, Loss (Lat): 0.5199, Loss (Long): 0.3501, Epoch Time: 3.88 seconds\n",
      "Epoch 15/20, Loss (Continent): 0.5304, Loss (Cities): 1.3199, Loss (Lat): 0.3321, Loss (Long): 0.1982, Epoch Time: 4.23 seconds\n",
      "Epoch 16/20, Loss (Continent): 0.4179, Loss (Cities): 1.2239, Loss (Lat): 0.2386, Loss (Long): 0.2042, Epoch Time: 3.96 seconds\n",
      "Epoch 17/20, Loss (Continent): 0.5720, Loss (Cities): 1.0050, Loss (Lat): 0.2850, Loss (Long): 0.2578, Epoch Time: 4.12 seconds\n",
      "Epoch 18/20, Loss (Continent): 0.4392, Loss (Cities): 1.4352, Loss (Lat): 0.2961, Loss (Long): 0.2500, Epoch Time: 4.11 seconds\n",
      "Epoch 19/20, Loss (Continent): 0.3455, Loss (Cities): 1.0255, Loss (Lat): 0.1925, Loss (Long): 0.2035, Epoch Time: 4.28 seconds\n",
      "Epoch 20/20, Loss (Continent): 0.4901, Loss (Cities): 0.8413, Loss (Lat): 0.1461, Loss (Long): 0.1573, Epoch Time: 4.13 seconds\n",
      "Total Training Time: 85.64 seconds\n"
     ]
    }
   ],
   "source": [
    "# Training loop (example - you'll need to adapt your existing loop)\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for batch_idx, (data, continent_city, lat_long) in enumerate(train_dl):\n",
    "        data = data.to(device)\n",
    "        target_continent = continent_city[:, 0].to(device)\n",
    "        target_cities = continent_city[:, 1].to(device)\n",
    "        target_lat = lat_long[:, 0].unsqueeze(1).to(device)\n",
    "        target_long = lat_long[:, 1].unsqueeze(1).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        continent_logits, city_logits, latitude_prediction, longitude_prediction = combined_model(data)\n",
    "\n",
    "        # Calculate losses\n",
    "        loss_continent = criterion_continent(continent_logits, target_continent)\n",
    "        loss_cities = criterion_cities(city_logits, target_cities)\n",
    "        loss_lat = criterion_lat_lon(latitude_prediction, target_lat)\n",
    "        loss_long = criterion_lat_lon(longitude_prediction, target_long)\n",
    "        total_loss = loss_continent + loss_cities + loss_lat + loss_long\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer_combined.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer_combined.step()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss (Continent): {loss_continent.item():.4f}, Loss (Cities): {loss_cities.item():.4f}, Loss (Lat): {loss_lat.item():.4f}, Loss (Long): {loss_long.item():.4f}, Epoch Time: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "end_time = time.time()\n",
    "total_duration = end_time - start_time\n",
    "print(f\"Total Training Time: {total_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy (Combined Model):\n",
      "Combined Model - Continent Accuracy: 83.78%\n",
      "Combined Model - Cities Accuracy: 67.44%\n",
      "Combined Model - Latitude Mean Absolute Error: 0.3049\n",
      "Combined Model - Longitude Mean Absolute Error: 0.2864\n",
      "\n",
      "Test Accuracy (Combined Model):\n",
      "Combined Model - Continent Accuracy: 80.34%\n",
      "Combined Model - Cities Accuracy: 62.04%\n",
      "Combined Model - Latitude Mean Absolute Error: 0.3653\n",
      "Combined Model - Longitude Mean Absolute Error: 0.3597\n"
     ]
    }
   ],
   "source": [
    "# Modified check_accuracy function for the combined model\n",
    "def check_combined_accuracy(loader, model):\n",
    "    num_correct_continent = 0\n",
    "    num_samples = 0\n",
    "    num_correct_cities = 0\n",
    "    total_absolute_error_lat = 0.0\n",
    "    total_absolute_error_long = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (data, continent_city, lat_long) in loader:\n",
    "            data = data.to(device)\n",
    "            target_continent = continent_city[:, 0].to(device)\n",
    "            target_cities = continent_city[:, 1].to(device)\n",
    "            target_lat = lat_long[:, 0].unsqueeze(1).to(device)\n",
    "            target_long = lat_long[:, 1].unsqueeze(1).to(device)\n",
    "\n",
    "            continent_logits, city_logits, latitude_prediction, longitude_prediction = model(data)\n",
    "\n",
    "            # Continent Accuracy\n",
    "            _, predictions_continent = continent_logits.max(1)\n",
    "            num_correct_continent += (predictions_continent == target_continent).sum()\n",
    "            num_samples += predictions_continent.size(0)\n",
    "\n",
    "            # Cities Accuracy\n",
    "            _, predictions_cities = city_logits.max(1)\n",
    "            num_correct_cities += (predictions_cities == target_cities).sum()\n",
    "\n",
    "            # Latitude Error\n",
    "            absolute_error_lat = torch.abs(latitude_prediction - target_lat)\n",
    "            total_absolute_error_lat += torch.sum(absolute_error_lat).item()\n",
    "\n",
    "            # Longitude Error\n",
    "            absolute_error_long = torch.abs(longitude_prediction - target_long)\n",
    "            total_absolute_error_long += torch.sum(absolute_error_long).item()\n",
    "\n",
    "    accuracy_continent = float(num_correct_continent) / float(num_samples) * 100\n",
    "    accuracy_cities = float(num_correct_cities) / float(num_samples) * 100\n",
    "    mean_absolute_error_lat = total_absolute_error_lat / num_samples\n",
    "    mean_absolute_error_long = total_absolute_error_long / num_samples\n",
    "\n",
    "    print(f'Combined Model - Continent Accuracy: {accuracy_continent:.2f}%')\n",
    "    print(f'Combined Model - Cities Accuracy: {accuracy_cities:.2f}%')\n",
    "    print(f'Combined Model - Latitude Mean Absolute Error: {mean_absolute_error_lat:.4f}')\n",
    "    print(f'Combined Model - Longitude Mean Absolute Error: {mean_absolute_error_long:.4f}')\n",
    "\n",
    "    model.train()\n",
    "\n",
    "# Check accuracy of the combined model\n",
    "print(\"\\nTraining Accuracy (Combined Model):\")\n",
    "check_combined_accuracy(train_dl, combined_model)\n",
    "print(\"\\nTest Accuracy (Combined Model):\")\n",
    "check_combined_accuracy(test_dl, combined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other iterations of neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.4705, Epoch Time: 2.37 seconds\n",
      "Epoch 2/10, Loss: 0.9484, Epoch Time: 2.31 seconds\n",
      "Epoch 3/10, Loss: 1.1979, Epoch Time: 2.50 seconds\n",
      "Epoch 4/10, Loss: 1.2478, Epoch Time: 2.39 seconds\n",
      "Epoch 5/10, Loss: 0.9250, Epoch Time: 2.60 seconds\n",
      "Epoch 6/10, Loss: 1.1541, Epoch Time: 2.39 seconds\n",
      "Epoch 7/10, Loss: 0.7663, Epoch Time: 2.60 seconds\n",
      "Epoch 8/10, Loss: 1.2085, Epoch Time: 2.40 seconds\n",
      "Epoch 9/10, Loss: 1.2017, Epoch Time: 2.54 seconds\n",
      "Epoch 10/10, Loss: 0.9313, Epoch Time: 2.58 seconds\n",
      "Total Training Time: 24.67 seconds\n",
      "Training Accuracy:\n",
      "Continent Model: Got 2787/3256 with accuracy 85.60%\n",
      "\n",
      "Test Accuracy:\n",
      "Continent Model: Got 660/814 with accuracy 81.08%\n",
      "\n",
      "Training Accuracy (Continent and Cities):\n",
      "Continent Model: Got 2787/3256 with accuracy 85.60%\n",
      "Cities Model: Got 2451/3256 with accuracy 75.28%\n",
      "\n",
      "Test Accuracy (Continent and Cities):\n",
      "Continent Model: Got 660/814 with accuracy 81.08%\n",
      "Cities Model: Got 557/814 with accuracy 68.43%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time() # start time of each epoch\n",
    "\n",
    "    for batch_idx, (data,continent_city,lat_long) in enumerate(train_dl):\n",
    "        data = data.to(device=device)\n",
    "    \n",
    "\n",
    "        # scores continent\n",
    "        scores_continent = nn_continent_model(data)\n",
    "        in_data_cities = torch.cat((data,scores_continent),1)\n",
    "\n",
    "        # scores cities\n",
    "        scores_cities = nn_cities_model(in_data_cities)\n",
    "\n",
    "        # loss\n",
    "        loss_cities = criterion(scores_cities, continent_city[:,1])\n",
    "        loss_continents = criterion(scores_continent,continent_city[:,0])\n",
    "\n",
    "        # backward propogation\n",
    "        optimizer_cities.zero_grad()\n",
    "        optimizer_continent.zero_grad()\n",
    "        \n",
    "        loss_cities.backward(retain_graph=True)\n",
    "        loss_continents.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        optimizer_cities.step()\n",
    "        optimizer_continent.step()\n",
    "    \n",
    "    epoch_end_time = time.time() # end time of each epoch\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss_cities.item():.4f}, Epoch Time: {epoch_duration:.2f} seconds\")\n",
    "    \n",
    "\n",
    "end_time = time.time()  # End time\n",
    "total_duration = end_time - start_time\n",
    "print(f\"Total Training Time: {total_duration:.2f} seconds\")   \n",
    "\n",
    "def check_accuracy(loader, continent_model, cities_model=None):\n",
    "    num_correct_continent = 0\n",
    "    num_samples = 0\n",
    "    num_correct_cities = 0\n",
    "\n",
    "    continent_model.eval()\n",
    "    if cities_model:\n",
    "        cities_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (data, continent_city, lat_long) in loader:\n",
    "            data = data.to(device=device)\n",
    "            target_continent = continent_city[:, 0].to(device=device)\n",
    "            target_cities = continent_city[:, 1].to(device=device)\n",
    "\n",
    "            # Continent predictions\n",
    "            scores_continent = continent_model(data)\n",
    "            _, predictions_continent = scores_continent.max(1)\n",
    "            num_correct_continent += (predictions_continent == target_continent).sum()\n",
    "\n",
    "            num_samples += predictions_continent.size(0)\n",
    "\n",
    "            # Cities predictions (if cities model is provided)\n",
    "            if cities_model:\n",
    "                scores_continent_for_cities = continent_model(data) # Get continent scores again\n",
    "                in_data_cities = torch.cat((data, scores_continent_for_cities), 1)\n",
    "                scores_cities = cities_model(in_data_cities)\n",
    "                _, predictions_cities = scores_cities.max(1)\n",
    "                num_correct_cities += (predictions_cities == target_cities).sum()\n",
    "\n",
    "    accuracy_continent = float(num_correct_continent) / float(num_samples) * 100\n",
    "    print(f'Continent Model: Got {num_correct_continent}/{num_samples} with accuracy {accuracy_continent:.2f}%')\n",
    "\n",
    "    if cities_model:\n",
    "        accuracy_cities = float(num_correct_cities) / float(num_samples) * 100\n",
    "        print(f'Cities Model: Got {num_correct_cities}/{num_samples} with accuracy {accuracy_cities:.2f}%')\n",
    "\n",
    "    continent_model.train()\n",
    "    if cities_model:\n",
    "        cities_model.train()\n",
    "\n",
    "# Check accuracy for the continent model\n",
    "print(\"Training Accuracy:\")\n",
    "check_accuracy(train_dl, nn_continent_model)\n",
    "print(\"\\nTest Accuracy:\")\n",
    "check_accuracy(test_dl, nn_continent_model)\n",
    "\n",
    "# Check accuracy for both continent and cities models\n",
    "print(\"\\nTraining Accuracy (Continent and Cities):\")\n",
    "check_accuracy(train_dl, nn_continent_model, nn_cities_model)\n",
    "print(\"\\nTest Accuracy (Continent and Cities):\")\n",
    "check_accuracy(test_dl, nn_continent_model, nn_cities_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.5210, Epoch Time: 1.28 seconds\n",
      "Epoch 2/10, Loss: 1.1578, Epoch Time: 1.49 seconds\n",
      "Epoch 3/10, Loss: 0.8090, Epoch Time: 1.50 seconds\n",
      "Epoch 4/10, Loss: 0.8966, Epoch Time: 1.42 seconds\n",
      "Epoch 5/10, Loss: 0.9593, Epoch Time: 1.38 seconds\n",
      "Epoch 6/10, Loss: 0.8749, Epoch Time: 1.51 seconds\n",
      "Epoch 7/10, Loss: 0.8830, Epoch Time: 1.39 seconds\n",
      "Epoch 8/10, Loss: 0.4622, Epoch Time: 1.31 seconds\n",
      "Epoch 9/10, Loss: 0.7493, Epoch Time: 1.47 seconds\n",
      "Epoch 10/10, Loss: 0.4795, Epoch Time: 1.34 seconds\n",
      "Total Training Time: 14.09 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  # Start time\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time() # start time of each epoch\n",
    "    for batch_idx, (data, target) in enumerate(train_dl):\n",
    "        data = data.to(device=device)\n",
    "        target = target[:,0].to(device=device)\n",
    "\n",
    "        # scores\n",
    "        scores = nn_continent_model(data)\n",
    "    \n",
    "\n",
    "        loss = criterion(scores, target)\n",
    "\n",
    "        # backward\n",
    "        optimizer_continent.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer_continent.step()\n",
    "    epoch_end_time = time.time() # end time of each epoch\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Epoch Time: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "end_time = time.time()  # End time\n",
    "total_duration = end_time - start_time\n",
    "print(f\"Total Training Time: {total_duration:.2f} seconds\")\n",
    "\n",
    "# Check accuracy on training and test to see how good model is\n",
    "def check_accuracy(loader,model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y[:,0].to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100}')\n",
    "\n",
    "    model.train() \n",
    "\n",
    "\n",
    "check_accuracy(train_dl,nn_continent_model)\n",
    "check_accuracy(test_dl,nn_continent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
