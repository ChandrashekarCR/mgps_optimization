{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metasub data mGPS algorithm - 31/03/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am just trying to get the data pre-processing steps right. The idea for doing this is to get the dataset in the right format for easier analysis using neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "os.chdir(\"/home/inf-21-2024/binp37/\")\n",
    "# Read the metadata for the metasub data.\n",
    "complete_meta = pd.read_csv(\"./data/metasub/complete_metadata.csv\")\n",
    "taxa_abund = pd.read_csv(\"./data/metasub/metasub_taxa_abundance.csv\")\n",
    "taxa_abund = taxa_abund.drop_duplicates(subset=['uuid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4288, 3711)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the bacterial and metadata\n",
    "metasub_data = pd.merge(complete_meta,taxa_abund,on='uuid')\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4157, 3711)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove control samples\n",
    "control_cities = {'control','other_control','neg_control','other','pos_control'}\n",
    "control_types = {'ctrl cities','negative_control','positive_control'}\n",
    "\n",
    "mask = metasub_data['city'].isin(control_cities) | metasub_data['control_type'].isin(control_types)\n",
    "metasub_data = metasub_data[~mask].copy()\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4157, 3711)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Re-label london boroughs\n",
    "metasub_data.loc[metasub_data['city'].isin(['kensington','islington']),'city'] = 'london'\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove sparse sample locations and doubtful samples\n",
    "city_counts = metasub_data['city'].value_counts()\n",
    "small_cities = city_counts[city_counts<8].index.tolist()\n",
    "remove_samples = metasub_data['city'].isin(['antarctica']+small_cities)\n",
    "metasub_data = metasub_data[~remove_samples]\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct the identified mislabeling of data\n",
    "kyiv_filter = metasub_data['city'] == 'kyiv'\n",
    "metasub_data.loc[kyiv_filter,'latitude'] = metasub_data.loc[kyiv_filter,'city_latitude'] # Set all the latitude to the city_latitude\n",
    "metasub_data.loc[kyiv_filter,'longitude'] = metasub_data.loc[kyiv_filter,'city_longitude'] # Set all the latitude to the city_longitutde\n",
    "\n",
    "porto_filter = metasub_data['city'] == 'porto'\n",
    "metasub_data.loc[porto_filter,'city'] = \"europe\"\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing latitude and longitude values with city-level data\n",
    "missing_lat = metasub_data[\"latitude\"].isna()\n",
    "missing_lon = metasub_data[\"longitude\"].isna()\n",
    "metasub_data.loc[missing_lat, \"latitude\"] = metasub_data.loc[missing_lat, \"city_latitude\"]\n",
    "metasub_data.loc[missing_lon, \"longitude\"] = metasub_data.loc[missing_lon, \"city_longitude\"]\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correction for incorrect London co-ordinates\n",
    "london_filter = metasub_data['city'] == 'london'\n",
    "metasub_data.loc[london_filter,'city_latitude'] = 51.50853\n",
    "metasub_data.loc[london_filter,'city_longitude'] = -0.12574\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uuid', 'metasub_name', 'core_project', 'project', 'city', 'city_code',\n",
       "       'latitude', 'longitude', 'surface_material', 'control_type',\n",
       "       'elevation', 'line', 'station', 'surface', 'temperature', 'traffic',\n",
       "       'setting', 'num_reads', 'library_post_PCR_Qubit',\n",
       "       'library_QC_concentration', 'city_latitude', 'city_longitude',\n",
       "       'coastal_city', 'city_total_population', 'city_population_density',\n",
       "       'city_land_area_km2', 'city_ave_june_temp_c', 'city_elevation_meters',\n",
       "       'continent', 'city_koppen_climate', 'barcode', 'ha_id',\n",
       "       'hudson_alpha_flowcell', 'hudson_alpha_project', 'index_sequence',\n",
       "       'location_type', 'hudson_alpha_uid', 'other_project_uid',\n",
       "       'plate_number', 'plate_pos', 'sample_type', 'sl_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metasub_data.iloc[:,:42].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination - 02/04/2025\n",
    "\n",
    "Here we use RFE as a part of a pipeline to get the best set of parameters for fitting the deep learning model. We need a suitable set of parameters for that are infromative enough to apply a deep learning model. These are called as geographically informative taxa (GITs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inf-21-2024/miniconda3/envs/ai_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-05 10:59:45,640\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import multiprocessing\n",
    "import time\n",
    "import ray\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "*Explanation of the Code*\n",
    "\n",
    "This function is designed to perform feature selection by:\n",
    "\n",
    "1. **Removing Highly Correlated Features:**\n",
    "   - Computes a correlation matrix of the features.\n",
    "   - Identifies features with correlation greater than 0.98 and removes them.\n",
    "\n",
    "2. **Recursive Feature Elimination (RFE) for Feature Selection:**\n",
    "   - Uses a Random Forest Classifier to determine feature importance.\n",
    "   - Applies RFE (Recursive Feature Elimination) to iteratively remove the least important features.\n",
    "   - The number of features to keep is determined based on predefined subset sizes.\n",
    "\n",
    "3. **Parallel Processing Support:**  \n",
    "   Uses multiple CPU cores when specified for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection algorithm\n",
    "def species_select(X,y,remove_correlated=True,subsets=None,cores=1):\n",
    "    \"\"\"\n",
    "    Feature selection algorithm for species classification.\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.DataFrame): Feature matrix\n",
    "    y (pd.Series): Target variable\n",
    "    remove_correlated (bool): Whether we need to remoce highly correlated variables. (default is set to True)\n",
    "    subsets (list): List of feature subset sizes to evaluate. If None, it is determined automatically.\n",
    "    cores (int): Number of CPU cores to use for parallel computation. (default is set to 1)\n",
    "    \n",
    "    Returns:\n",
    "    RFE object: Trained Recursive Feature Elimination (RFE) model.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()  # Track execution time\n",
    "\n",
    "    # Set parallel processing\n",
    "    num_cores = multiprocessing.cpu_count() if cores > 1 else 1\n",
    "    print(f\"Using {num_cores} CPU cores for computation.\")\n",
    "\n",
    "    if remove_correlated:\n",
    "        # Compute correlation matrix\n",
    "        print(\"Calculating correlation matrix...\")\n",
    "        corr_matrix = X.corr()\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "        # Identify correlated features (above 0.98)\n",
    "        correlated_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.98)]\n",
    "\n",
    "        # Drop correlated features\n",
    "        X = X.drop(columns=correlated_features)\n",
    "        print(f\"Correlated feature removed: {len(correlated_features)}\")\n",
    "\n",
    "    # Determine default subset sizes if not provided\n",
    "    num_features = X.shape[1]\n",
    "    if subsets is None:\n",
    "        subsets = [num_features // 2, num_features // 4, num_features // 8, num_features // 16, num_features // 32, num_features // 64]\n",
    "        subsets = [s for s in subsets  if s > 0] # Remove non-positive values\n",
    "\n",
    "    print(f\"Feature selection subsets: {subsets}\")\n",
    "\n",
    "    # Define model (Random Forrest for fearure ranking)\n",
    "    model = RandomForestClassifier(n_jobs=num_cores, random_state=123)\n",
    "    print(\"Initialized RandomForestClassifier.\")\n",
    "\n",
    "    # Recursive Feature Elimination (RFE)\n",
    "    for subset in subsets:\n",
    "        print(f\"\\nStarting RFE with {subset} features....\")\n",
    "        start_rfe = time.time()\n",
    "        rfe = RFE(estimator=model, n_features_to_select=min(subsets),step=20)\n",
    "        # Fit RFE to the data\n",
    "        rfe.fit(X,y)\n",
    "        print(f\"Completed RFE with {subset} features in {time.time() - start_rfe:.2f} seconds.\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nFeature selection completed in {total_time:.2f} seconds.\")\n",
    "    return rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureElim = species_select(X=metasub_data.iloc[:,42:500],\n",
    "                             y=metasub_data['city'],\n",
    "                             remove_correlated=False,\n",
    "                             subsets=[50,100,200,500],\n",
    "                             cores=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You Can Do If It's Still Too Slow\n",
    "\n",
    "1. **Reduce Feature Set Before Running RFE:**\n",
    "    - If remove_correlated=False, you still have 3711 features.\n",
    "    - Try using only top 500-1000 features based on variance or importance.\n",
    "\n",
    "2. **Increase Step Size in RFE:**\n",
    "    - Default step=1 removes one feature per iteration, which is slow.\n",
    "    - Try step=5 or step=10 to remove multiple features at once:\n",
    "    - rfe = RFE(estimatoe=model, n_features_to_select=subset, step=5)\n",
    "3. **Use XGBoost Instead of Random Forest:**\n",
    "    - model = XGBClassifier(n_jobs=num_cores, random_state=123)\n",
    "4. **Sequential Feature Selector:**\n",
    "    - from sklearn.feature_selection import SequentialFeatureSelector\n",
    "    sfs = SequentialFeatureSelector(model, n_features_to_select=100, direction='backward', n_jobs=num_cores)\n",
    "    sfs.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 09:17:52,877\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting RFE with subsets of features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(evaluate_rfe pid=301904)\u001b[0m /home/inf-21-2024/miniconda3/envs/ai_env/lib/python3.12/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\n",
      "\u001b[36m(evaluate_rfe pid=301904)\u001b[0m   warnings.warn(\n",
      "Parallel RFE + Cross-validation: 100%|██████████| 60/60 [1:15:27<00:00, 75.46s/it]   \n",
      "\u001b[36m(evaluate_rfe pid=301544)\u001b[0m /home/inf-21-2024/miniconda3/envs/ai_env/lib/python3.12/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\u001b[32m [repeated 59x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(evaluate_rfe pid=301544)\u001b[0m   warnings.warn(\u001b[32m [repeated 59x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {\"rfe__n_features_to_select\": 200}\n",
      "Best accuracy: 0.891155\n",
      "Total time taken: 4531.09 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ray\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True,num_cpus=100)\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=1,random_state=123)\n",
    "parameters = {\"rfe__n_features_to_select\":[50,100,200,300,500,1500]}\n",
    "cv = 10\n",
    "n_features_options = parameters['rfe__n_features_to_select']\n",
    "total_iterations = len(n_features_options) * cv\n",
    "\n",
    "print(f\"\\nStarting RFE with subsets of features...\")\n",
    "\n",
    "# Define remote function for parallel excecution\n",
    "@ray.remote\n",
    "def evaluate_rfe(n_features, fold, X, y):\n",
    "    \"\"\"Perfroms RFE feature selection and evaluates performance for a given fold.\"\"\"\n",
    "    pipe = make_pipeline(RFE(estimator=model, n_features_to_select=n_features,step=10))\n",
    "\n",
    "    # We use the stratified K fold to split the data into training and validation sets\n",
    "    skf = StratifiedKFold(n_splits=cv,shuffle=True, random_state=fold)\n",
    "    train_index, test_index = list(skf.split(X,y))[fold]\n",
    "\n",
    "    # train_index and test_index contain the index values for extracting training and testing data from X and y variables.\n",
    "    X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit the model using the training data and the evaluate the score based on the testing data\n",
    "    pipe.fit(X_train, y_train)\n",
    "    score = pipe.score(X_test, y_test)\n",
    "\n",
    "    return n_features, fold, score\n",
    "\n",
    "start_time = time.time()\n",
    "tasks = [evaluate_rfe.remote(n_features,fold, X,y) for n_features in n_features_options for fold in range(cv)]\n",
    "\n",
    "results = []\n",
    "with tqdm(total=total_iterations, desc='Parallel RFE + Cross-validation') as pbar:\n",
    "    while tasks:\n",
    "        done, tasks = ray.wait(tasks, num_returns=1)\n",
    "        result = ray.get(done[0])\n",
    "        results.append((result[0],result[2])) # (n_features, score)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Aggregate mean accuracy for each feature subset\n",
    "results_df = pd.DataFrame(results, columns=[\"n_features\", \"accuracy\"])\n",
    "results_df = results_df.groupby(\"n_features\").mean().reset_index()\n",
    "\n",
    "# Find best feature subset\n",
    "best_n_features, best_score = results_df.loc[results_df[\"accuracy\"].idxmax()].values\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f'Best params: {{\"rfe__n_features_to_select\": {int(best_n_features)}}}')\n",
    "print(f'Best accuracy: {best_score:.6f}')\n",
    "print(f'Total time taken: {elapsed_time:.2f} seconds')\n",
    "\n",
    "ray.shutdown()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 11:00:26,661\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating correlation matrix...\n",
      "Correlated features removed: 34\n",
      "\n",
      "Starting RFE with subsets of features: [50, 100, 200, 300, 500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parallel RFE + Cross-validation:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[36m(evaluate_rfe_remote pid=3403706)\u001b[0m /home/inf-21-2024/miniconda3/envs/ai_env/lib/python3.12/site-packages/sklearn/feature_selection/_rfe.py:291: UserWarning: Found n_features_to_select=500 > n_features=324. There will be no feature selection and all features will be kept.\n",
      "\u001b[36m(evaluate_rfe_remote pid=3403706)\u001b[0m   warnings.warn(\n",
      "Parallel RFE + Cross-validation: 100%|██████████| 25/25 [00:35<00:00,  1.43s/it]\n",
      "\u001b[36m(evaluate_rfe_remote pid=3403704)\u001b[0m /home/inf-21-2024/miniconda3/envs/ai_env/lib/python3.12/site-packages/sklearn/feature_selection/_rfe.py:291: UserWarning: Found n_features_to_select=500 > n_features=324. There will be no feature selection and all features will be kept.\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(evaluate_rfe_remote pid=3403704)\u001b[0m   warnings.warn(\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best params: {'rfe__n_features_to_select': 50}\n",
      "Best accuracy: 0.749631\n",
      "Mean accuracy for all tested feature subsets:\n",
      "   n_features  accuracy\n",
      "0          50  0.749631\n",
      "1         100  0.742015\n",
      "2         200  0.733661\n",
      "3         300  0.732187\n",
      "4         500  0.735135\n",
      "Total time taken: 35.92 seconds\n"
     ]
    }
   ],
   "source": [
    "def parallel_rfe_feature_selection(X: pd.DataFrame, y: pd.Series, n_jobs: int = 1, random_state: int = 123,\n",
    "                                   cv: int = 10, subsets: list = None, remove_correlated: bool = True,\n",
    "                                   correlation_threshold: float = 0.98, num_cpus: int = None):\n",
    "    \"\"\"\n",
    "    Performs parallel Recursive Feature Elimination (RFE) with cross-validation to select the best feature subset.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame of features.\n",
    "        y (pd.Series): Series of the target variable.\n",
    "        n_jobs (int): Number of jobs for the base estimator (RandomForestClassifier).\n",
    "        random_state (int): Random state for reproducibility.\n",
    "        cv (int): Number of cross-validation folds.\n",
    "        subsets (list, optional): List of feature subset sizes to evaluate. If None, default subsets are used. Defaults to None.\n",
    "        remove_correlated (bool, optional): Whether to remove highly correlated features before RFE. Defaults to True.\n",
    "        correlation_threshold (float, optional): Threshold for identifying highly correlated features. Defaults to 0.98.\n",
    "        num_cpus (int, optional): Number of CPUs to use for Ray. If None, Ray will auto-detect. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - best_params (dict): Dictionary with the best RFE parameters.\n",
    "            - best_accuracy (float): The best mean cross-validation accuracy achieved.\n",
    "            - results_df (pd.DataFrame): DataFrame containing the mean accuracy for each feature subset size.\n",
    "            - elapsed_time (float): Total time taken for the feature selection process.\n",
    "    \"\"\"\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "    ray.init(ignore_reinit_error=True, num_cpus=num_cpus)\n",
    "\n",
    "    model = RandomForestClassifier(n_jobs=n_jobs, random_state=random_state)\n",
    "\n",
    "    if remove_correlated:\n",
    "        # Compute correlation matrix\n",
    "        print(\"Calculating correlation matrix...\")\n",
    "        corr_matrix = X.corr()\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "        # Identify correlated features (above threshold)\n",
    "        correlated_features = [column for column in upper_tri.columns if any(upper_tri[column] > correlation_threshold)]\n",
    "\n",
    "        # Drop correlated features\n",
    "        X = X.drop(columns=correlated_features)\n",
    "        print(f\"Correlated features removed: {len(correlated_features)}\")\n",
    "\n",
    "    # Determine default subset sizes if not provided\n",
    "    num_features = X.shape[1]\n",
    "    if subsets is None:\n",
    "        subsets = [num_features // 2, num_features // 4, num_features // 8, num_features // 16, num_features // 32, num_features // 64]\n",
    "        subsets = [s for s in subsets if s > 0]  # Remove non-positive values\n",
    "\n",
    "    n_features_options = sorted(list(set(subsets))) # Ensure unique and sorted subset sizes\n",
    "    total_iterations = len(n_features_options) * cv\n",
    "\n",
    "    print(f\"\\nStarting RFE with subsets of features: {n_features_options}\")\n",
    "\n",
    "    # Define remote function for parallel execution\n",
    "    @ray.remote\n",
    "    def evaluate_rfe_remote(n_features, fold, X_remote, y_remote):\n",
    "        \"\"\"Performs RFE feature selection and evaluates performance for a given fold.\"\"\"\n",
    "        pipe = make_pipeline(RFE(estimator=model, n_features_to_select=n_features, step=10))\n",
    "\n",
    "        # We use the stratified K fold to split the data into training and validation sets\n",
    "        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=fold)\n",
    "        train_index, test_index = list(skf.split(X_remote, y_remote))[fold]\n",
    "\n",
    "        # train_index and test_index contain the index values for extracting training and testing data\n",
    "        X_train = X_remote.iloc[train_index, :]\n",
    "        X_test = X_remote.iloc[test_index, :]\n",
    "        y_train = y_remote.iloc[train_index]\n",
    "        y_test = y_remote.iloc[test_index]\n",
    "\n",
    "        # Fit the model using the training data and then evaluate the score based on the testing data\n",
    "        pipe.fit(X_train, y_train)\n",
    "        score = pipe.score(X_test, y_test)\n",
    "\n",
    "        return n_features, fold, score\n",
    "\n",
    "    start_time = time.time()\n",
    "    X_ray = ray.put(X)\n",
    "    y_ray = ray.put(y)\n",
    "    tasks = [evaluate_rfe_remote.remote(n_features, fold, X_ray, y_ray)\n",
    "             for n_features in n_features_options for fold in range(cv)]\n",
    "\n",
    "    results = []\n",
    "    with tqdm(total=total_iterations, desc='Parallel RFE + Cross-validation') as pbar:\n",
    "        while tasks:\n",
    "            done, tasks = ray.wait(tasks, num_returns=1)\n",
    "            result = ray.get(done[0])\n",
    "            results.append((result[0], result[2]))  # (n_features, score)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Aggregate mean accuracy for each feature subset\n",
    "    results_df = pd.DataFrame(results, columns=[\"n_features\", \"accuracy\"])\n",
    "    results_df = results_df.groupby(\"n_features\").mean().reset_index()\n",
    "\n",
    "    # Find best feature subset\n",
    "    best_row = results_df.loc[results_df[\"accuracy\"].idxmax()]\n",
    "    best_n_features = int(best_row[\"n_features\"])\n",
    "    best_accuracy = best_row[\"accuracy\"]\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    best_params = {\"rfe__n_features_to_select\": best_n_features}\n",
    "\n",
    "    ray.shutdown()\n",
    "\n",
    "    return best_params, best_accuracy, results_df, elapsed_time\n",
    "\n",
    "X = metasub_data.iloc[:,42:400]\n",
    "y= metasub_data['city']\n",
    "\n",
    "best_parameters, best_score, all_results, time_taken = parallel_rfe_feature_selection(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        n_jobs=-1,  # Use all available cores for RandomForest within each Ray task\n",
    "        random_state=123,\n",
    "        cv=5,\n",
    "        subsets=[50, 100, 200, 300, 500],\n",
    "        remove_correlated=True,\n",
    "        correlation_threshold=0.95,\n",
    "        num_cpus=50  # Limit Ray to 4 CPUs for this example\n",
    "    )\n",
    "\n",
    "print(f'\\nBest params: {best_parameters}')\n",
    "print(f'Best accuracy: {best_score:.6f}')\n",
    "print(f'Mean accuracy for all tested feature subsets:\\n{all_results}')\n",
    "print(f'Total time taken: {time_taken:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Acidovorax ebreus', 'Acidovorax sp. JS42', 'Acidovorax sp. KKS102',\n",
      "       'Acinetobacter baumannii', 'Acinetobacter haemolyticus',\n",
      "       'Acinetobacter johnsonii', 'Acinetobacter junii',\n",
      "       'Acinetobacter pittii', 'Acinetobacter schindleri',\n",
      "       'Acinetobacter sp. LoGeW2-3',\n",
      "       ...\n",
      "       'Thermothelomyces thermophila', 'Thielavia terrestris',\n",
      "       'Truepera radiovictrix', 'Tsukamurella sp. MH1',\n",
      "       'Variovorax boronicumulans', 'Variovorax paradoxus',\n",
      "       'Variovorax sp. PAMC 28711', 'Veillonella parvula', 'Weissella cibaria',\n",
      "       'Xanthomonas campestris'],\n",
      "      dtype='object', length=200)\n",
      "3669\n"
     ]
    }
   ],
   "source": [
    "# Since I have lost the model, but I know that 200 features gives the best accuracy of 0.89. \n",
    "X = metasub_data.iloc[:,42:]\n",
    "y = metasub_data['city']\n",
    "model_200 = RandomForestClassifier(n_jobs=24,random_state=123)\n",
    "rfe = RFE(estimator=model_200,n_features_to_select=200,step=20)\n",
    "rfe.fit(X,y)\n",
    "\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(selected_features)\n",
    "\n",
    "print(len(rfe.support_))\n",
    "# All the accuracy results from the previous runs\n",
    "#results_df = pd.DataFrame(results,columns=['n_vars','accuracy'])\n",
    "#results_df.to_csv('mgps_git_taxa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Neural Networks Tests - 03/04/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acidovorax ebreus</th>\n",
       "      <th>Acidovorax sp. JS42</th>\n",
       "      <th>Acidovorax sp. KKS102</th>\n",
       "      <th>Acinetobacter baumannii</th>\n",
       "      <th>Acinetobacter haemolyticus</th>\n",
       "      <th>Acinetobacter johnsonii</th>\n",
       "      <th>Acinetobacter junii</th>\n",
       "      <th>Acinetobacter pittii</th>\n",
       "      <th>Acinetobacter schindleri</th>\n",
       "      <th>Acinetobacter sp. LoGeW2-3</th>\n",
       "      <th>...</th>\n",
       "      <th>Variovorax boronicumulans</th>\n",
       "      <th>Variovorax paradoxus</th>\n",
       "      <th>Variovorax sp. PAMC 28711</th>\n",
       "      <th>Veillonella parvula</th>\n",
       "      <th>Weissella cibaria</th>\n",
       "      <th>Xanthomonas campestris</th>\n",
       "      <th>continent</th>\n",
       "      <th>city</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00480</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00028</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>0.00142</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00091</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00181</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.00274</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>0.00191</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00208</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00397</td>\n",
       "      <td>oceania</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Acidovorax ebreus  Acidovorax sp. JS42  Acidovorax sp. KKS102  \\\n",
       "0            0.00000              0.00000                0.00023   \n",
       "1            0.00000              0.00001                0.00003   \n",
       "2            0.00003              0.00000                0.00011   \n",
       "3            0.00000              0.00000                0.00000   \n",
       "4            0.00000              0.00000                0.00000   \n",
       "\n",
       "   Acinetobacter baumannii  Acinetobacter haemolyticus  \\\n",
       "0                  0.00015                     0.00000   \n",
       "1                  0.00028                     0.00016   \n",
       "2                  0.00181                     0.00060   \n",
       "3                  0.00002                     0.00001   \n",
       "4                  0.00003                     0.00000   \n",
       "\n",
       "   Acinetobacter johnsonii  Acinetobacter junii  Acinetobacter pittii  \\\n",
       "0                  0.00006              0.00001               0.00007   \n",
       "1                  0.00142              0.00017               0.00013   \n",
       "2                  0.00274              0.00030               0.00110   \n",
       "3                  0.00003              0.00000               0.00000   \n",
       "4                  0.00000              0.00000               0.00002   \n",
       "\n",
       "   Acinetobacter schindleri  Acinetobacter sp. LoGeW2-3  ...  \\\n",
       "0                   0.00010                     0.00005  ...   \n",
       "1                   0.00262                     0.00140  ...   \n",
       "2                   0.00191                     0.00132  ...   \n",
       "3                   0.00003                     0.00001  ...   \n",
       "4                   0.00009                     0.00001  ...   \n",
       "\n",
       "   Variovorax boronicumulans  Variovorax paradoxus  Variovorax sp. PAMC 28711  \\\n",
       "0                    0.00031               0.00075                    0.00021   \n",
       "1                    0.00013               0.00024                    0.00003   \n",
       "2                    0.00010               0.00025                    0.00001   \n",
       "3                    0.00003               0.00002                    0.00000   \n",
       "4                    0.00004               0.00008                    0.00003   \n",
       "\n",
       "   Veillonella parvula  Weissella cibaria  Xanthomonas campestris  continent  \\\n",
       "0                  0.0                0.0                 0.00480    oceania   \n",
       "1                  0.0                0.0                 0.00091    oceania   \n",
       "2                  0.0                0.0                 0.00208    oceania   \n",
       "3                  0.0                0.0                 0.00137    oceania   \n",
       "4                  0.0                0.0                 0.00397    oceania   \n",
       "\n",
       "       city  latitude  longitude  \n",
       "0  hamilton -37.78333  175.28333  \n",
       "1  hamilton -37.78333  175.28333  \n",
       "2  hamilton -37.78333  175.28333  \n",
       "3  hamilton -37.78333  175.28333  \n",
       "4  hamilton -37.78333  175.28333  \n",
       "\n",
       "[5 rows x 204 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_data = pd.concat([metasub_data[selected_features],metasub_data[['continent','city','latitude','longitude']]],axis=1)\n",
    "nn_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the city and continent names into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique cities in the metasub dataset are 40.\n",
      "The unique continents in the metasub dataset are ['oceania', 'south_america', 'east_asia', 'sub_saharan_africa', 'middle_east', 'north_america', 'europe']\n"
     ]
    }
   ],
   "source": [
    "print(f\"The unique cities in the metasub dataset are {len(list(nn_data['city'].unique()))}.\")\n",
    "print(f\"The unique continents in the metasub dataset are {list(nn_data['continent'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acidovorax ebreus</th>\n",
       "      <th>Acidovorax sp. JS42</th>\n",
       "      <th>Acidovorax sp. KKS102</th>\n",
       "      <th>Acinetobacter baumannii</th>\n",
       "      <th>Acinetobacter haemolyticus</th>\n",
       "      <th>Acinetobacter johnsonii</th>\n",
       "      <th>Acinetobacter junii</th>\n",
       "      <th>Acinetobacter pittii</th>\n",
       "      <th>Acinetobacter schindleri</th>\n",
       "      <th>Acinetobacter sp. LoGeW2-3</th>\n",
       "      <th>...</th>\n",
       "      <th>Variovorax boronicumulans</th>\n",
       "      <th>Variovorax paradoxus</th>\n",
       "      <th>Variovorax sp. PAMC 28711</th>\n",
       "      <th>Veillonella parvula</th>\n",
       "      <th>Weissella cibaria</th>\n",
       "      <th>Xanthomonas campestris</th>\n",
       "      <th>city_encoding</th>\n",
       "      <th>continent_encoding</th>\n",
       "      <th>lat_scaled</th>\n",
       "      <th>long_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00480</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00028</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>0.00142</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00091</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00181</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.00274</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>0.00191</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00208</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00397</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Acidovorax ebreus  Acidovorax sp. JS42  Acidovorax sp. KKS102  \\\n",
       "0            0.00000              0.00000                0.00023   \n",
       "1            0.00000              0.00001                0.00003   \n",
       "2            0.00003              0.00000                0.00011   \n",
       "3            0.00000              0.00000                0.00000   \n",
       "4            0.00000              0.00000                0.00000   \n",
       "\n",
       "   Acinetobacter baumannii  Acinetobacter haemolyticus  \\\n",
       "0                  0.00015                     0.00000   \n",
       "1                  0.00028                     0.00016   \n",
       "2                  0.00181                     0.00060   \n",
       "3                  0.00002                     0.00001   \n",
       "4                  0.00003                     0.00000   \n",
       "\n",
       "   Acinetobacter johnsonii  Acinetobacter junii  Acinetobacter pittii  \\\n",
       "0                  0.00006              0.00001               0.00007   \n",
       "1                  0.00142              0.00017               0.00013   \n",
       "2                  0.00274              0.00030               0.00110   \n",
       "3                  0.00003              0.00000               0.00000   \n",
       "4                  0.00000              0.00000               0.00002   \n",
       "\n",
       "   Acinetobacter schindleri  Acinetobacter sp. LoGeW2-3  ...  \\\n",
       "0                   0.00010                     0.00005  ...   \n",
       "1                   0.00262                     0.00140  ...   \n",
       "2                   0.00191                     0.00132  ...   \n",
       "3                   0.00003                     0.00001  ...   \n",
       "4                   0.00009                     0.00001  ...   \n",
       "\n",
       "   Variovorax boronicumulans  Variovorax paradoxus  Variovorax sp. PAMC 28711  \\\n",
       "0                    0.00031               0.00075                    0.00021   \n",
       "1                    0.00013               0.00024                    0.00003   \n",
       "2                    0.00010               0.00025                    0.00001   \n",
       "3                    0.00003               0.00002                    0.00000   \n",
       "4                    0.00004               0.00008                    0.00003   \n",
       "\n",
       "   Veillonella parvula  Weissella cibaria  Xanthomonas campestris  \\\n",
       "0                  0.0                0.0                 0.00480   \n",
       "1                  0.0                0.0                 0.00091   \n",
       "2                  0.0                0.0                 0.00208   \n",
       "3                  0.0                0.0                 0.00137   \n",
       "4                  0.0                0.0                 0.00397   \n",
       "\n",
       "   city_encoding  continent_encoding  lat_scaled  long_scaled  \n",
       "0             10                   4   -3.548641     1.899948  \n",
       "1             10                   4   -3.548641     1.899948  \n",
       "2             10                   4   -3.548641     1.899948  \n",
       "3             10                   4   -3.548641     1.899948  \n",
       "4             10                   4   -3.548641     1.899948  \n",
       "\n",
       "[5 rows x 204 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize label and scalers\n",
    "le = LabelEncoder()\n",
    "stdscaler = StandardScaler() # I can try MinMaxScaler as well\n",
    "# Convert all the categorical variables into numbers\n",
    "nn_data['city_encoding'] = nn_data[['city']].apply(le.fit_transform)\n",
    "nn_data['continent_encoding'] = nn_data[['continent']].apply(le.fit_transform)\n",
    "nn_data['lat_scaled'] = stdscaler.fit_transform(nn_data[['latitude']])\n",
    "nn_data['long_scaled'] = stdscaler.fit_transform(nn_data[['longitude']])\n",
    "# Store all the new scaled and encoded data in a new dataframe\n",
    "encoded_nn_data = nn_data.drop(columns=['city','continent','latitude','longitude'],axis=1)\n",
    "encoded_nn_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3256, 200)\n",
      "(3256, 4)\n",
      "(814, 200)\n",
      "(814, 4)\n"
     ]
    }
   ],
   "source": [
    "# KFold - Shuffle=True, I will do this later. For now, I will just use the train_test_split.\n",
    "kf = KFold(n_splits=5,shuffle=True, random_state=123)\n",
    "\n",
    "X = encoded_nn_data.iloc[:,:200].values\n",
    "y = encoded_nn_data[['continent_encoding','city_encoding','lat_scaled','long_scaled']].values \n",
    "\n",
    "for train_idx, val_idx in kf.split(X,y[:,1]): # We will use only the city column to create the split. Based on the ordering of the columns in the previous cell.\n",
    "    X_train = pd.DataFrame(X[train_idx])\n",
    "    y_train = pd.DataFrame(y[train_idx])\n",
    "\n",
    "    X_test = pd.DataFrame(X[val_idx])\n",
    "    y_test = pd.DataFrame(y[val_idx])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_nn_data.iloc[:,:200].values,\n",
    "                                                    encoded_nn_data[['continent_encoding','city_encoding','lat_scaled','long_scaled']].values,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustDat(torch.utils.data.Dataset):\n",
    "    def __init__(self,df,target):\n",
    "        self.df = df.astype(np.float32)\n",
    "        self.target = target\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        dp = self.df[idx]\n",
    "        targ = self.target[idx]\n",
    "        dp = torch.from_numpy(dp)\n",
    "        targ = torch.tensor(targ,dtype=torch.long)[0] # I am getting only the continent here.\n",
    "        return dp,targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.5000e-04, 1.5000e-04, 1.2300e-03, 2.8000e-04, 1.8000e-04, 1.3000e-03,\n",
       "         1.4000e-04, 1.4000e-04, 1.1100e-03, 2.2000e-04, 8.5000e-04, 8.0000e-05,\n",
       "         0.0000e+00, 2.2300e-03, 0.0000e+00, 1.0000e-05, 2.0200e-03, 2.1000e-03,\n",
       "         8.8000e-04, 5.3630e-02, 8.8000e-04, 1.7500e-03, 6.4800e-03, 9.6000e-04,\n",
       "         6.6000e-04, 8.1000e-04, 4.8000e-04, 2.7000e-04, 1.1000e-03, 4.0000e-05,\n",
       "         0.0000e+00, 9.3000e-04, 3.0000e-05, 0.0000e+00, 4.0000e-05, 9.0000e-05,\n",
       "         1.4000e-04, 2.4000e-04, 7.6000e-04, 9.0300e-03, 4.5000e-04, 2.2600e-03,\n",
       "         9.9800e-03, 1.3200e-03, 1.0720e-02, 3.3245e-01, 1.7700e-03, 9.4000e-04,\n",
       "         7.3000e-04, 3.7100e-03, 6.9400e-03, 1.4000e-04, 3.8000e-04, 3.0000e-04,\n",
       "         1.0000e-05, 1.0000e-04, 9.0000e-05, 1.0000e-04, 4.6000e-04, 1.4000e-04,\n",
       "         1.7900e-03, 2.6000e-03, 0.0000e+00, 1.0300e-03, 9.1000e-04, 0.0000e+00,\n",
       "         8.5000e-04, 1.5000e-03, 6.6000e-04, 6.5000e-04, 3.1000e-03, 0.0000e+00,\n",
       "         3.8500e-03, 2.0000e-05, 0.0000e+00, 2.2000e-04, 2.1600e-03, 3.9600e-03,\n",
       "         7.5900e-03, 1.2000e-04, 9.1000e-04, 6.0000e-05, 1.1000e-04, 4.8000e-04,\n",
       "         1.9000e-04, 1.3000e-04, 2.4000e-04, 3.0000e-05, 9.5000e-04, 1.8000e-04,\n",
       "         0.0000e+00, 0.0000e+00, 7.3000e-04, 6.0000e-05, 9.8000e-04, 5.0000e-04,\n",
       "         1.6400e-03, 0.0000e+00, 1.5500e-03, 3.8000e-04, 2.9000e-04, 1.7500e-03,\n",
       "         6.1000e-04, 5.8000e-04, 1.9300e-03, 1.1400e-03, 1.1900e-03, 1.1300e-03,\n",
       "         8.7000e-04, 1.6020e-02, 1.0800e-03, 7.7000e-04, 3.2900e-03, 3.8700e-03,\n",
       "         6.6000e-04, 3.0100e-03, 8.4000e-04, 1.4100e-03, 1.3700e-03, 1.5400e-03,\n",
       "         4.9000e-04, 2.0000e-04, 2.5000e-04, 2.2000e-04, 5.0000e-04, 9.6000e-04,\n",
       "         1.7900e-03, 7.7000e-04, 2.0000e-04, 1.5000e-04, 4.7000e-04, 1.7700e-03,\n",
       "         3.0000e-05, 1.7300e-03, 6.4000e-04, 1.4000e-04, 6.6600e-03, 9.3000e-04,\n",
       "         9.9000e-04, 5.5000e-04, 5.7000e-04, 3.9100e-03, 2.5000e-04, 4.7000e-04,\n",
       "         2.0000e-04, 2.7700e-03, 1.7000e-04, 9.0000e-05, 2.3000e-04, 9.2000e-04,\n",
       "         6.0000e-05, 6.5000e-04, 5.6000e-04, 2.6000e-04, 6.2000e-04, 4.8000e-04,\n",
       "         9.2000e-04, 7.0000e-04, 5.7000e-04, 5.5000e-04, 1.0500e-03, 3.1200e-03,\n",
       "         4.0000e-05, 8.0000e-05, 6.9600e-03, 1.1200e-03, 5.0000e-05, 2.0000e-05,\n",
       "         9.7000e-04, 7.3000e-04, 3.0000e-05, 1.2000e-04, 2.8000e-04, 7.3000e-04,\n",
       "         9.0000e-05, 5.1000e-04, 2.0000e-04, 8.6000e-04, 5.7300e-03, 6.0000e-04,\n",
       "         7.0000e-04, 4.5000e-04, 4.7000e-04, 7.6000e-04, 4.8000e-04, 2.3000e-04,\n",
       "         5.3000e-04, 1.2000e-04, 1.1300e-03, 9.8000e-04, 1.2200e-03, 1.0100e-03,\n",
       "         9.0000e-05, 1.1830e-02, 8.3000e-04, 1.4500e-03, 5.4000e-04, 2.8000e-04,\n",
       "         2.1000e-04, 2.6200e-03]),\n",
       " tensor(3))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CustDat(X_train,y_train).__getitem__(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(CustDat(X_train,y_train),\n",
    "                                       batch_size=64,shuffle=True,num_workers=4,pin_memory=False)\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(CustDat(X_test,y_test),\n",
    "                                       batch_size=64,shuffle=True,num_workers=4,pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the class for the netural network continent, citites and latitude and longitutude\n",
    "\n",
    "class NeuralNetContinent(nn.Module):\n",
    "    def __init__(self,input_size_continents,num_continents):\n",
    "        super(NeuralNetContinent,self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size_continents,400) # 200 GITs\n",
    "        self.layer2 = nn.Linear(400,400)\n",
    "        self.layer3 = nn.Linear(400,200)\n",
    "        self.layer4 = nn.Linear(200,num_continents) # 7 continents\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.relu(self.layer1(x))\n",
    "        out = self.relu(self.layer2(out))\n",
    "        out = self.relu(self.layer3(out))\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class NeuralNetCities(nn.Module):\n",
    "    def __init__(self, input_size_cities, num_cities):\n",
    "        super(NeuralNetCities,self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size_cities,400) # 207 GITs\n",
    "        self.layer2 = nn.Linear(400,400)\n",
    "        self.layer3 = nn.Linear(400,200)\n",
    "        self.layer4 = nn.Linear(200,num_cities) # 40 continents\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.relu(self.layer1(x))\n",
    "        out = self.relu(self.layer2(out))\n",
    "        out = self.relu(self.layer3(out))\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "    \n",
    "# Hyperparameters\n",
    "input_size_continents = 200\n",
    "input_size_cities = 207\n",
    "num_continents = 7\n",
    "num_cities = 40\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize thr network\n",
    "nn_continent_model = NeuralNetContinent(input_size_continents=input_size_continents,num_continents=num_continents).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(nn_continent_model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.4372, Epoch Time: 1.37 seconds\n",
      "Epoch 2/10, Loss: 1.0299, Epoch Time: 1.43 seconds\n",
      "Epoch 3/10, Loss: 1.0685, Epoch Time: 1.39 seconds\n",
      "Epoch 4/10, Loss: 0.9691, Epoch Time: 1.32 seconds\n",
      "Epoch 5/10, Loss: 0.9231, Epoch Time: 1.30 seconds\n",
      "Epoch 6/10, Loss: 0.3848, Epoch Time: 1.30 seconds\n",
      "Epoch 7/10, Loss: 0.8575, Epoch Time: 1.38 seconds\n",
      "Epoch 8/10, Loss: 0.4016, Epoch Time: 1.38 seconds\n",
      "Epoch 9/10, Loss: 0.7561, Epoch Time: 1.24 seconds\n",
      "Epoch 10/10, Loss: 0.4587, Epoch Time: 1.30 seconds\n",
      "Total Training Time: 13.41 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  # Start time\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time() # start time of each epoch\n",
    "    for batch_idx, (data, target) in enumerate(train_dl):\n",
    "        data = data.to(device=device)\n",
    "        target = target.to(device=device)\n",
    "                \n",
    "        # scores\n",
    "        scores = nn_continent_model(data)\n",
    "\n",
    "        loss = criterion(scores, target)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    epoch_end_time = time.time() # end time of each epoch\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Epoch Time: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "end_time = time.time()  # End time\n",
    "total_duration = end_time - start_time\n",
    "print(f\"Total Training Time: {total_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2651/3256 with accuracy 81.41891891891892\n",
      "Got 639/814 with accuracy 78.50122850122851\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy on training and test to see how good model is\n",
    "def check_accuracy(loader,model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100}')\n",
    "\n",
    "    model.train() \n",
    "\n",
    "\n",
    "check_accuracy(train_dl,nn_continent_model)\n",
    "check_accuracy(test_dl,nn_continent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
