{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metasub data mGPS algorithm - 31/03/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am just trying to get the data pre-processing steps right. The idea for doing this is to get the dataset in the right format for easier analysis using neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "os.chdir(\"/home/inf-21-2024/binp37/\")\n",
    "# Read the metadata for the metasub data.\n",
    "complete_meta = pd.read_csv(\"./data/metasub/complete_metadata.csv\")\n",
    "taxa_abund = pd.read_csv(\"./data/metasub/metasub_taxa_abundance.csv\")\n",
    "taxa_abund = taxa_abund.drop_duplicates(subset=['uuid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4288, 3711)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the bacterial and metadata\n",
    "metasub_data = pd.merge(complete_meta,taxa_abund,on='uuid')\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4157, 3711)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove control samples\n",
    "control_cities = {'control','other_control','neg_control','other','pos_control'}\n",
    "control_types = {'ctrl cities','negative_control','positive_control'}\n",
    "\n",
    "mask = metasub_data['city'].isin(control_cities) | metasub_data['control_type'].isin(control_types)\n",
    "metasub_data = metasub_data[~mask].copy()\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4157, 3711)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Re-label london boroughs\n",
    "metasub_data.loc[metasub_data['city'].isin(['kensington','islington']),'city'] = 'london'\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove sparse sample locations and doubtful samples\n",
    "city_counts = metasub_data['city'].value_counts()\n",
    "small_cities = city_counts[city_counts<8].index.tolist()\n",
    "remove_samples = metasub_data['city'].isin(['antarctica']+small_cities)\n",
    "metasub_data = metasub_data[~remove_samples]\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct the identified mislabeling of data\n",
    "kyiv_filter = metasub_data['city'] == 'kyiv'\n",
    "metasub_data.loc[kyiv_filter,'latitude'] = metasub_data.loc[kyiv_filter,'city_latitude'] # Set all the latitude to the city_latitude\n",
    "metasub_data.loc[kyiv_filter,'longitude'] = metasub_data.loc[kyiv_filter,'city_longitude'] # Set all the latitude to the city_longitutde\n",
    "\n",
    "porto_filter = metasub_data['city'] == 'porto'\n",
    "metasub_data.loc[porto_filter,'city'] = \"europe\"\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing latitude and longitude values with city-level data\n",
    "missing_lat = metasub_data[\"latitude\"].isna()\n",
    "missing_lon = metasub_data[\"longitude\"].isna()\n",
    "metasub_data.loc[missing_lat, \"latitude\"] = metasub_data.loc[missing_lat, \"city_latitude\"]\n",
    "metasub_data.loc[missing_lon, \"longitude\"] = metasub_data.loc[missing_lon, \"city_longitude\"]\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 3711)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correction for incorrect London co-ordinates\n",
    "london_filter = metasub_data['city'] == 'london'\n",
    "metasub_data.loc[london_filter,'city_latitude'] = 51.50853\n",
    "metasub_data.loc[london_filter,'city_longitude'] = -0.12574\n",
    "metasub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uuid', 'metasub_name', 'core_project', 'project', 'city', 'city_code',\n",
       "       'latitude', 'longitude', 'surface_material', 'control_type',\n",
       "       'elevation', 'line', 'station', 'surface', 'temperature', 'traffic',\n",
       "       'setting', 'num_reads', 'library_post_PCR_Qubit',\n",
       "       'library_QC_concentration', 'city_latitude', 'city_longitude',\n",
       "       'coastal_city', 'city_total_population', 'city_population_density',\n",
       "       'city_land_area_km2', 'city_ave_june_temp_c', 'city_elevation_meters',\n",
       "       'continent', 'city_koppen_climate', 'barcode', 'ha_id',\n",
       "       'hudson_alpha_flowcell', 'hudson_alpha_project', 'index_sequence',\n",
       "       'location_type', 'hudson_alpha_uid', 'other_project_uid',\n",
       "       'plate_number', 'plate_pos', 'sample_type', 'sl_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metasub_data.iloc[:,:42].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination - 02/04/2025\n",
    "\n",
    "Here we use RFE as a part of a pipeline to get the best set of parameters for fitting the deep learning model. We need a suitable set of parameters for that are infromative enough to apply a deep learning model. These are called as geographically informative taxa (GITs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import multiprocessing\n",
    "import time\n",
    "import ray\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "*Explanation of the Code*\n",
    "\n",
    "This function is designed to perform feature selection by:\n",
    "\n",
    "1. **Removing Highly Correlated Features:**\n",
    "   - Computes a correlation matrix of the features.\n",
    "   - Identifies features with correlation greater than 0.98 and removes them.\n",
    "\n",
    "2. **Recursive Feature Elimination (RFE) for Feature Selection:**\n",
    "   - Uses a Random Forest Classifier to determine feature importance.\n",
    "   - Applies RFE (Recursive Feature Elimination) to iteratively remove the least important features.\n",
    "   - The number of features to keep is determined based on predefined subset sizes.\n",
    "\n",
    "3. **Parallel Processing Support:**  \n",
    "   Uses multiple CPU cores when specified for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection algorithm\n",
    "def species_select(X,y,remove_correlated=True,subsets=None,cores=1):\n",
    "    \"\"\"\n",
    "    Feature selection algorithm for species classification.\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.DataFrame): Feature matrix\n",
    "    y (pd.Series): Target variable\n",
    "    remove_correlated (bool): Whether we need to remoce highly correlated variables. (default is set to True)\n",
    "    subsets (list): List of feature subset sizes to evaluate. If None, it is determined automatically.\n",
    "    cores (int): Number of CPU cores to use for parallel computation. (default is set to 1)\n",
    "    \n",
    "    Returns:\n",
    "    RFE object: Trained Recursive Feature Elimination (RFE) model.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()  # Track execution time\n",
    "\n",
    "    # Set parallel processing\n",
    "    num_cores = multiprocessing.cpu_count() if cores > 1 else 1\n",
    "    print(f\"Using {num_cores} CPU cores for computation.\")\n",
    "\n",
    "    if remove_correlated:\n",
    "        # Compute correlation matrix\n",
    "        print(\"Calculating correlation matrix...\")\n",
    "        corr_matrix = X.corr()\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "        # Identify correlated features (above 0.98)\n",
    "        correlated_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.98)]\n",
    "\n",
    "        # Drop correlated features\n",
    "        X = X.drop(columns=correlated_features)\n",
    "        print(f\"Correlated feature removed: {len(correlated_features)}\")\n",
    "\n",
    "    # Determine default subset sizes if not provided\n",
    "    num_features = X.shape[1]\n",
    "    if subsets is None:\n",
    "        subsets = [num_features // 2, num_features // 4, num_features // 8, num_features // 16, num_features // 32, num_features // 64]\n",
    "        subsets = [s for s in subsets  if s > 0] # Remove non-positive values\n",
    "\n",
    "    print(f\"Feature selection subsets: {subsets}\")\n",
    "\n",
    "    # Define model (Random Forrest for fearure ranking)\n",
    "    model = RandomForestClassifier(n_jobs=num_cores, random_state=123)\n",
    "    print(\"Initialized RandomForestClassifier.\")\n",
    "\n",
    "    # Recursive Feature Elimination (RFE)\n",
    "    for subset in subsets:\n",
    "        print(f\"\\nStarting RFE with {subset} features....\")\n",
    "        start_rfe = time.time()\n",
    "        rfe = RFE(estimator=model, n_features_to_select=min(subsets),step=20)\n",
    "        # Fit RFE to the data\n",
    "        rfe.fit(X,y)\n",
    "        print(f\"Completed RFE with {subset} features in {time.time() - start_rfe:.2f} seconds.\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nFeature selection completed in {total_time:.2f} seconds.\")\n",
    "    return rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureElim = species_select(X=metasub_data.iloc[:,42:500],\n",
    "                             y=metasub_data['city'],\n",
    "                             remove_correlated=False,\n",
    "                             subsets=[50,100,200,500],\n",
    "                             cores=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You Can Do If It's Still Too Slow\n",
    "\n",
    "1. **Reduce Feature Set Before Running RFE:**\n",
    "    - If remove_correlated=False, you still have 3711 features.\n",
    "    - Try using only top 500-1000 features based on variance or importance.\n",
    "\n",
    "2. **Increase Step Size in RFE:**\n",
    "    - Default step=1 removes one feature per iteration, which is slow.\n",
    "    - Try step=5 or step=10 to remove multiple features at once:\n",
    "    - rfe = RFE(estimatoe=model, n_features_to_select=subset, step=5)\n",
    "3. **Use XGBoost Instead of Random Forest:**\n",
    "    - model = XGBClassifier(n_jobs=num_cores, random_state=123)\n",
    "4. **Sequential Feature Selector:**\n",
    "    - from sklearn.feature_selection import SequentialFeatureSelector\n",
    "    sfs = SequentialFeatureSelector(model, n_features_to_select=100, direction='backward', n_jobs=num_cores)\n",
    "    sfs.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = metasub_data.iloc[:,42:]\n",
    "y= metasub_data['city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can use ray for parallel processing. I just need to run this once to get the best number of features to select. So I can think about it whether I really need to use ray.\n",
    "model = RandomForestClassifier(n_jobs=12, random_state=123)\n",
    "pipe = make_pipeline(RFE(estimator=model,step=20))\n",
    "parameters= {\"rfe__n_features_to_select\":[50,100,200,300,500,1500]}\n",
    "print(f\"\\nStartinf RFE with subsets features ...\")\n",
    "\n",
    "n_features_options = parameters['rfe__n_features_to_select']\n",
    "cv = 10\n",
    "total_iterations = len(n_features_options)*cv\n",
    "\n",
    "results = []\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=total_iterations, desc='GridSearchCV + RFE') as pbar:\n",
    "    for n_features in n_features_options:\n",
    "        pipe.set_params(rfe__n_features_to_select=n_features)\n",
    "        fold_scores = []\n",
    "        for fold in range(cv):\n",
    "            # Manually perfrom cross-validation using StratifiedKFold\n",
    "            skf = StratifiedKFold(n_splits=cv,shuffle=True,random_state=fold)\n",
    "            splits = list(skf.split(X,y))\n",
    "            train_index, test_index = splits[fold]\n",
    "            X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            pipe.fit(X_train,y_train)\n",
    "            score = pipe.score(X_test,y_test)\n",
    "            fold_scores.append(score)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        mean_score = sum(fold_scores)/cv\n",
    "        results.append((n_features,mean_score))\n",
    "\n",
    "best_n_features, best_score = max(results, key=lambda x: x[1])\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print('Best params: {\"rfe__n_features_to_select\":', best_n_features, \"}\")\n",
    "print('Best accuracy:', best_score)\n",
    "print(f\"Total time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 09:17:52,877\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting RFE with subsets of features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(evaluate_rfe pid=301904)\u001b[0m /home/inf-21-2024/miniconda3/envs/ai_env/lib/python3.12/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\n",
      "\u001b[36m(evaluate_rfe pid=301904)\u001b[0m   warnings.warn(\n",
      "Parallel RFE + Cross-validation: 100%|██████████| 60/60 [1:15:27<00:00, 75.46s/it]   \n",
      "\u001b[36m(evaluate_rfe pid=301544)\u001b[0m /home/inf-21-2024/miniconda3/envs/ai_env/lib/python3.12/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\u001b[32m [repeated 59x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(evaluate_rfe pid=301544)\u001b[0m   warnings.warn(\u001b[32m [repeated 59x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {\"rfe__n_features_to_select\": 200}\n",
      "Best accuracy: 0.891155\n",
      "Total time taken: 4531.09 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initializr Ray\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True,num_cpus=100)\n",
    "\n",
    "# Get the X and y dataframes\n",
    "X = metasub_data.iloc[:,42:]\n",
    "y = metasub_data['city']\n",
    "model = RandomForestClassifier(n_jobs=1,random_state=123)\n",
    "parameters = {\"rfe__n_features_to_select\":[50,100,200,300,500,1500]}\n",
    "cv = 10\n",
    "n_features_options = parameters['rfe__n_features_to_select']\n",
    "total_iterations = len(n_features_options) * cv\n",
    "\n",
    "print(f\"\\nStarting RFE with subsets of features...\")\n",
    "\n",
    "# Define remote function for parallel excecution\n",
    "@ray.remote\n",
    "def evaluate_rfe(n_features, fold, X, y):\n",
    "    \"\"\"Perfroms RFE feature selection and evaluates performance for a given fold.\"\"\"\n",
    "    pipe = make_pipeline(RFE(estimator=model, n_features_to_select=n_features,step=10))\n",
    "\n",
    "    # We use the stratified K fold to split the data into training and validation sets\n",
    "    skf = StratifiedKFold(n_splits=cv,shuffle=True, random_state=fold)\n",
    "    train_index, test_index = list(skf.split(X,y))[fold]\n",
    "\n",
    "    # train_index and test_index contain the index values for extracting training and testing data from X and y variables.\n",
    "    X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit the model using the training data and the evaluate the score based on the testing data\n",
    "    pipe.fit(X_train, y_train)\n",
    "    score = pipe.score(X_test, y_test)\n",
    "\n",
    "    return n_features, fold, score\n",
    "\n",
    "start_time = time.time()\n",
    "tasks = [evaluate_rfe.remote(n_features,fold, X,y) for n_features in n_features_options for fold in range(cv)]\n",
    "\n",
    "results = []\n",
    "with tqdm(total=total_iterations, desc='Parallel RFE + Cross-validation') as pbar:\n",
    "    while tasks:\n",
    "        done, tasks = ray.wait(tasks, num_returns=1)\n",
    "        result = ray.get(done[0])\n",
    "        results.append((result[0],result[2])) # (n_features, score)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Aggregate mean accuracy for each feature subset\n",
    "results_df = pd.DataFrame(results, columns=[\"n_features\", \"accuracy\"])\n",
    "results_df = results_df.groupby(\"n_features\").mean().reset_index()\n",
    "\n",
    "# Find best feature subset\n",
    "best_n_features, best_score = results_df.loc[results_df[\"accuracy\"].idxmax()].values\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f'Best params: {{\"rfe__n_features_to_select\": {int(best_n_features)}}}')\n",
    "print(f'Best accuracy: {best_score:.6f}')\n",
    "print(f'Total time taken: {elapsed_time:.2f} seconds')\n",
    "\n",
    "ray.shutdown()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Acidovorax ebreus', 'Acidovorax sp. JS42', 'Acidovorax sp. KKS102',\n",
      "       'Acinetobacter baumannii', 'Acinetobacter haemolyticus',\n",
      "       'Acinetobacter johnsonii', 'Acinetobacter junii',\n",
      "       'Acinetobacter pittii', 'Acinetobacter schindleri',\n",
      "       'Acinetobacter sp. LoGeW2-3',\n",
      "       ...\n",
      "       'Thermothelomyces thermophila', 'Thielavia terrestris',\n",
      "       'Truepera radiovictrix', 'Tsukamurella sp. MH1',\n",
      "       'Variovorax boronicumulans', 'Variovorax paradoxus',\n",
      "       'Variovorax sp. PAMC 28711', 'Veillonella parvula', 'Weissella cibaria',\n",
      "       'Xanthomonas campestris'],\n",
      "      dtype='object', length=200)\n"
     ]
    }
   ],
   "source": [
    "# Since I have lost the model, but I know that 200 features gives the best accuracy of 0.89. \n",
    "model_200 = RandomForestClassifier(n_jobs=24,random_state=123)\n",
    "rfe = RFE(estimator=model_200,n_features_to_select=200,step=20)\n",
    "rfe.fit(X,y)\n",
    "\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(selected_features)\n",
    "\n",
    "\n",
    "# All the accuracy results from the previous runs\n",
    "#results_df = pd.DataFrame(results,columns=['n_vars','accuracy'])\n",
    "#results_df.to_csv('mgps_git_taxa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acidovorax ebreus</th>\n",
       "      <th>Acidovorax sp. JS42</th>\n",
       "      <th>Acidovorax sp. KKS102</th>\n",
       "      <th>Acinetobacter baumannii</th>\n",
       "      <th>Acinetobacter haemolyticus</th>\n",
       "      <th>Acinetobacter johnsonii</th>\n",
       "      <th>Acinetobacter junii</th>\n",
       "      <th>Acinetobacter pittii</th>\n",
       "      <th>Acinetobacter schindleri</th>\n",
       "      <th>Acinetobacter sp. LoGeW2-3</th>\n",
       "      <th>...</th>\n",
       "      <th>Variovorax boronicumulans</th>\n",
       "      <th>Variovorax paradoxus</th>\n",
       "      <th>Variovorax sp. PAMC 28711</th>\n",
       "      <th>Veillonella parvula</th>\n",
       "      <th>Weissella cibaria</th>\n",
       "      <th>Xanthomonas campestris</th>\n",
       "      <th>city</th>\n",
       "      <th>continent</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00480</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>oceania</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00028</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>0.00142</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00091</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>oceania</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00181</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.00274</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>0.00191</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00208</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>oceania</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>oceania</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00397</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>oceania</td>\n",
       "      <td>-37.78333</td>\n",
       "      <td>175.28333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4191</th>\n",
       "      <td>0.00044</td>\n",
       "      <td>0.00052</td>\n",
       "      <td>0.00019</td>\n",
       "      <td>0.00072</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.01410</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>0.00046</td>\n",
       "      <td>0.00052</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00037</td>\n",
       "      <td>0.00107</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>0.00042</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>london</td>\n",
       "      <td>europe</td>\n",
       "      <td>51.50000</td>\n",
       "      <td>-0.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>0.00014</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00019</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00147</td>\n",
       "      <td>0.00350</td>\n",
       "      <td>0.00113</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00079</td>\n",
       "      <td>london</td>\n",
       "      <td>europe</td>\n",
       "      <td>51.50000</td>\n",
       "      <td>-0.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00018</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00019</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00057</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>london</td>\n",
       "      <td>europe</td>\n",
       "      <td>51.50000</td>\n",
       "      <td>-0.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>0.00026</td>\n",
       "      <td>0.00038</td>\n",
       "      <td>0.00051</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00128</td>\n",
       "      <td>0.00304</td>\n",
       "      <td>0.00126</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00106</td>\n",
       "      <td>london</td>\n",
       "      <td>europe</td>\n",
       "      <td>51.50000</td>\n",
       "      <td>-0.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>0.00044</td>\n",
       "      <td>0.00080</td>\n",
       "      <td>0.00053</td>\n",
       "      <td>0.01328</td>\n",
       "      <td>0.00040</td>\n",
       "      <td>0.00609</td>\n",
       "      <td>0.00104</td>\n",
       "      <td>0.00134</td>\n",
       "      <td>0.00056</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00053</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>0.00093</td>\n",
       "      <td>0.00167</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00226</td>\n",
       "      <td>london</td>\n",
       "      <td>europe</td>\n",
       "      <td>51.50000</td>\n",
       "      <td>-0.20000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4070 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Acidovorax ebreus  Acidovorax sp. JS42  Acidovorax sp. KKS102  \\\n",
       "0               0.00000              0.00000                0.00023   \n",
       "1               0.00000              0.00001                0.00003   \n",
       "2               0.00003              0.00000                0.00011   \n",
       "3               0.00000              0.00000                0.00000   \n",
       "4               0.00000              0.00000                0.00000   \n",
       "...                 ...                  ...                    ...   \n",
       "4191            0.00044              0.00052                0.00019   \n",
       "4192            0.00000              0.00000                0.00022   \n",
       "4193            0.00003              0.00002                0.00002   \n",
       "4194            0.00026              0.00038                0.00051   \n",
       "4195            0.00044              0.00080                0.00053   \n",
       "\n",
       "      Acinetobacter baumannii  Acinetobacter haemolyticus  \\\n",
       "0                     0.00015                     0.00000   \n",
       "1                     0.00028                     0.00016   \n",
       "2                     0.00181                     0.00060   \n",
       "3                     0.00002                     0.00001   \n",
       "4                     0.00003                     0.00000   \n",
       "...                       ...                         ...   \n",
       "4191                  0.00072                     0.00015   \n",
       "4192                  0.00014                     0.00000   \n",
       "4193                  0.00018                     0.00009   \n",
       "4194                  0.00009                     0.00000   \n",
       "4195                  0.01328                     0.00040   \n",
       "\n",
       "      Acinetobacter johnsonii  Acinetobacter junii  Acinetobacter pittii  \\\n",
       "0                     0.00006              0.00001               0.00007   \n",
       "1                     0.00142              0.00017               0.00013   \n",
       "2                     0.00274              0.00030               0.00110   \n",
       "3                     0.00003              0.00000               0.00000   \n",
       "4                     0.00000              0.00000               0.00002   \n",
       "...                       ...                  ...                   ...   \n",
       "4191                  0.01410              0.00036               0.00046   \n",
       "4192                  0.00019              0.00000               0.00000   \n",
       "4193                  0.00055              0.00006               0.00019   \n",
       "4194                  0.00025              0.00004               0.00010   \n",
       "4195                  0.00609              0.00104               0.00134   \n",
       "\n",
       "      Acinetobacter schindleri  Acinetobacter sp. LoGeW2-3  ...  \\\n",
       "0                      0.00010                     0.00005  ...   \n",
       "1                      0.00262                     0.00140  ...   \n",
       "2                      0.00191                     0.00132  ...   \n",
       "3                      0.00003                     0.00001  ...   \n",
       "4                      0.00009                     0.00001  ...   \n",
       "...                        ...                         ...  ...   \n",
       "4191                   0.00052                     0.00024  ...   \n",
       "4192                   0.00000                     0.00000  ...   \n",
       "4193                   0.00009                     0.00001  ...   \n",
       "4194                   0.00006                     0.00000  ...   \n",
       "4195                   0.00056                     0.00007  ...   \n",
       "\n",
       "      Variovorax boronicumulans  Variovorax paradoxus  \\\n",
       "0                       0.00031               0.00075   \n",
       "1                       0.00013               0.00024   \n",
       "2                       0.00010               0.00025   \n",
       "3                       0.00003               0.00002   \n",
       "4                       0.00004               0.00008   \n",
       "...                         ...                   ...   \n",
       "4191                    0.00037               0.00107   \n",
       "4192                    0.00147               0.00350   \n",
       "4193                    0.00005               0.00016   \n",
       "4194                    0.00128               0.00304   \n",
       "4195                    0.00053               0.00140   \n",
       "\n",
       "      Variovorax sp. PAMC 28711  Veillonella parvula  Weissella cibaria  \\\n",
       "0                       0.00021              0.00000            0.00000   \n",
       "1                       0.00003              0.00000            0.00000   \n",
       "2                       0.00001              0.00000            0.00000   \n",
       "3                       0.00000              0.00000            0.00000   \n",
       "4                       0.00003              0.00000            0.00000   \n",
       "...                         ...                  ...                ...   \n",
       "4191                    0.00016              0.00042            0.00013   \n",
       "4192                    0.00113              0.00012            0.00002   \n",
       "4193                    0.00011              0.00057            0.00001   \n",
       "4194                    0.00126              0.00013            0.00008   \n",
       "4195                    0.00093              0.00167            0.00009   \n",
       "\n",
       "      Xanthomonas campestris      city  continent  latitude  longitude  \n",
       "0                    0.00480  hamilton    oceania -37.78333  175.28333  \n",
       "1                    0.00091  hamilton    oceania -37.78333  175.28333  \n",
       "2                    0.00208  hamilton    oceania -37.78333  175.28333  \n",
       "3                    0.00137  hamilton    oceania -37.78333  175.28333  \n",
       "4                    0.00397  hamilton    oceania -37.78333  175.28333  \n",
       "...                      ...       ...        ...       ...        ...  \n",
       "4191                 0.00132    london     europe  51.50000   -0.20000  \n",
       "4192                 0.00079    london     europe  51.50000   -0.20000  \n",
       "4193                 0.00043    london     europe  51.50000   -0.20000  \n",
       "4194                 0.00106    london     europe  51.50000   -0.20000  \n",
       "4195                 0.00226    london     europe  51.50000   -0.20000  \n",
       "\n",
       "[4070 rows x 204 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_data = X.loc[:,selected_features]\n",
    "nn_data = pd.concat([nn_data,metasub_data[['city','continent','latitude','longitude']]],axis=1)\n",
    "nn_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Neural Networks Tests - 03/04/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the city and continent names into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique cities in the metasub dataset are 40.\n",
      "The unique continents in the metasub dataset are ['oceania', 'south_america', 'east_asia', 'sub_saharan_africa', 'middle_east', 'north_america', 'europe']\n"
     ]
    }
   ],
   "source": [
    "print(f\"The unique cities in the metasub dataset are {len(list(nn_data['city'].unique()))}.\")\n",
    "print(f\"The unique continents in the metasub dataset are {list(nn_data['continent'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acidovorax ebreus</th>\n",
       "      <th>Acidovorax sp. JS42</th>\n",
       "      <th>Acidovorax sp. KKS102</th>\n",
       "      <th>Acinetobacter baumannii</th>\n",
       "      <th>Acinetobacter haemolyticus</th>\n",
       "      <th>Acinetobacter johnsonii</th>\n",
       "      <th>Acinetobacter junii</th>\n",
       "      <th>Acinetobacter pittii</th>\n",
       "      <th>Acinetobacter schindleri</th>\n",
       "      <th>Acinetobacter sp. LoGeW2-3</th>\n",
       "      <th>...</th>\n",
       "      <th>Variovorax boronicumulans</th>\n",
       "      <th>Variovorax paradoxus</th>\n",
       "      <th>Variovorax sp. PAMC 28711</th>\n",
       "      <th>Veillonella parvula</th>\n",
       "      <th>Weissella cibaria</th>\n",
       "      <th>Xanthomonas campestris</th>\n",
       "      <th>city_encoding</th>\n",
       "      <th>continent_encoding</th>\n",
       "      <th>lat_scaled</th>\n",
       "      <th>long_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00480</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00028</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>0.00142</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00091</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00181</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.00274</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>0.00191</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00208</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00397</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.548641</td>\n",
       "      <td>1.899948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Acidovorax ebreus  Acidovorax sp. JS42  Acidovorax sp. KKS102  \\\n",
       "0            0.00000              0.00000                0.00023   \n",
       "1            0.00000              0.00001                0.00003   \n",
       "2            0.00003              0.00000                0.00011   \n",
       "3            0.00000              0.00000                0.00000   \n",
       "4            0.00000              0.00000                0.00000   \n",
       "\n",
       "   Acinetobacter baumannii  Acinetobacter haemolyticus  \\\n",
       "0                  0.00015                     0.00000   \n",
       "1                  0.00028                     0.00016   \n",
       "2                  0.00181                     0.00060   \n",
       "3                  0.00002                     0.00001   \n",
       "4                  0.00003                     0.00000   \n",
       "\n",
       "   Acinetobacter johnsonii  Acinetobacter junii  Acinetobacter pittii  \\\n",
       "0                  0.00006              0.00001               0.00007   \n",
       "1                  0.00142              0.00017               0.00013   \n",
       "2                  0.00274              0.00030               0.00110   \n",
       "3                  0.00003              0.00000               0.00000   \n",
       "4                  0.00000              0.00000               0.00002   \n",
       "\n",
       "   Acinetobacter schindleri  Acinetobacter sp. LoGeW2-3  ...  \\\n",
       "0                   0.00010                     0.00005  ...   \n",
       "1                   0.00262                     0.00140  ...   \n",
       "2                   0.00191                     0.00132  ...   \n",
       "3                   0.00003                     0.00001  ...   \n",
       "4                   0.00009                     0.00001  ...   \n",
       "\n",
       "   Variovorax boronicumulans  Variovorax paradoxus  Variovorax sp. PAMC 28711  \\\n",
       "0                    0.00031               0.00075                    0.00021   \n",
       "1                    0.00013               0.00024                    0.00003   \n",
       "2                    0.00010               0.00025                    0.00001   \n",
       "3                    0.00003               0.00002                    0.00000   \n",
       "4                    0.00004               0.00008                    0.00003   \n",
       "\n",
       "   Veillonella parvula  Weissella cibaria  Xanthomonas campestris  \\\n",
       "0                  0.0                0.0                 0.00480   \n",
       "1                  0.0                0.0                 0.00091   \n",
       "2                  0.0                0.0                 0.00208   \n",
       "3                  0.0                0.0                 0.00137   \n",
       "4                  0.0                0.0                 0.00397   \n",
       "\n",
       "   city_encoding  continent_encoding  lat_scaled  long_scaled  \n",
       "0             10                   4   -3.548641     1.899948  \n",
       "1             10                   4   -3.548641     1.899948  \n",
       "2             10                   4   -3.548641     1.899948  \n",
       "3             10                   4   -3.548641     1.899948  \n",
       "4             10                   4   -3.548641     1.899948  \n",
       "\n",
       "[5 rows x 204 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize label and scalers\n",
    "le = LabelEncoder()\n",
    "stdscaler = StandardScaler() # I can try MinMaxScaler as well\n",
    "# Convert all the categorical variables into numbers\n",
    "nn_data['city_encoding'] = nn_data[['city']].apply(le.fit_transform)\n",
    "nn_data['continent_encoding'] = nn_data[['continent']].apply(le.fit_transform)\n",
    "nn_data['lat_scaled'] = stdscaler.fit_transform(nn_data[['latitude']])\n",
    "nn_data['long_scaled'] = stdscaler.fit_transform(nn_data[['longitude']])\n",
    "# Store all the new scaled and encoded data in a new dataframe\n",
    "encoded_nn_data = nn_data.drop(columns=['city','continent','latitude','longitude'],axis=1)\n",
    "encoded_nn_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3256, 200)\n",
      "(3256, 4)\n",
      "(814, 200)\n",
      "(814, 4)\n"
     ]
    }
   ],
   "source": [
    "# KFold - Shuffle=True, I will do this later. For now, I will just use the train_test_split.\n",
    "kf = KFold(n_splits=5,shuffle=True, random_state=123)\n",
    "\n",
    "X = encoded_nn_data.iloc[:,:200].values\n",
    "y = encoded_nn_data[['continent_encoding','city_encoding','lat_scaled','long_scaled']].values \n",
    "\n",
    "for train_idx, val_idx in kf.split(X,y[:,1]): # We will use only the city column to create the split. Based on the ordering of the columns in the previous cell.\n",
    "    X_train = pd.DataFrame(X[train_idx])\n",
    "    y_train = pd.DataFrame(y[train_idx])\n",
    "\n",
    "    X_test = pd.DataFrame(X[val_idx])\n",
    "    y_test = pd.DataFrame(y[val_idx])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_nn_data.iloc[:,:200].values,\n",
    "                                                    encoded_nn_data[['continent_encoding','city_encoding','lat_scaled','long_scaled']].values,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustDat(torch.utils.data.Dataset):\n",
    "    def __init__(self,df,target):\n",
    "        self.df = df\n",
    "        self.target = target\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        dp = self.df[idx]\n",
    "        targ = self.target[idx]\n",
    "        dp = torch.from_numpy(dp)\n",
    "        targ = torch.tensor(targ)[0] # I am getting only the continent here.\n",
    "        return dp,targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.5000e-04, 1.5000e-04, 1.2300e-03, 2.8000e-04, 1.8000e-04, 1.3000e-03,\n",
       "         1.4000e-04, 1.4000e-04, 1.1100e-03, 2.2000e-04, 8.5000e-04, 8.0000e-05,\n",
       "         0.0000e+00, 2.2300e-03, 0.0000e+00, 1.0000e-05, 2.0200e-03, 2.1000e-03,\n",
       "         8.8000e-04, 5.3630e-02, 8.8000e-04, 1.7500e-03, 6.4800e-03, 9.6000e-04,\n",
       "         6.6000e-04, 8.1000e-04, 4.8000e-04, 2.7000e-04, 1.1000e-03, 4.0000e-05,\n",
       "         0.0000e+00, 9.3000e-04, 3.0000e-05, 0.0000e+00, 4.0000e-05, 9.0000e-05,\n",
       "         1.4000e-04, 2.4000e-04, 7.6000e-04, 9.0300e-03, 4.5000e-04, 2.2600e-03,\n",
       "         9.9800e-03, 1.3200e-03, 1.0720e-02, 3.3245e-01, 1.7700e-03, 9.4000e-04,\n",
       "         7.3000e-04, 3.7100e-03, 6.9400e-03, 1.4000e-04, 3.8000e-04, 3.0000e-04,\n",
       "         1.0000e-05, 1.0000e-04, 9.0000e-05, 1.0000e-04, 4.6000e-04, 1.4000e-04,\n",
       "         1.7900e-03, 2.6000e-03, 0.0000e+00, 1.0300e-03, 9.1000e-04, 0.0000e+00,\n",
       "         8.5000e-04, 1.5000e-03, 6.6000e-04, 6.5000e-04, 3.1000e-03, 0.0000e+00,\n",
       "         3.8500e-03, 2.0000e-05, 0.0000e+00, 2.2000e-04, 2.1600e-03, 3.9600e-03,\n",
       "         7.5900e-03, 1.2000e-04, 9.1000e-04, 6.0000e-05, 1.1000e-04, 4.8000e-04,\n",
       "         1.9000e-04, 1.3000e-04, 2.4000e-04, 3.0000e-05, 9.5000e-04, 1.8000e-04,\n",
       "         0.0000e+00, 0.0000e+00, 7.3000e-04, 6.0000e-05, 9.8000e-04, 5.0000e-04,\n",
       "         1.6400e-03, 0.0000e+00, 1.5500e-03, 3.8000e-04, 2.9000e-04, 1.7500e-03,\n",
       "         6.1000e-04, 5.8000e-04, 1.9300e-03, 1.1400e-03, 1.1900e-03, 1.1300e-03,\n",
       "         8.7000e-04, 1.6020e-02, 1.0800e-03, 7.7000e-04, 3.2900e-03, 3.8700e-03,\n",
       "         6.6000e-04, 3.0100e-03, 8.4000e-04, 1.4100e-03, 1.3700e-03, 1.5400e-03,\n",
       "         4.9000e-04, 2.0000e-04, 2.5000e-04, 2.2000e-04, 5.0000e-04, 9.6000e-04,\n",
       "         1.7900e-03, 7.7000e-04, 2.0000e-04, 1.5000e-04, 4.7000e-04, 1.7700e-03,\n",
       "         3.0000e-05, 1.7300e-03, 6.4000e-04, 1.4000e-04, 6.6600e-03, 9.3000e-04,\n",
       "         9.9000e-04, 5.5000e-04, 5.7000e-04, 3.9100e-03, 2.5000e-04, 4.7000e-04,\n",
       "         2.0000e-04, 2.7700e-03, 1.7000e-04, 9.0000e-05, 2.3000e-04, 9.2000e-04,\n",
       "         6.0000e-05, 6.5000e-04, 5.6000e-04, 2.6000e-04, 6.2000e-04, 4.8000e-04,\n",
       "         9.2000e-04, 7.0000e-04, 5.7000e-04, 5.5000e-04, 1.0500e-03, 3.1200e-03,\n",
       "         4.0000e-05, 8.0000e-05, 6.9600e-03, 1.1200e-03, 5.0000e-05, 2.0000e-05,\n",
       "         9.7000e-04, 7.3000e-04, 3.0000e-05, 1.2000e-04, 2.8000e-04, 7.3000e-04,\n",
       "         9.0000e-05, 5.1000e-04, 2.0000e-04, 8.6000e-04, 5.7300e-03, 6.0000e-04,\n",
       "         7.0000e-04, 4.5000e-04, 4.7000e-04, 7.6000e-04, 4.8000e-04, 2.3000e-04,\n",
       "         5.3000e-04, 1.2000e-04, 1.1300e-03, 9.8000e-04, 1.2200e-03, 1.0100e-03,\n",
       "         9.0000e-05, 1.1830e-02, 8.3000e-04, 1.4500e-03, 5.4000e-04, 2.8000e-04,\n",
       "         2.1000e-04, 2.6200e-03], dtype=torch.float64),\n",
       " tensor(3., dtype=torch.float64))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CustDat(X_train,y_train).__getitem__(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(CustDat(X_train,y_train),\n",
    "                                       batch_size=64,shuffle=True,num_workers=4,pin_memory=False)\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(CustDat(X_test,y_test),\n",
    "                                       batch_size=64,shuffle=True,num_workers=4,pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the class for the netural network\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,input_size,num_continents):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size,400) # 200 GITs\n",
    "        self.layer2 = nn.Linear(400,400)\n",
    "        self.layer3 = nn.Linear(400,200)\n",
    "        self.layer4 = nn.Linear(200,num_continents) # 7 continents\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.relu(self.layer1(x))\n",
    "        out = self.relu(self.layer2(out))\n",
    "        out = self.relu(self.layer3(out))\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "# Hyperparameters\n",
    "input_size = 200\n",
    "num_continents = 7\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "num_epochs = 2\n",
    "\n",
    "# Initialize thr network\n",
    "nn_model = NeuralNet(input_size=input_size,num_continents=num_continents).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([56, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([56, 200])\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_dl):\n",
    "        data = data.to(device=device)\n",
    "        target = target.to(device=device)\n",
    "\n",
    "        # scores\n",
    "        scores = nn_model(data)\n",
    "        loss = criterion(scores,target)\n",
    "\n",
    "        #backward\n",
    "        optimizer.zero_grad() # Set all the gradient to zero for each batch, so that it does not store the back propogation calculations\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training and test to see how good model is\n",
    "def check_accuracy(loader,model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = score.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100}')\n",
    "\n",
    "    model.train() \n",
    "\n",
    "\n",
    "check_accuracy(train_dl,nn_model)\n",
    "check_accuracy(test_dl,nn_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
